{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gutenberg Corpus\n",
    "Contains some books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.corpus.reader.util.StreamBackedCorpusView"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "type(emma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webchat Corpus\n",
    "Contains informal conversations and bodies of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['firefox.txt', 'grail.txt', 'overheard.txt', 'pirates.txt', 'singles.txt', 'wine.txt']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import webtext\n",
    "print(webtext.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'want',\n",
       " 'hot',\n",
       " 'pics',\n",
       " 'of',\n",
       " 'a',\n",
       " 'female',\n",
       " ',',\n",
       " 'I',\n",
       " 'can',\n",
       " 'look',\n",
       " 'in',\n",
       " 'a',\n",
       " 'mirror',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More examples of informal webchat, haha\n",
    "from nltk.corpus import nps_chat\n",
    "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
    "chatroom[123]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brown Corpus\n",
    "Contains tagged bodies of text including genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...],\n",
       " ['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words(categories='news'), brown.words(fileids=['cg22'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freq Dist and Conditional Freq Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_text = brown.words(categories=\"news\")\n",
    "fdist = nltk.FreqDist(w.lower() for w in news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modals = ['maybe', 'can', 'have', 'eventually']\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((genre, word) for genre in brown.categories() for word in brown.words(categories=genre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                maybe  can have eventually \n",
      "           news    1   93  265    4 \n",
      "       religion    0   82  190    3 \n",
      "        hobbies    4  268  351    2 \n",
      "science_fiction    3   16   61    4 \n",
      "        romance   15   74  258    1 \n",
      "          humor    1   16   55    3 \n"
     ]
    }
   ],
   "source": [
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters Corpus\n",
    "Contains documents classified into 90 topics and grouped into training and test categories by fileid ex) \"test/14826\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14826',\n",
       " 'test/14828',\n",
       " 'test/14829',\n",
       " 'test/14832',\n",
       " 'test/14833',\n",
       " 'test/14839',\n",
       " 'test/14840',\n",
       " 'test/14841',\n",
       " 'test/14842',\n",
       " 'test/14843',\n",
       " 'test/14844',\n",
       " 'test/14849',\n",
       " 'test/14852',\n",
       " 'test/14854',\n",
       " 'test/14858',\n",
       " 'test/14859',\n",
       " 'test/14860',\n",
       " 'test/14861',\n",
       " 'test/14862',\n",
       " 'test/14863',\n",
       " 'test/14865',\n",
       " 'test/14867',\n",
       " 'test/14872',\n",
       " 'test/14873',\n",
       " 'test/14875',\n",
       " 'test/14876',\n",
       " 'test/14877',\n",
       " 'test/14881',\n",
       " 'test/14882',\n",
       " 'test/14885',\n",
       " 'test/14886',\n",
       " 'test/14888',\n",
       " 'test/14890',\n",
       " 'test/14891',\n",
       " 'test/14892',\n",
       " 'test/14899',\n",
       " 'test/14900',\n",
       " 'test/14903',\n",
       " 'test/14904',\n",
       " 'test/14907',\n",
       " 'test/14909',\n",
       " 'test/14911',\n",
       " 'test/14912',\n",
       " 'test/14913',\n",
       " 'test/14918',\n",
       " 'test/14919',\n",
       " 'test/14921',\n",
       " 'test/14922',\n",
       " 'test/14923',\n",
       " 'test/14926',\n",
       " 'test/14928',\n",
       " 'test/14930',\n",
       " 'test/14931',\n",
       " 'test/14932',\n",
       " 'test/14933',\n",
       " 'test/14934',\n",
       " 'test/14941',\n",
       " 'test/14943',\n",
       " 'test/14949',\n",
       " 'test/14951',\n",
       " 'test/14954',\n",
       " 'test/14957',\n",
       " 'test/14958',\n",
       " 'test/14959',\n",
       " 'test/14960',\n",
       " 'test/14962',\n",
       " 'test/14963',\n",
       " 'test/14964',\n",
       " 'test/14965',\n",
       " 'test/14967',\n",
       " 'test/14968',\n",
       " 'test/14969',\n",
       " 'test/14970',\n",
       " 'test/14971',\n",
       " 'test/14974',\n",
       " 'test/14975',\n",
       " 'test/14978',\n",
       " 'test/14981',\n",
       " 'test/14982',\n",
       " 'test/14983',\n",
       " 'test/14984',\n",
       " 'test/14985',\n",
       " 'test/14986',\n",
       " 'test/14987',\n",
       " 'test/14988',\n",
       " 'test/14993',\n",
       " 'test/14995',\n",
       " 'test/14998',\n",
       " 'test/15000',\n",
       " 'test/15001',\n",
       " 'test/15002',\n",
       " 'test/15004',\n",
       " 'test/15005',\n",
       " 'test/15006',\n",
       " 'test/15011',\n",
       " 'test/15012',\n",
       " 'test/15013',\n",
       " 'test/15016',\n",
       " 'test/15017',\n",
       " 'test/15020',\n",
       " 'test/15023',\n",
       " 'test/15024',\n",
       " 'test/15026',\n",
       " 'test/15027',\n",
       " 'test/15028',\n",
       " 'test/15029',\n",
       " 'test/15031',\n",
       " 'test/15032',\n",
       " 'test/15033',\n",
       " 'test/15037',\n",
       " 'test/15038',\n",
       " 'test/15043',\n",
       " 'test/15045',\n",
       " 'test/15046',\n",
       " 'test/15048',\n",
       " 'test/15049',\n",
       " 'test/15052',\n",
       " 'test/15053',\n",
       " 'test/15055',\n",
       " 'test/15056',\n",
       " 'test/15060',\n",
       " 'test/15061',\n",
       " 'test/15062',\n",
       " 'test/15063',\n",
       " 'test/15065',\n",
       " 'test/15067',\n",
       " 'test/15069',\n",
       " 'test/15070',\n",
       " 'test/15074',\n",
       " 'test/15077',\n",
       " 'test/15078',\n",
       " 'test/15079',\n",
       " 'test/15082',\n",
       " 'test/15090',\n",
       " 'test/15091',\n",
       " 'test/15092',\n",
       " 'test/15093',\n",
       " 'test/15094',\n",
       " 'test/15095',\n",
       " 'test/15096',\n",
       " 'test/15097',\n",
       " 'test/15103',\n",
       " 'test/15104',\n",
       " 'test/15106',\n",
       " 'test/15107',\n",
       " 'test/15109',\n",
       " 'test/15110',\n",
       " 'test/15111',\n",
       " 'test/15112',\n",
       " 'test/15118',\n",
       " 'test/15119',\n",
       " 'test/15120',\n",
       " 'test/15121',\n",
       " 'test/15122',\n",
       " 'test/15124',\n",
       " 'test/15126',\n",
       " 'test/15128',\n",
       " 'test/15129',\n",
       " 'test/15130',\n",
       " 'test/15132',\n",
       " 'test/15136',\n",
       " 'test/15138',\n",
       " 'test/15141',\n",
       " 'test/15144',\n",
       " 'test/15145',\n",
       " 'test/15146',\n",
       " 'test/15149',\n",
       " 'test/15152',\n",
       " 'test/15153',\n",
       " 'test/15154',\n",
       " 'test/15156',\n",
       " 'test/15157',\n",
       " 'test/15161',\n",
       " 'test/15162',\n",
       " 'test/15171',\n",
       " 'test/15172',\n",
       " 'test/15175',\n",
       " 'test/15179',\n",
       " 'test/15180',\n",
       " 'test/15185',\n",
       " 'test/15188',\n",
       " 'test/15189',\n",
       " 'test/15190',\n",
       " 'test/15193',\n",
       " 'test/15194',\n",
       " 'test/15197',\n",
       " 'test/15198',\n",
       " 'test/15200',\n",
       " 'test/15204',\n",
       " 'test/15205',\n",
       " 'test/15206',\n",
       " 'test/15207',\n",
       " 'test/15208',\n",
       " 'test/15210',\n",
       " 'test/15211',\n",
       " 'test/15212',\n",
       " 'test/15213',\n",
       " 'test/15217',\n",
       " 'test/15219',\n",
       " 'test/15220',\n",
       " 'test/15221',\n",
       " 'test/15222',\n",
       " 'test/15223',\n",
       " 'test/15226',\n",
       " 'test/15227',\n",
       " 'test/15230',\n",
       " 'test/15233',\n",
       " 'test/15234',\n",
       " 'test/15237',\n",
       " 'test/15238',\n",
       " 'test/15239',\n",
       " 'test/15240',\n",
       " 'test/15242',\n",
       " 'test/15243',\n",
       " 'test/15244',\n",
       " 'test/15246',\n",
       " 'test/15247',\n",
       " 'test/15250',\n",
       " 'test/15253',\n",
       " 'test/15254',\n",
       " 'test/15255',\n",
       " 'test/15258',\n",
       " 'test/15259',\n",
       " 'test/15262',\n",
       " 'test/15263',\n",
       " 'test/15264',\n",
       " 'test/15265',\n",
       " 'test/15270',\n",
       " 'test/15271',\n",
       " 'test/15273',\n",
       " 'test/15274',\n",
       " 'test/15276',\n",
       " 'test/15278',\n",
       " 'test/15280',\n",
       " 'test/15281',\n",
       " 'test/15283',\n",
       " 'test/15287',\n",
       " 'test/15290',\n",
       " 'test/15292',\n",
       " 'test/15294',\n",
       " 'test/15295',\n",
       " 'test/15296',\n",
       " 'test/15299',\n",
       " 'test/15300',\n",
       " 'test/15302',\n",
       " 'test/15303',\n",
       " 'test/15306',\n",
       " 'test/15307',\n",
       " 'test/15308',\n",
       " 'test/15309',\n",
       " 'test/15310',\n",
       " 'test/15311',\n",
       " 'test/15312',\n",
       " 'test/15313',\n",
       " 'test/15314',\n",
       " 'test/15315',\n",
       " 'test/15321',\n",
       " 'test/15322',\n",
       " 'test/15324',\n",
       " 'test/15325',\n",
       " 'test/15326',\n",
       " 'test/15327',\n",
       " 'test/15329',\n",
       " 'test/15335',\n",
       " 'test/15336',\n",
       " 'test/15337',\n",
       " 'test/15339',\n",
       " 'test/15341',\n",
       " 'test/15344',\n",
       " 'test/15345',\n",
       " 'test/15348',\n",
       " 'test/15349',\n",
       " 'test/15351',\n",
       " 'test/15352',\n",
       " 'test/15354',\n",
       " 'test/15356',\n",
       " 'test/15357',\n",
       " 'test/15359',\n",
       " 'test/15363',\n",
       " 'test/15364',\n",
       " 'test/15365',\n",
       " 'test/15366',\n",
       " 'test/15367',\n",
       " 'test/15368',\n",
       " 'test/15372',\n",
       " 'test/15375',\n",
       " 'test/15378',\n",
       " 'test/15379',\n",
       " 'test/15380',\n",
       " 'test/15383',\n",
       " 'test/15384',\n",
       " 'test/15386',\n",
       " 'test/15387',\n",
       " 'test/15388',\n",
       " 'test/15389',\n",
       " 'test/15391',\n",
       " 'test/15394',\n",
       " 'test/15396',\n",
       " 'test/15397',\n",
       " 'test/15400',\n",
       " 'test/15404',\n",
       " 'test/15406',\n",
       " 'test/15409',\n",
       " 'test/15410',\n",
       " 'test/15411',\n",
       " 'test/15413',\n",
       " 'test/15415',\n",
       " 'test/15416',\n",
       " 'test/15417',\n",
       " 'test/15420',\n",
       " 'test/15421',\n",
       " 'test/15424',\n",
       " 'test/15425',\n",
       " 'test/15427',\n",
       " 'test/15428',\n",
       " 'test/15429',\n",
       " 'test/15430',\n",
       " 'test/15431',\n",
       " 'test/15432',\n",
       " 'test/15436',\n",
       " 'test/15438',\n",
       " 'test/15441',\n",
       " 'test/15442',\n",
       " 'test/15444',\n",
       " 'test/15446',\n",
       " 'test/15447',\n",
       " 'test/15448',\n",
       " 'test/15449',\n",
       " 'test/15450',\n",
       " 'test/15451',\n",
       " 'test/15452',\n",
       " 'test/15453',\n",
       " 'test/15454',\n",
       " 'test/15455',\n",
       " 'test/15457',\n",
       " 'test/15459',\n",
       " 'test/15460',\n",
       " 'test/15462',\n",
       " 'test/15464',\n",
       " 'test/15467',\n",
       " 'test/15468',\n",
       " 'test/15471',\n",
       " 'test/15472',\n",
       " 'test/15476',\n",
       " 'test/15477',\n",
       " 'test/15478',\n",
       " 'test/15479',\n",
       " 'test/15481',\n",
       " 'test/15482',\n",
       " 'test/15483',\n",
       " 'test/15484',\n",
       " 'test/15485',\n",
       " 'test/15487',\n",
       " 'test/15489',\n",
       " 'test/15494',\n",
       " 'test/15495',\n",
       " 'test/15496',\n",
       " 'test/15500',\n",
       " 'test/15501',\n",
       " 'test/15503',\n",
       " 'test/15504',\n",
       " 'test/15510',\n",
       " 'test/15511',\n",
       " 'test/15515',\n",
       " 'test/15520',\n",
       " 'test/15521',\n",
       " 'test/15522',\n",
       " 'test/15523',\n",
       " 'test/15527',\n",
       " 'test/15528',\n",
       " 'test/15531',\n",
       " 'test/15532',\n",
       " 'test/15535',\n",
       " 'test/15536',\n",
       " 'test/15539',\n",
       " 'test/15540',\n",
       " 'test/15542',\n",
       " 'test/15543',\n",
       " 'test/15544',\n",
       " 'test/15545',\n",
       " 'test/15547',\n",
       " 'test/15548',\n",
       " 'test/15549',\n",
       " 'test/15550',\n",
       " 'test/15551',\n",
       " 'test/15552',\n",
       " 'test/15553',\n",
       " 'test/15556',\n",
       " 'test/15558',\n",
       " 'test/15559',\n",
       " 'test/15560',\n",
       " 'test/15561',\n",
       " 'test/15562',\n",
       " 'test/15563',\n",
       " 'test/15565',\n",
       " 'test/15566',\n",
       " 'test/15567',\n",
       " 'test/15568',\n",
       " 'test/15569',\n",
       " 'test/15570',\n",
       " 'test/15571',\n",
       " 'test/15572',\n",
       " 'test/15573',\n",
       " 'test/15574',\n",
       " 'test/15575',\n",
       " 'test/15578',\n",
       " 'test/15579',\n",
       " 'test/15580',\n",
       " 'test/15581',\n",
       " 'test/15582',\n",
       " 'test/15583',\n",
       " 'test/15584',\n",
       " 'test/15585',\n",
       " 'test/15590',\n",
       " 'test/15591',\n",
       " 'test/15593',\n",
       " 'test/15594',\n",
       " 'test/15595',\n",
       " 'test/15596',\n",
       " 'test/15597',\n",
       " 'test/15598',\n",
       " 'test/15600',\n",
       " 'test/15601',\n",
       " 'test/15602',\n",
       " 'test/15603',\n",
       " 'test/15605',\n",
       " 'test/15607',\n",
       " 'test/15610',\n",
       " 'test/15613',\n",
       " 'test/15615',\n",
       " 'test/15616',\n",
       " 'test/15617',\n",
       " 'test/15618',\n",
       " 'test/15620',\n",
       " 'test/15621',\n",
       " 'test/15623',\n",
       " 'test/15624',\n",
       " 'test/15625',\n",
       " 'test/15626',\n",
       " 'test/15629',\n",
       " 'test/15632',\n",
       " 'test/15634',\n",
       " 'test/15636',\n",
       " 'test/15637',\n",
       " 'test/15639',\n",
       " 'test/15640',\n",
       " 'test/15641',\n",
       " 'test/15642',\n",
       " 'test/15643',\n",
       " 'test/15646',\n",
       " 'test/15648',\n",
       " 'test/15649',\n",
       " 'test/15651',\n",
       " 'test/15653',\n",
       " 'test/15655',\n",
       " 'test/15656',\n",
       " 'test/15664',\n",
       " 'test/15666',\n",
       " 'test/15667',\n",
       " 'test/15668',\n",
       " 'test/15669',\n",
       " 'test/15672',\n",
       " 'test/15674',\n",
       " 'test/15675',\n",
       " 'test/15676',\n",
       " 'test/15677',\n",
       " 'test/15679',\n",
       " 'test/15680',\n",
       " 'test/15682',\n",
       " 'test/15686',\n",
       " 'test/15688',\n",
       " 'test/15689',\n",
       " 'test/15691',\n",
       " 'test/15692',\n",
       " 'test/15694',\n",
       " 'test/15695',\n",
       " 'test/15696',\n",
       " 'test/15698',\n",
       " 'test/15702',\n",
       " 'test/15703',\n",
       " 'test/15704',\n",
       " 'test/15707',\n",
       " 'test/15708',\n",
       " 'test/15709',\n",
       " 'test/15710',\n",
       " 'test/15713',\n",
       " 'test/15715',\n",
       " 'test/15717',\n",
       " 'test/15719',\n",
       " 'test/15720',\n",
       " 'test/15721',\n",
       " 'test/15723',\n",
       " 'test/15725',\n",
       " 'test/15726',\n",
       " 'test/15727',\n",
       " 'test/15728',\n",
       " 'test/15729',\n",
       " 'test/15732',\n",
       " 'test/15733',\n",
       " 'test/15736',\n",
       " 'test/15737',\n",
       " 'test/15739',\n",
       " 'test/15742',\n",
       " 'test/15749',\n",
       " 'test/15751',\n",
       " 'test/15753',\n",
       " 'test/15757',\n",
       " 'test/15759',\n",
       " 'test/15762',\n",
       " 'test/15767',\n",
       " 'test/15768',\n",
       " 'test/15769',\n",
       " 'test/15772',\n",
       " 'test/15777',\n",
       " 'test/15778',\n",
       " 'test/15780',\n",
       " 'test/15782',\n",
       " 'test/15785',\n",
       " 'test/15790',\n",
       " 'test/15793',\n",
       " 'test/15797',\n",
       " 'test/15798',\n",
       " 'test/15800',\n",
       " 'test/15801',\n",
       " 'test/15803',\n",
       " 'test/15804',\n",
       " 'test/15805',\n",
       " 'test/15807',\n",
       " 'test/15808',\n",
       " 'test/15810',\n",
       " 'test/15811',\n",
       " 'test/15816',\n",
       " 'test/15817',\n",
       " 'test/15819',\n",
       " 'test/15821',\n",
       " 'test/15822',\n",
       " 'test/15823',\n",
       " 'test/15829',\n",
       " 'test/15831',\n",
       " 'test/15832',\n",
       " 'test/15833',\n",
       " 'test/15834',\n",
       " 'test/15836',\n",
       " 'test/15838',\n",
       " 'test/15840',\n",
       " 'test/15841',\n",
       " 'test/15842',\n",
       " 'test/15844',\n",
       " 'test/15845',\n",
       " 'test/15846',\n",
       " 'test/15847',\n",
       " 'test/15851',\n",
       " 'test/15852',\n",
       " 'test/15853',\n",
       " 'test/15854',\n",
       " 'test/15855',\n",
       " 'test/15856',\n",
       " 'test/15858',\n",
       " 'test/15859',\n",
       " 'test/15860',\n",
       " 'test/15861',\n",
       " 'test/15863',\n",
       " 'test/15864',\n",
       " 'test/15865',\n",
       " 'test/15866',\n",
       " 'test/15867',\n",
       " 'test/15868',\n",
       " 'test/15869',\n",
       " 'test/15870',\n",
       " 'test/15871',\n",
       " 'test/15872',\n",
       " 'test/15874',\n",
       " 'test/15875',\n",
       " 'test/15876',\n",
       " 'test/15877',\n",
       " 'test/15878',\n",
       " 'test/15879',\n",
       " 'test/15881',\n",
       " 'test/15885',\n",
       " 'test/15886',\n",
       " 'test/15888',\n",
       " 'test/15889',\n",
       " 'test/15890',\n",
       " 'test/15892',\n",
       " 'test/15893',\n",
       " 'test/15894',\n",
       " 'test/15895',\n",
       " 'test/15896',\n",
       " 'test/15897',\n",
       " 'test/15898',\n",
       " 'test/15899',\n",
       " 'test/15900',\n",
       " 'test/15901',\n",
       " 'test/15902',\n",
       " 'test/15903',\n",
       " 'test/15904',\n",
       " 'test/15906',\n",
       " 'test/15908',\n",
       " 'test/15909',\n",
       " 'test/15910',\n",
       " 'test/15911',\n",
       " 'test/15912',\n",
       " 'test/15913',\n",
       " 'test/15914',\n",
       " 'test/15916',\n",
       " 'test/15917',\n",
       " 'test/15918',\n",
       " 'test/15920',\n",
       " 'test/15921',\n",
       " 'test/15922',\n",
       " 'test/15923',\n",
       " 'test/15924',\n",
       " 'test/15925',\n",
       " 'test/15927',\n",
       " 'test/15928',\n",
       " 'test/15929',\n",
       " 'test/15930',\n",
       " 'test/15932',\n",
       " 'test/15933',\n",
       " 'test/15934',\n",
       " 'test/15937',\n",
       " 'test/15939',\n",
       " 'test/15942',\n",
       " 'test/15944',\n",
       " 'test/15949',\n",
       " 'test/15950',\n",
       " 'test/15951',\n",
       " 'test/15952',\n",
       " 'test/15953',\n",
       " 'test/15956',\n",
       " 'test/15959',\n",
       " 'test/15960',\n",
       " 'test/15961',\n",
       " 'test/15963',\n",
       " 'test/15964',\n",
       " 'test/15967',\n",
       " 'test/15968',\n",
       " 'test/15969',\n",
       " 'test/15970',\n",
       " 'test/15973',\n",
       " 'test/15975',\n",
       " 'test/15976',\n",
       " 'test/15977',\n",
       " 'test/15978',\n",
       " 'test/15979',\n",
       " 'test/15980',\n",
       " 'test/15981',\n",
       " 'test/15984',\n",
       " 'test/15985',\n",
       " 'test/15987',\n",
       " 'test/15988',\n",
       " 'test/15989',\n",
       " 'test/15993',\n",
       " 'test/15995',\n",
       " 'test/15996',\n",
       " 'test/15997',\n",
       " 'test/15999',\n",
       " 'test/16002',\n",
       " 'test/16003',\n",
       " 'test/16004',\n",
       " 'test/16005',\n",
       " 'test/16006',\n",
       " 'test/16007',\n",
       " 'test/16009',\n",
       " 'test/16012',\n",
       " 'test/16013',\n",
       " 'test/16014',\n",
       " 'test/16015',\n",
       " 'test/16016',\n",
       " 'test/16021',\n",
       " 'test/16022',\n",
       " 'test/16023',\n",
       " 'test/16026',\n",
       " 'test/16029',\n",
       " 'test/16030',\n",
       " 'test/16033',\n",
       " 'test/16037',\n",
       " 'test/16040',\n",
       " 'test/16041',\n",
       " 'test/16045',\n",
       " 'test/16052',\n",
       " 'test/16053',\n",
       " 'test/16055',\n",
       " 'test/16063',\n",
       " 'test/16066',\n",
       " 'test/16067',\n",
       " 'test/16068',\n",
       " 'test/16069',\n",
       " 'test/16071',\n",
       " 'test/16072',\n",
       " 'test/16074',\n",
       " 'test/16075',\n",
       " 'test/16076',\n",
       " 'test/16077',\n",
       " 'test/16079',\n",
       " 'test/16080',\n",
       " 'test/16083',\n",
       " 'test/16086',\n",
       " 'test/16088',\n",
       " 'test/16091',\n",
       " 'test/16093',\n",
       " 'test/16094',\n",
       " 'test/16095',\n",
       " 'test/16096',\n",
       " 'test/16097',\n",
       " 'test/16098',\n",
       " 'test/16099',\n",
       " 'test/16100',\n",
       " 'test/16103',\n",
       " 'test/16106',\n",
       " 'test/16107',\n",
       " 'test/16108',\n",
       " 'test/16110',\n",
       " 'test/16111',\n",
       " 'test/16112',\n",
       " 'test/16115',\n",
       " 'test/16117',\n",
       " 'test/16118',\n",
       " 'test/16119',\n",
       " 'test/16120',\n",
       " 'test/16122',\n",
       " 'test/16123',\n",
       " 'test/16125',\n",
       " 'test/16126',\n",
       " 'test/16130',\n",
       " 'test/16133',\n",
       " 'test/16134',\n",
       " 'test/16136',\n",
       " 'test/16139',\n",
       " 'test/16140',\n",
       " 'test/16141',\n",
       " 'test/16142',\n",
       " 'test/16143',\n",
       " 'test/16144',\n",
       " 'test/16145',\n",
       " 'test/16146',\n",
       " 'test/16147',\n",
       " 'test/16148',\n",
       " 'test/16149',\n",
       " 'test/16150',\n",
       " 'test/16152',\n",
       " 'test/16155',\n",
       " 'test/16158',\n",
       " 'test/16159',\n",
       " 'test/16161',\n",
       " 'test/16162',\n",
       " 'test/16163',\n",
       " 'test/16164',\n",
       " 'test/16166',\n",
       " 'test/16170',\n",
       " 'test/16171',\n",
       " 'test/16172',\n",
       " 'test/16173',\n",
       " 'test/16175',\n",
       " 'test/16176',\n",
       " 'test/16177',\n",
       " 'test/16179',\n",
       " 'test/16180',\n",
       " 'test/16185',\n",
       " 'test/16188',\n",
       " 'test/16189',\n",
       " 'test/16190',\n",
       " 'test/16193',\n",
       " 'test/16194',\n",
       " 'test/16195',\n",
       " 'test/16196',\n",
       " 'test/16197',\n",
       " 'test/16200',\n",
       " 'test/16201',\n",
       " 'test/16202',\n",
       " 'test/16203',\n",
       " 'test/16206',\n",
       " 'test/16207',\n",
       " 'test/16210',\n",
       " 'test/16211',\n",
       " 'test/16212',\n",
       " 'test/16213',\n",
       " 'test/16214',\n",
       " 'test/16215',\n",
       " 'test/16216',\n",
       " 'test/16219',\n",
       " 'test/16221',\n",
       " 'test/16223',\n",
       " 'test/16225',\n",
       " 'test/16226',\n",
       " 'test/16228',\n",
       " 'test/16230',\n",
       " 'test/16232',\n",
       " 'test/16233',\n",
       " 'test/16234',\n",
       " 'test/16236',\n",
       " 'test/16238',\n",
       " 'test/16241',\n",
       " 'test/16243',\n",
       " 'test/16244',\n",
       " 'test/16246',\n",
       " 'test/16247',\n",
       " 'test/16248',\n",
       " 'test/16250',\n",
       " 'test/16251',\n",
       " 'test/16252',\n",
       " 'test/16255',\n",
       " 'test/16256',\n",
       " 'test/16257',\n",
       " 'test/16258',\n",
       " 'test/16260',\n",
       " 'test/16262',\n",
       " 'test/16263',\n",
       " 'test/16264',\n",
       " 'test/16265',\n",
       " 'test/16266',\n",
       " 'test/16268',\n",
       " 'test/16269',\n",
       " 'test/16270',\n",
       " 'test/16271',\n",
       " 'test/16274',\n",
       " 'test/16275',\n",
       " 'test/16277',\n",
       " 'test/16278',\n",
       " 'test/16279',\n",
       " 'test/16281',\n",
       " 'test/16282',\n",
       " 'test/16283',\n",
       " 'test/16284',\n",
       " 'test/16285',\n",
       " 'test/16286',\n",
       " 'test/16287',\n",
       " 'test/16288',\n",
       " 'test/16289',\n",
       " 'test/16291',\n",
       " 'test/16294',\n",
       " 'test/16297',\n",
       " 'test/16298',\n",
       " 'test/16299',\n",
       " 'test/16300',\n",
       " 'test/16301',\n",
       " 'test/16302',\n",
       " 'test/16303',\n",
       " 'test/16304',\n",
       " 'test/16307',\n",
       " 'test/16310',\n",
       " 'test/16311',\n",
       " 'test/16312',\n",
       " 'test/16314',\n",
       " 'test/16315',\n",
       " 'test/16316',\n",
       " 'test/16317',\n",
       " 'test/16318',\n",
       " 'test/16319',\n",
       " 'test/16320',\n",
       " 'test/16324',\n",
       " 'test/16327',\n",
       " 'test/16331',\n",
       " 'test/16332',\n",
       " 'test/16336',\n",
       " 'test/16337',\n",
       " 'test/16339',\n",
       " 'test/16342',\n",
       " 'test/16343',\n",
       " 'test/16346',\n",
       " 'test/16347',\n",
       " 'test/16348',\n",
       " 'test/16350',\n",
       " 'test/16354',\n",
       " 'test/16357',\n",
       " 'test/16359',\n",
       " 'test/16360',\n",
       " 'test/16362',\n",
       " 'test/16363',\n",
       " 'test/16365',\n",
       " 'test/16366',\n",
       " 'test/16367',\n",
       " 'test/16369',\n",
       " 'test/16370',\n",
       " 'test/16371',\n",
       " 'test/16372',\n",
       " 'test/16374',\n",
       " 'test/16376',\n",
       " 'test/16377',\n",
       " 'test/16379',\n",
       " 'test/16380',\n",
       " 'test/16383',\n",
       " 'test/16385',\n",
       " 'test/16386',\n",
       " 'test/16388',\n",
       " 'test/16390',\n",
       " 'test/16392',\n",
       " 'test/16393',\n",
       " 'test/16394',\n",
       " 'test/16395',\n",
       " 'test/16396',\n",
       " 'test/16398',\n",
       " 'test/16399',\n",
       " 'test/16400',\n",
       " 'test/16401',\n",
       " 'test/16402',\n",
       " 'test/16403',\n",
       " 'test/16404',\n",
       " 'test/16405',\n",
       " 'test/16406',\n",
       " 'test/16407',\n",
       " 'test/16409',\n",
       " 'test/16410',\n",
       " 'test/16415',\n",
       " 'test/16417',\n",
       " 'test/16418',\n",
       " 'test/16419',\n",
       " 'test/16420',\n",
       " 'test/16421',\n",
       " 'test/16422',\n",
       " 'test/16424',\n",
       " 'test/16426',\n",
       " 'test/16427',\n",
       " 'test/16428',\n",
       " 'test/16429',\n",
       " 'test/16430',\n",
       " 'test/16432',\n",
       " 'test/16433',\n",
       " 'test/16434',\n",
       " 'test/16437',\n",
       " 'test/16438',\n",
       " 'test/16440',\n",
       " 'test/16441',\n",
       " 'test/16442',\n",
       " 'test/16443',\n",
       " 'test/16444',\n",
       " 'test/16448',\n",
       " 'test/16449',\n",
       " 'test/16450',\n",
       " 'test/16454',\n",
       " 'test/16457',\n",
       " 'test/16458',\n",
       " 'test/16459',\n",
       " 'test/16460',\n",
       " 'test/16461',\n",
       " 'test/16463',\n",
       " 'test/16465',\n",
       " 'test/16468',\n",
       " 'test/16469',\n",
       " 'test/16470',\n",
       " 'test/16471',\n",
       " 'test/16472',\n",
       " 'test/16473',\n",
       " 'test/16475',\n",
       " 'test/16476',\n",
       " 'test/16478',\n",
       " 'test/16479',\n",
       " 'test/16480',\n",
       " 'test/16481',\n",
       " 'test/16483',\n",
       " 'test/16486',\n",
       " 'test/16487',\n",
       " 'test/16488',\n",
       " 'test/16490',\n",
       " 'test/16492',\n",
       " 'test/16493',\n",
       " 'test/16495',\n",
       " 'test/16496',\n",
       " 'test/16499',\n",
       " 'test/16502',\n",
       " 'test/16505',\n",
       " 'test/16510',\n",
       " 'test/16512',\n",
       " 'test/16513',\n",
       " 'test/16518',\n",
       " 'test/16519',\n",
       " 'test/16521',\n",
       " 'test/16522',\n",
       " 'test/16523',\n",
       " 'test/16525',\n",
       " 'test/16527',\n",
       " 'test/16530',\n",
       " 'test/16531',\n",
       " 'test/16533',\n",
       " 'test/16538',\n",
       " 'test/16539',\n",
       " 'test/16545',\n",
       " 'test/16546',\n",
       " 'test/16549',\n",
       " 'test/16551',\n",
       " 'test/16554',\n",
       " 'test/16555',\n",
       " 'test/16561',\n",
       " 'test/16563',\n",
       " 'test/16564',\n",
       " 'test/16565',\n",
       " 'test/16568',\n",
       " 'test/16569',\n",
       " 'test/16570',\n",
       " 'test/16574',\n",
       " 'test/16577',\n",
       " 'test/16581',\n",
       " 'test/16583',\n",
       " 'test/16584',\n",
       " 'test/16585',\n",
       " 'test/16587',\n",
       " 'test/16588',\n",
       " 'test/16589',\n",
       " 'test/16590',\n",
       " 'test/16591',\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acq',\n",
       " 'alum',\n",
       " 'barley',\n",
       " 'bop',\n",
       " 'carcass',\n",
       " 'castor-oil',\n",
       " 'cocoa',\n",
       " 'coconut',\n",
       " 'coconut-oil',\n",
       " 'coffee',\n",
       " 'copper',\n",
       " 'copra-cake',\n",
       " 'corn',\n",
       " 'cotton',\n",
       " 'cotton-oil',\n",
       " 'cpi',\n",
       " 'cpu',\n",
       " 'crude',\n",
       " 'dfl',\n",
       " 'dlr',\n",
       " 'dmk',\n",
       " 'earn',\n",
       " 'fuel',\n",
       " 'gas',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'grain',\n",
       " 'groundnut',\n",
       " 'groundnut-oil',\n",
       " 'heat',\n",
       " 'hog',\n",
       " 'housing',\n",
       " 'income',\n",
       " 'instal-debt',\n",
       " 'interest',\n",
       " 'ipi',\n",
       " 'iron-steel',\n",
       " 'jet',\n",
       " 'jobs',\n",
       " 'l-cattle',\n",
       " 'lead',\n",
       " 'lei',\n",
       " 'lin-oil',\n",
       " 'livestock',\n",
       " 'lumber',\n",
       " 'meal-feed',\n",
       " 'money-fx',\n",
       " 'money-supply',\n",
       " 'naphtha',\n",
       " 'nat-gas',\n",
       " 'nickel',\n",
       " 'nkr',\n",
       " 'nzdlr',\n",
       " 'oat',\n",
       " 'oilseed',\n",
       " 'orange',\n",
       " 'palladium',\n",
       " 'palm-oil',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'platinum',\n",
       " 'potato',\n",
       " 'propane',\n",
       " 'rand',\n",
       " 'rape-oil',\n",
       " 'rapeseed',\n",
       " 'reserves',\n",
       " 'retail',\n",
       " 'rice',\n",
       " 'rubber',\n",
       " 'rye',\n",
       " 'ship',\n",
       " 'silver',\n",
       " 'sorghum',\n",
       " 'soy-meal',\n",
       " 'soy-oil',\n",
       " 'soybean',\n",
       " 'strategic-metal',\n",
       " 'sugar',\n",
       " 'sun-meal',\n",
       " 'sun-oil',\n",
       " 'sunseed',\n",
       " 'tea',\n",
       " 'tin',\n",
       " 'trade',\n",
       " 'veg-oil',\n",
       " 'wheat',\n",
       " 'wpi',\n",
       " 'yen',\n",
       " 'zinc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuters.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['barley', 'corn', 'grain', 'wheat']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember a single article may have multiple topics\n",
    "reuters.categories('training/9865')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Innaugural Address Corpus\n",
    "Contains US Presidential Innaugural Addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt',\n",
       " '1805-Jefferson.txt',\n",
       " '1809-Madison.txt',\n",
       " '1813-Madison.txt',\n",
       " '1817-Monroe.txt',\n",
       " '1821-Monroe.txt',\n",
       " '1825-Adams.txt',\n",
       " '1829-Jackson.txt',\n",
       " '1833-Jackson.txt',\n",
       " '1837-VanBuren.txt',\n",
       " '1841-Harrison.txt',\n",
       " '1845-Polk.txt',\n",
       " '1849-Taylor.txt',\n",
       " '1853-Pierce.txt',\n",
       " '1857-Buchanan.txt',\n",
       " '1861-Lincoln.txt',\n",
       " '1865-Lincoln.txt',\n",
       " '1869-Grant.txt',\n",
       " '1873-Grant.txt',\n",
       " '1877-Hayes.txt',\n",
       " '1881-Garfield.txt',\n",
       " '1885-Cleveland.txt',\n",
       " '1889-Harrison.txt',\n",
       " '1893-Cleveland.txt',\n",
       " '1897-McKinley.txt',\n",
       " '1901-McKinley.txt',\n",
       " '1905-Roosevelt.txt',\n",
       " '1909-Taft.txt',\n",
       " '1913-Wilson.txt',\n",
       " '1917-Wilson.txt',\n",
       " '1921-Harding.txt',\n",
       " '1925-Coolidge.txt',\n",
       " '1929-Hoover.txt',\n",
       " '1933-Roosevelt.txt',\n",
       " '1937-Roosevelt.txt',\n",
       " '1941-Roosevelt.txt',\n",
       " '1945-Roosevelt.txt',\n",
       " '1949-Truman.txt',\n",
       " '1953-Eisenhower.txt',\n",
       " '1957-Eisenhower.txt',\n",
       " '1961-Kennedy.txt',\n",
       " '1965-Johnson.txt',\n",
       " '1969-Nixon.txt',\n",
       " '1973-Nixon.txt',\n",
       " '1977-Carter.txt',\n",
       " '1981-Reagan.txt',\n",
       " '1985-Reagan.txt',\n",
       " '1989-Bush.txt',\n",
       " '1993-Clinton.txt',\n",
       " '1997-Clinton.txt',\n",
       " '2001-Bush.txt',\n",
       " '2005-Bush.txt',\n",
       " '2009-Obama.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "# Split by fileid\n",
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEdCAYAAADtk8dMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njBjbAGJlHyNGjBjbAP4/6Lo6JFAYXwwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108d8f9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['america', 'citizen']\n",
    "    if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## All NLTK Corpora\n",
    "\n",
    "<table border=\"1\" class=\"docutils\" id=\"tab-corpora\">\n",
    "<colgroup>\n",
    "<col width=\"31%\">\n",
    "<col width=\"18%\">\n",
    "<col width=\"51%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr><th class=\"head\">Corpus</th>\n",
    "<th class=\"head\">Compiler</th>\n",
    "<th class=\"head\">Contents</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr><td>Brown Corpus</td>\n",
    "<td>Francis, Kucera</td>\n",
    "<td>15 genres, 1.15M words, tagged, categorized</td>\n",
    "</tr>\n",
    "<tr><td>CESS Treebanks</td>\n",
    "<td>CLiC-UB</td>\n",
    "<td>1M words, tagged and parsed (Catalan, Spanish)</td>\n",
    "</tr>\n",
    "<tr><td>Chat-80 Data Files</td>\n",
    "<td>Pereira &amp; Warren</td>\n",
    "<td>World Geographic Database</td>\n",
    "</tr>\n",
    "<tr><td>CMU Pronouncing Dictionary</td>\n",
    "<td>CMU</td>\n",
    "<td>127k entries</td>\n",
    "</tr>\n",
    "<tr><td>CoNLL 2000 Chunking Data</td>\n",
    "<td>CoNLL</td>\n",
    "<td>270k words, tagged and chunked</td>\n",
    "</tr>\n",
    "<tr><td>CoNLL 2002 Named Entity</td>\n",
    "<td>CoNLL</td>\n",
    "<td>700k words, pos- and named-entity-tagged (Dutch, Spanish)</td>\n",
    "</tr>\n",
    "<tr><td>CoNLL 2007 Dependency Treebanks (sel)</td>\n",
    "<td>CoNLL</td>\n",
    "<td>150k words, dependency parsed (Basque, Catalan)</td>\n",
    "</tr>\n",
    "<tr><td>Dependency Treebank</td>\n",
    "<td>Narad</td>\n",
    "<td>Dependency parsed version of Penn Treebank sample</td>\n",
    "</tr>\n",
    "<tr><td>FrameNet</td>\n",
    "<td>Fillmore, Baker et al</td>\n",
    "<td>10k word senses, 170k manually annotated sentences</td>\n",
    "</tr>\n",
    "<tr><td>Floresta Treebank</td>\n",
    "<td>Diana Santos et al</td>\n",
    "<td>9k sentences, tagged and parsed (Portuguese)</td>\n",
    "</tr>\n",
    "<tr><td>Gazetteer Lists</td>\n",
    "<td>Various</td>\n",
    "<td>Lists of cities and countries</td>\n",
    "</tr>\n",
    "<tr><td>Genesis Corpus</td>\n",
    "<td>Misc web sources</td>\n",
    "<td>6 texts, 200k words, 6 languages</td>\n",
    "</tr>\n",
    "<tr><td>Gutenberg (selections)</td>\n",
    "<td>Hart, Newby, et al</td>\n",
    "<td>18 texts, 2M words</td>\n",
    "</tr>\n",
    "<tr><td>Inaugural Address Corpus</td>\n",
    "<td>CSpan</td>\n",
    "<td>US Presidential Inaugural Addresses (1789-present)</td>\n",
    "</tr>\n",
    "<tr><td>Indian POS-Tagged Corpus</td>\n",
    "<td>Kumaran et al</td>\n",
    "<td>60k words, tagged (Bangla, Hindi, Marathi, Telugu)</td>\n",
    "</tr>\n",
    "<tr><td>MacMorpho Corpus</td>\n",
    "<td>NILC, USP, Brazil</td>\n",
    "<td>1M words, tagged (Brazilian Portuguese)</td>\n",
    "</tr>\n",
    "<tr><td>Movie Reviews</td>\n",
    "<td>Pang, Lee</td>\n",
    "<td>2k movie reviews with sentiment polarity classification</td>\n",
    "</tr>\n",
    "<tr><td>Names Corpus</td>\n",
    "<td>Kantrowitz, Ross</td>\n",
    "<td>8k male and female names</td>\n",
    "</tr>\n",
    "<tr><td>NIST 1999 Info Extr (selections)</td>\n",
    "<td>Garofolo</td>\n",
    "<td>63k words, newswire and named-entity SGML markup</td>\n",
    "</tr>\n",
    "<tr><td>Nombank</td>\n",
    "<td>Meyers</td>\n",
    "<td>115k propositions, 1400 noun frames</td>\n",
    "</tr>\n",
    "<tr><td>NPS Chat Corpus</td>\n",
    "<td>Forsyth, Martell</td>\n",
    "<td>10k IM chat posts, POS-tagged and dialogue-act tagged</td>\n",
    "</tr>\n",
    "<tr><td>Open Multilingual WordNet</td>\n",
    "<td>Bond et al</td>\n",
    "<td>15 languages, aligned to English WordNet</td>\n",
    "</tr>\n",
    "<tr><td>PP Attachment Corpus</td>\n",
    "<td>Ratnaparkhi</td>\n",
    "<td>28k prepositional phrases, tagged as noun or verb modifiers</td>\n",
    "</tr>\n",
    "<tr><td>Proposition Bank</td>\n",
    "<td>Palmer</td>\n",
    "<td>113k propositions, 3300 verb frames</td>\n",
    "</tr>\n",
    "<tr><td>Question Classification</td>\n",
    "<td>Li, Roth</td>\n",
    "<td>6k questions, categorized</td>\n",
    "</tr>\n",
    "<tr><td>Reuters Corpus</td>\n",
    "<td>Reuters</td>\n",
    "<td>1.3M words, 10k news documents, categorized</td>\n",
    "</tr>\n",
    "<tr><td>Roget's Thesaurus</td>\n",
    "<td>Project Gutenberg</td>\n",
    "<td>200k words, formatted text</td>\n",
    "</tr>\n",
    "<tr><td>RTE Textual Entailment</td>\n",
    "<td>Dagan et al</td>\n",
    "<td>8k sentence pairs, categorized</td>\n",
    "</tr>\n",
    "<tr><td>SEMCOR</td>\n",
    "<td>Rus, Mihalcea</td>\n",
    "<td>880k words, part-of-speech and sense tagged</td>\n",
    "</tr>\n",
    "<tr><td>Senseval 2 Corpus</td>\n",
    "<td>Pedersen</td>\n",
    "<td>600k words, part-of-speech and sense tagged</td>\n",
    "</tr>\n",
    "<tr><td>SentiWordNet</td>\n",
    "<td>Esuli, Sebastiani</td>\n",
    "<td>sentiment scores for 145k WordNet synonym sets</td>\n",
    "</tr>\n",
    "<tr><td>Shakespeare texts (selections)</td>\n",
    "<td>Bosak</td>\n",
    "<td>8 books in XML format</td>\n",
    "</tr>\n",
    "<tr><td>State of the Union Corpus</td>\n",
    "<td>CSPAN</td>\n",
    "<td>485k words, formatted text</td>\n",
    "</tr>\n",
    "<tr><td>Stopwords Corpus</td>\n",
    "<td>Porter et al</td>\n",
    "<td>2,400 stopwords for 11 languages</td>\n",
    "</tr>\n",
    "<tr><td>Swadesh Corpus</td>\n",
    "<td>Wiktionary</td>\n",
    "<td>comparative wordlists in 24 languages</td>\n",
    "</tr>\n",
    "<tr><td>Switchboard Corpus (selections)</td>\n",
    "<td>LDC</td>\n",
    "<td>36 phonecalls, transcribed, parsed</td>\n",
    "</tr>\n",
    "<tr><td>Univ Decl of Human Rights</td>\n",
    "<td>United Nations</td>\n",
    "<td>480k words, 300+ languages</td>\n",
    "</tr>\n",
    "<tr><td>Penn Treebank (selections)</td>\n",
    "<td>LDC</td>\n",
    "<td>40k words, tagged and parsed</td>\n",
    "</tr>\n",
    "<tr><td>TIMIT Corpus (selections)</td>\n",
    "<td>NIST/LDC</td>\n",
    "<td>audio files and transcripts for 16 speakers</td>\n",
    "</tr>\n",
    "<tr><td>VerbNet 2.1</td>\n",
    "<td>Palmer et al</td>\n",
    "<td>5k verbs, hierarchically organized, linked to WordNet</td>\n",
    "</tr>\n",
    "<tr><td>Wordlist Corpus</td>\n",
    "<td>OpenOffice.org et al</td>\n",
    "<td>960k words and 20k affixes for 8 languages</td>\n",
    "</tr>\n",
    "<tr><td>WordNet 3.0 (English)</td>\n",
    "<td>Miller, Fellbaum</td>\n",
    "<td>145k synonym sets</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El', 'grupo', 'estatal', 'Electricit_de_France', ...]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.cess_esp.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.floresta.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import udhr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "languages = ['Chickasaw', 'English', 'German_Deutsch', 'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEQCAYAAACqduMIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNxAeHh4eHinx/2iHck8f4ixKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a04fe10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (lang, len(word))\n",
    "    for lang in languages\n",
    "    for word in udhr.words(lang+'-Latin1'))\n",
    "cfd.plot(cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package nltk.corpus.reader in nltk.corpus:\n",
      "\n",
      "NAME\n",
      "    nltk.corpus.reader\n",
      "\n",
      "DESCRIPTION\n",
      "    NLTK corpus readers.  The modules in this package provide functions\n",
      "    that can be used to read corpus fileids in a variety of formats.  These\n",
      "    functions can be used to read both the corpus fileids that are\n",
      "    distributed in the NLTK corpus package, and corpus fileids that are part\n",
      "    of external corpora.\n",
      "    \n",
      "    Corpus Reader Functions\n",
      "    =======================\n",
      "    Each corpus module defines one or more \"corpus reader functions\",\n",
      "    which can be used to read documents from that corpus.  These functions\n",
      "    take an argument, ``item``, which is used to indicate which document\n",
      "    should be read from the corpus:\n",
      "    \n",
      "    - If ``item`` is one of the unique identifiers listed in the corpus\n",
      "      module's ``items`` variable, then the corresponding document will\n",
      "      be loaded from the NLTK corpus package.\n",
      "    - If ``item`` is a fileid, then that file will be read.\n",
      "    \n",
      "    Additionally, corpus reader functions can be given lists of item\n",
      "    names; in which case, they will return a concatenation of the\n",
      "    corresponding documents.\n",
      "    \n",
      "    Corpus reader functions are named based on the type of information\n",
      "    they return.  Some common examples, and their return types, are:\n",
      "    \n",
      "    - words(): list of str\n",
      "    - sents(): list of (list of str)\n",
      "    - paras(): list of (list of (list of str))\n",
      "    - tagged_words(): list of (str,str) tuple\n",
      "    - tagged_sents(): list of (list of (str,str))\n",
      "    - tagged_paras(): list of (list of (list of (str,str)))\n",
      "    - chunked_sents(): list of (Tree w/ (str,str) leaves)\n",
      "    - parsed_sents(): list of (Tree with str leaves)\n",
      "    - parsed_paras(): list of (list of (Tree with str leaves))\n",
      "    - xml(): A single xml ElementTree\n",
      "    - raw(): unprocessed corpus contents\n",
      "    \n",
      "    For example, to read a list of the words in the Brown Corpus, use\n",
      "    ``nltk.corpus.brown.words()``:\n",
      "    \n",
      "        >>> from nltk.corpus import brown\n",
      "        >>> print(\", \".join(brown.words()))\n",
      "        The, Fulton, County, Grand, Jury, said, ...\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    aligned\n",
      "    api\n",
      "    bnc\n",
      "    bracket_parse\n",
      "    chasen\n",
      "    childes\n",
      "    chunked\n",
      "    cmudict\n",
      "    conll\n",
      "    dependency\n",
      "    framenet\n",
      "    ieer\n",
      "    indian\n",
      "    ipipan\n",
      "    knbc\n",
      "    lin\n",
      "    nombank\n",
      "    nps_chat\n",
      "    pl196x\n",
      "    plaintext\n",
      "    ppattach\n",
      "    propbank\n",
      "    rte\n",
      "    semcor\n",
      "    senseval\n",
      "    sentiwordnet\n",
      "    sinica_treebank\n",
      "    string_category\n",
      "    switchboard\n",
      "    tagged\n",
      "    timit\n",
      "    toolbox\n",
      "    udhr\n",
      "    util\n",
      "    verbnet\n",
      "    wordlist\n",
      "    wordnet\n",
      "    xmldocs\n",
      "    ycoe\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "            nltk.corpus.reader.bracket_parse.CategorizedBracketParseCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.bracket_parse.BracketParseCorpusReader)\n",
      "            nltk.corpus.reader.pl196x.Pl196xCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "            nltk.corpus.reader.plaintext.CategorizedPlaintextCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.plaintext.PlaintextCorpusReader)\n",
      "                nltk.corpus.reader.plaintext.PortugueseCategorizedPlaintextCorpusReader\n",
      "            nltk.corpus.reader.tagged.CategorizedTaggedCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.tagged.TaggedCorpusReader)\n",
      "        nltk.corpus.reader.api.CorpusReader\n",
      "            nltk.corpus.reader.aligned.AlignedCorpusReader\n",
      "            nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "                nltk.corpus.reader.bracket_parse.BracketParseCorpusReader\n",
      "                    nltk.corpus.reader.bracket_parse.AlpinoCorpusReader\n",
      "                nltk.corpus.reader.dependency.DependencyCorpusReader\n",
      "                nltk.corpus.reader.knbc.KNBCorpusReader\n",
      "                nltk.corpus.reader.sinica_treebank.SinicaTreebankCorpusReader\n",
      "            nltk.corpus.reader.chasen.ChasenCorpusReader\n",
      "            nltk.corpus.reader.chunked.ChunkedCorpusReader\n",
      "            nltk.corpus.reader.cmudict.CMUDictCorpusReader\n",
      "            nltk.corpus.reader.conll.ConllCorpusReader\n",
      "                nltk.corpus.reader.conll.ConllChunkCorpusReader\n",
      "            nltk.corpus.reader.ieer.IEERCorpusReader\n",
      "            nltk.corpus.reader.indian.IndianCorpusReader\n",
      "            nltk.corpus.reader.ipipan.IPIPANCorpusReader\n",
      "            nltk.corpus.reader.lin.LinThesaurusCorpusReader\n",
      "            nltk.corpus.reader.nombank.NombankCorpusReader\n",
      "            nltk.corpus.reader.plaintext.PlaintextCorpusReader\n",
      "                nltk.corpus.reader.plaintext.EuroparlCorpusReader\n",
      "                nltk.corpus.reader.udhr.UdhrCorpusReader\n",
      "            nltk.corpus.reader.ppattach.PPAttachmentCorpusReader\n",
      "            nltk.corpus.reader.propbank.PropbankCorpusReader\n",
      "            nltk.corpus.reader.senseval.SensevalCorpusReader\n",
      "            nltk.corpus.reader.sentiwordnet.SentiWordNetCorpusReader\n",
      "            nltk.corpus.reader.string_category.StringCategoryCorpusReader\n",
      "            nltk.corpus.reader.switchboard.SwitchboardCorpusReader\n",
      "            nltk.corpus.reader.tagged.TaggedCorpusReader\n",
      "                nltk.corpus.reader.tagged.MacMorphoCorpusReader\n",
      "                nltk.corpus.reader.tagged.TimitTaggedCorpusReader\n",
      "            nltk.corpus.reader.timit.TimitCorpusReader\n",
      "            nltk.corpus.reader.toolbox.ToolboxCorpusReader\n",
      "            nltk.corpus.reader.wordlist.WordListCorpusReader\n",
      "                nltk.corpus.reader.wordlist.SwadeshCorpusReader\n",
      "            nltk.corpus.reader.wordnet.WordNetCorpusReader\n",
      "            nltk.corpus.reader.wordnet.WordNetICCorpusReader\n",
      "            nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "                nltk.corpus.reader.bnc.BNCCorpusReader\n",
      "                nltk.corpus.reader.childes.CHILDESCorpusReader\n",
      "                nltk.corpus.reader.framenet.FramenetCorpusReader\n",
      "                nltk.corpus.reader.nps_chat.NPSChatCorpusReader\n",
      "                nltk.corpus.reader.rte.RTECorpusReader\n",
      "                nltk.corpus.reader.semcor.SemcorCorpusReader\n",
      "                nltk.corpus.reader.verbnet.VerbnetCorpusReader\n",
      "            nltk.corpus.reader.ycoe.YCOECorpusReader\n",
      "        nltk.corpus.reader.sentiwordnet.SentiSynset\n",
      "    nltk.corpus.reader.util.StreamBackedCorpusView(nltk.util.AbstractLazySequence)\n",
      "        nltk.corpus.reader.pl196x.TEICorpusView\n",
      "    \n",
      "    class AlignedCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Reader for corpora of word-aligned sentences.  Tokens are assumed\n",
      "     |  to be separated by whitespace.  Sentences begin on separate lines.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AlignedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, sep='/', word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=56), sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=56), alignedsent_block_reader=<function read_alignedsent_block at 0x1078aad08>, encoding='latin1')\n",
      "     |      Construct a new Aligned Corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/...path to corpus.../'\n",
      "     |          >>> reader = AlignedCorpusReader(root, '.*', '.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |  \n",
      "     |  aligned_sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of AlignedSent objects.\n",
      "     |      :rtype: list(AlignedSent)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class AlpinoCorpusReader(BracketParseCorpusReader)\n",
      "     |  Reader for the Alpino Dutch Treebank.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AlpinoCorpusReader\n",
      "     |      BracketParseCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, encoding='ISO-8859-1', tagset=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class BNCCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  Corpus reader for the XML version of the British National Corpus.\n",
      "     |  \n",
      "     |  For access to the complete XML data structure, use the ``xml()``\n",
      "     |  method.  For access to simple word lists and tagged word lists, use\n",
      "     |  ``words()``, ``sents()``, ``tagged_words()``, and ``tagged_sents()``.\n",
      "     |  \n",
      "     |  You can obtain the full version of the BNC corpus at\n",
      "     |  http://www.ota.ox.ac.uk/desc/2554\n",
      "     |  \n",
      "     |  If you extracted the archive to a directory called `BNC`, then you can\n",
      "     |  instantiate the reder as::\n",
      "     |  \n",
      "     |      BNCCorpusReader(root='BNC/Texts/', fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BNCCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, lazy=True)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, strip_space=True, stem=False)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |      \n",
      "     |      :param strip_space: If true, then strip trailing spaces from\n",
      "     |          word tokens.  Otherwise, leave the spaces on the tokens.\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, c5=False, strip_space=True, stem=False)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |      \n",
      "     |      :param c5: If true, then the tags used will be the more detailed\n",
      "     |          c5 tags.  Otherwise, the simplified tags will be used.\n",
      "     |      :param strip_space: If true, then strip trailing spaces from\n",
      "     |          word tokens.  Otherwise, leave the spaces on the tokens.\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, c5=False, strip_space=True, stem=False)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |      \n",
      "     |      :param c5: If true, then the tags used will be the more detailed\n",
      "     |          c5 tags.  Otherwise, the simplified tags will be used.\n",
      "     |      :param strip_space: If true, then strip trailing spaces from\n",
      "     |          word tokens.  Otherwise, leave the spaces on the tokens.\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |  \n",
      "     |  words(self, fileids=None, strip_space=True, stem=False)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |      \n",
      "     |      :param strip_space: If true, then strip trailing spaces from\n",
      "     |          word tokens.  Otherwise, leave the spaces on the tokens.\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class BracketParseCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      "     |  Reader for corpora that consist of parenthesis-delineated parse\n",
      "     |  trees.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BracketParseCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, comment_char=None, detect_blocks='unindented_paren', encoding='utf8', tagset=None)\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param comment_char: The character which can appear at the start of\n",
      "     |          a line to indicate that the rest of the line is a comment.\n",
      "     |      :param detect_blocks: The method that is used to find blocks\n",
      "     |        in the corpus; can be 'unindented_paren' (every unindented\n",
      "     |        parenthesis starts a new parse) or 'sexpr' (brackets are\n",
      "     |        matched).\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CHILDESCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  Corpus reader for the XML version of the CHILDES corpus.\n",
      "     |  The CHILDES corpus is available at ``http://childes.psy.cmu.edu/``. The XML\n",
      "     |  version of CHILDES is located at ``http://childes.psy.cmu.edu/data-xml/``.\n",
      "     |  Copy the needed parts of the CHILDES XML corpus into the NLTK data directory\n",
      "     |  (``nltk_data/corpora/CHILDES/``).\n",
      "     |  \n",
      "     |  For access to the file text use the usual nltk functions,\n",
      "     |  ``words()``, ``sents()``, ``tagged_words()`` and ``tagged_sents()``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CHILDESCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  MLU(self, fileids=None, speaker='CHI')\n",
      "     |      :return: the given file(s) as a floating number\n",
      "     |      :rtype: list(float)\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, lazy=True)\n",
      "     |  \n",
      "     |  age(self, fileids=None, speaker='CHI', month=False)\n",
      "     |      :return: the given file(s) as string or int\n",
      "     |      :rtype: list or int\n",
      "     |      \n",
      "     |      :param month: If true, return months instead of year-month-date\n",
      "     |  \n",
      "     |  convert_age(self, age_year)\n",
      "     |      Caclculate age in months from a string in CHILDES format\n",
      "     |  \n",
      "     |  corpus(self, fileids=None)\n",
      "     |      :return: the given file(s) as a dict of ``(corpus_property_key, value)``\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  participants(self, fileids=None)\n",
      "     |      :return: the given file(s) as a dict of\n",
      "     |          ``(participant_property_key, value)``\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, speaker='ALL', stem=False, relation=None, strip_space=True, replace=False)\n",
      "     |      :return: the given file(s) as a list of sentences or utterances, each\n",
      "     |          encoded as a list of word strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |      \n",
      "     |      :param speaker: If specified, select specific speaker(s) defined\n",
      "     |          in the corpus. Default is 'ALL' (all participants). Common choices\n",
      "     |          are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude\n",
      "     |          researchers)\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |      :param relation: If true, then return tuples of ``(str,pos,relation_list)``.\n",
      "     |          If there is manually-annotated relation info, it will return\n",
      "     |          tuples of ``(str,pos,test_relation_list,str,pos,gold_relation_list)``\n",
      "     |      :param strip_space: If true, then strip trailing spaces from word\n",
      "     |          tokens. Otherwise, leave the spaces on the tokens.\n",
      "     |      :param replace: If true, then use the replaced (intended) word instead\n",
      "     |          of the original word (e.g., 'wat' will be replaced with 'watch')\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, speaker='ALL', stem=False, relation=None, strip_space=True, replace=False)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |      \n",
      "     |      :param speaker: If specified, select specific speaker(s) defined\n",
      "     |          in the corpus. Default is 'ALL' (all participants). Common choices\n",
      "     |          are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude\n",
      "     |          researchers)\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |      :param relation: If true, then return tuples of ``(str,pos,relation_list)``.\n",
      "     |          If there is manually-annotated relation info, it will return\n",
      "     |          tuples of ``(str,pos,test_relation_list,str,pos,gold_relation_list)``\n",
      "     |      :param strip_space: If true, then strip trailing spaces from word\n",
      "     |          tokens. Otherwise, leave the spaces on the tokens.\n",
      "     |      :param replace: If true, then use the replaced (intended) word instead\n",
      "     |          of the original word (e.g., 'wat' will be replaced with 'watch')\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, speaker='ALL', stem=False, relation=False, strip_space=True, replace=False)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |      \n",
      "     |      :param speaker: If specified, select specific speaker(s) defined\n",
      "     |          in the corpus. Default is 'ALL' (all participants). Common choices\n",
      "     |          are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude\n",
      "     |          researchers)\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |      :param relation: If true, then return tuples of (stem, index,\n",
      "     |          dependent_index)\n",
      "     |      :param strip_space: If true, then strip trailing spaces from word\n",
      "     |          tokens. Otherwise, leave the spaces on the tokens.\n",
      "     |      :param replace: If true, then use the replaced (intended) word instead\n",
      "     |          of the original word (e.g., 'wat' will be replaced with 'watch')\n",
      "     |  \n",
      "     |  webview_file(self, fileid, urlbase=None)\n",
      "     |      Map a corpus file to its web version on the CHILDES website,\n",
      "     |      and open it in a web browser.\n",
      "     |      \n",
      "     |      The complete URL to be used is:\n",
      "     |          childes.childes_url_base + urlbase + fileid.replace('.xml', '.cha')\n",
      "     |      \n",
      "     |      If no urlbase is passed, we try to calculate it.  This\n",
      "     |      requires that the childes corpus was set up to mirror the\n",
      "     |      folder hierarchy under childes.psy.cmu.edu/data-xml/, e.g.:\n",
      "     |      nltk_data/corpora/childes/Eng-USA/Cornell/??? or\n",
      "     |      nltk_data/corpora/childes/Romance/Spanish/Aguirre/???\n",
      "     |      \n",
      "     |      The function first looks (as a special case) if \"Eng-USA\" is\n",
      "     |      on the path consisting of <corpus root>+fileid; then if\n",
      "     |      \"childes\", possibly followed by \"data-xml\", appears. If neither\n",
      "     |      one is found, we use the unmodified fileid and hope for the best.\n",
      "     |      If this is not right, specify urlbase explicitly, e.g., if the\n",
      "     |      corpus root points to the Cornell folder, urlbase='Eng-USA/Cornell'.\n",
      "     |  \n",
      "     |  words(self, fileids=None, speaker='ALL', stem=False, relation=False, strip_space=True, replace=False)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |      :rtype: list(str)\n",
      "     |      \n",
      "     |      :param speaker: If specified, select specific speaker(s) defined\n",
      "     |          in the corpus. Default is 'ALL' (all participants). Common choices\n",
      "     |          are 'CHI' (the child), 'MOT' (mother), ['CHI','MOT'] (exclude\n",
      "     |          researchers)\n",
      "     |      :param stem: If true, then use word stems instead of word strings.\n",
      "     |      :param relation: If true, then return tuples of (stem, index,\n",
      "     |          dependent_index)\n",
      "     |      :param strip_space: If true, then strip trailing spaces from word\n",
      "     |          tokens. Otherwise, leave the spaces on the tokens.\n",
      "     |      :param replace: If true, then use the replaced (intended) word instead\n",
      "     |          of the original word (e.g., 'wat' will be replaced with 'watch')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  childes_url_base = 'http://childes.psy.cmu.edu/browser/index.php?url='\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CMUDictCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      CMUDictCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  dict(self)\n",
      "     |      :return: the cmudict lexicon as a dictionary, whose keys are\n",
      "     |      lowercase words and whose values are lists of pronunciations.\n",
      "     |  \n",
      "     |  entries(self)\n",
      "     |      :return: the cmudict lexicon as a list of entries\n",
      "     |      containing (word, transcriptions) tuples.\n",
      "     |  \n",
      "     |  raw(self)\n",
      "     |      :return: the cmudict lexicon as a raw string.\n",
      "     |  \n",
      "     |  words(self)\n",
      "     |      :return: a list of all words defined in the cmudict lexicon.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CategorizedBracketParseCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, BracketParseCorpusReader)\n",
      "     |  A reader for parsed corpora whose documents are\n",
      "     |  divided into categories based on their file identifiers.\n",
      "     |  @author: Nathan Schneider <nschneid@cs.cmu.edu>\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategorizedBracketParseCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      BracketParseCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize the corpus reader.  Categorization arguments\n",
      "     |      (C{cat_pattern}, C{cat_map}, and C{cat_file}) are passed to\n",
      "     |      the L{CategorizedCorpusReader constructor\n",
      "     |      <CategorizedCorpusReader.__init__>}.  The remaining arguments\n",
      "     |      are passed to the L{BracketParseCorpusReader constructor\n",
      "     |      <BracketParseCorpusReader.__init__>}.\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  parsed_paras(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  parsed_words(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CategorizedCorpusReader(builtins.object)\n",
      "     |  A mixin class used to aid in the implementation of corpus readers\n",
      "     |  for categorized corpora.  This class defines the method\n",
      "     |  ``categories()``, which returns a list of the categories for the\n",
      "     |  corpus or for a specified set of fileids; and overrides ``fileids()``\n",
      "     |  to take a ``categories`` argument, restricting the set of fileids to\n",
      "     |  be returned.\n",
      "     |  \n",
      "     |  Subclasses are expected to:\n",
      "     |  \n",
      "     |    - Call ``__init__()`` to set up the mapping.\n",
      "     |  \n",
      "     |    - Override all view methods to accept a ``categories`` parameter,\n",
      "     |      which can be used *instead* of the ``fileids`` parameter, to\n",
      "     |      select which fileids should be included in the returned view.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, kwargs)\n",
      "     |      Initialize this mapping based on keyword arguments, as\n",
      "     |      follows:\n",
      "     |      \n",
      "     |        - cat_pattern: A regular expression pattern used to find the\n",
      "     |          category for each file identifier.  The pattern will be\n",
      "     |          applied to each file identifier, and the first matching\n",
      "     |          group will be used as the category label for that file.\n",
      "     |      \n",
      "     |        - cat_map: A dictionary, mapping from file identifiers to\n",
      "     |          category labels.\n",
      "     |      \n",
      "     |        - cat_file: The name of a file that contains the mapping\n",
      "     |          from file identifiers to categories.  The argument\n",
      "     |          ``cat_delimiter`` can be used to specify a delimiter.\n",
      "     |      \n",
      "     |      The corresponding argument will be deleted from ``kwargs``.  If\n",
      "     |      more than one argument is specified, an exception will be\n",
      "     |      raised.\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CategorizedPlaintextCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, PlaintextCorpusReader)\n",
      "     |  A reader for plaintext corpora whose documents are divided into\n",
      "     |  categories based on their file identifiers.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategorizedPlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize the corpus reader.  Categorization arguments\n",
      "     |      (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
      "     |      the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
      "     |      are passed to the ``PlaintextCorpusReader`` constructor.\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CategorizedTaggedCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, TaggedCorpusReader)\n",
      "     |  A reader for part-of-speech tagged corpora whose documents are\n",
      "     |  divided into categories based on their file identifiers.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategorizedTaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |      Initialize the corpus reader.  Categorization arguments\n",
      "     |      (``cat_pattern``, ``cat_map``, and ``cat_file``) are passed to\n",
      "     |      the ``CategorizedCorpusReader`` constructor.  The remaining arguments\n",
      "     |      are passed to the ``TaggedCorpusReader``.\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, categories=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ChasenCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      ChasenCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', sent_splitter=None)\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ChunkedCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Reader for chunked (and optionally tagged) corpora.  Paragraphs\n",
      "     |  are split using a block reader.  They are then tokenized into\n",
      "     |  sentences using a sentence tokenizer.  Finally, these sentences\n",
      "     |  are parsed into chunk trees using a string-to-chunktree conversion\n",
      "     |  function.  Each of these steps can be performed using a default\n",
      "     |  function or a custom function.  By default, paragraphs are split\n",
      "     |  on blank lines; sentences are listed one per line; and sentences\n",
      "     |  are parsed into chunk trees using ``nltk.chunk.tagstr2tree``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ChunkedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, extension='', str2chunktree=<function tagstr2tree at 0x107b08400>, sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=56), para_block_reader=<function read_blankline_block at 0x1078aac80>, encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |  \n",
      "     |  chunked_paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as a shallow Tree.  The leaves of these\n",
      "     |          trees are encoded as ``(word, tag)`` tuples (if the corpus\n",
      "     |          has tags) or word strings (if the corpus has no tags).\n",
      "     |      :rtype: list(list(Tree))\n",
      "     |  \n",
      "     |  chunked_sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a shallow Tree.  The leaves\n",
      "     |          of these trees are encoded as ``(word, tag)`` tuples (if\n",
      "     |          the corpus has tags) or word strings (if the corpus has no\n",
      "     |          tags).\n",
      "     |      :rtype: list(Tree)\n",
      "     |  \n",
      "     |  chunked_words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and chunks.  Words are encoded as ``(word, tag)``\n",
      "     |          tuples (if the corpus has tags) or word strings (if the\n",
      "     |          corpus has no tags).  Chunks are encoded as depth-one\n",
      "     |          trees over ``(word,tag)`` tuples or word strings.\n",
      "     |      :rtype: list(tuple(str,str) and Tree)\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(list(tuple(str,str))))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ConllChunkCorpusReader(ConllCorpusReader)\n",
      "     |  A ConllCorpusReader whose data file contains three columns: words,\n",
      "     |  pos, and chunk.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ConllChunkCorpusReader\n",
      "     |      ConllCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, chunk_types, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ConllCorpusReader:\n",
      "     |  \n",
      "     |  chunked_sents(self, fileids=None, chunk_types=None, tagset=None)\n",
      "     |  \n",
      "     |  chunked_words(self, fileids=None, chunk_types=None, tagset=None)\n",
      "     |  \n",
      "     |  iob_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: a list of lists of word/tag/IOB tuples\n",
      "     |      :rtype: list(list)\n",
      "     |      :param fileids: the list of fileids that make up this corpus\n",
      "     |      :type fileids: None or str or list\n",
      "     |  \n",
      "     |  iob_words(self, fileids=None, tagset=None)\n",
      "     |      :return: a list of word/tag/IOB tuples\n",
      "     |      :rtype: list(tuple)\n",
      "     |      :param fileids: the list of fileids that make up this corpus\n",
      "     |      :type fileids: None or str or list\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  srl_instances(self, fileids=None, pos_in_tree=None, flatten=True)\n",
      "     |  \n",
      "     |  srl_spans(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from ConllCorpusReader:\n",
      "     |  \n",
      "     |  CHUNK = 'chunk'\n",
      "     |  \n",
      "     |  COLUMN_TYPES = ('words', 'pos', 'tree', 'chunk', 'ne', 'srl', 'ignore'...\n",
      "     |  \n",
      "     |  IGNORE = 'ignore'\n",
      "     |  \n",
      "     |  NE = 'ne'\n",
      "     |  \n",
      "     |  POS = 'pos'\n",
      "     |  \n",
      "     |  SRL = 'srl'\n",
      "     |  \n",
      "     |  TREE = 'tree'\n",
      "     |  \n",
      "     |  WORDS = 'words'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ConllCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  A corpus reader for CoNLL-style files.  These files consist of a\n",
      "     |  series of sentences, separated by blank lines.  Each sentence is\n",
      "     |  encoded using a table (or \"grid\") of values, where each line\n",
      "     |  corresponds to a single word, and each column corresponds to an\n",
      "     |  annotation type.  The set of columns used by CoNLL-style files can\n",
      "     |  vary from corpus to corpus; the ``ConllCorpusReader`` constructor\n",
      "     |  therefore takes an argument, ``columntypes``, which is used to\n",
      "     |  specify the columns that are used by a given corpus.\n",
      "     |  \n",
      "     |  @todo: Add support for reading from corpora where different\n",
      "     |      parallel files contain different columns.\n",
      "     |  @todo: Possibly add caching of the grid corpus view?  This would\n",
      "     |      allow the same grid view to be used by different data access\n",
      "     |      methods (eg words() and parsed_sents() could both share the\n",
      "     |      same grid corpus view object).\n",
      "     |  @todo: Better support for -DOCSTART-.  Currently, we just ignore\n",
      "     |      it, but it could be used to define methods that retrieve a\n",
      "     |      document at a time (eg parsed_documents()).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ConllCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, columntypes, chunk_types=None, root_label='S', pos_in_tree=False, srl_includes_roleset=True, encoding='utf8', tree_class=<class 'nltk.tree.Tree'>, tagset=None)\n",
      "     |  \n",
      "     |  chunked_sents(self, fileids=None, chunk_types=None, tagset=None)\n",
      "     |  \n",
      "     |  chunked_words(self, fileids=None, chunk_types=None, tagset=None)\n",
      "     |  \n",
      "     |  iob_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: a list of lists of word/tag/IOB tuples\n",
      "     |      :rtype: list(list)\n",
      "     |      :param fileids: the list of fileids that make up this corpus\n",
      "     |      :type fileids: None or str or list\n",
      "     |  \n",
      "     |  iob_words(self, fileids=None, tagset=None)\n",
      "     |      :return: a list of word/tag/IOB tuples\n",
      "     |      :rtype: list(tuple)\n",
      "     |      :param fileids: the list of fileids that make up this corpus\n",
      "     |      :type fileids: None or str or list\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None, pos_in_tree=None, tagset=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  srl_instances(self, fileids=None, pos_in_tree=None, flatten=True)\n",
      "     |  \n",
      "     |  srl_spans(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CHUNK = 'chunk'\n",
      "     |  \n",
      "     |  COLUMN_TYPES = ('words', 'pos', 'tree', 'chunk', 'ne', 'srl', 'ignore'...\n",
      "     |  \n",
      "     |  IGNORE = 'ignore'\n",
      "     |  \n",
      "     |  NE = 'ne'\n",
      "     |  \n",
      "     |  POS = 'pos'\n",
      "     |  \n",
      "     |  SRL = 'srl'\n",
      "     |  \n",
      "     |  TREE = 'tree'\n",
      "     |  \n",
      "     |  WORDS = 'words'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class CorpusReader(builtins.object)\n",
      "     |  A base class for \"corpus reader\" classes, each of which can be\n",
      "     |  used to read a specific corpus format.  Each individual corpus\n",
      "     |  reader instance is used to read a specific corpus, consisting of\n",
      "     |  one or more files under a common root directory.  Each file is\n",
      "     |  identified by its ``file identifier``, which is the relative path\n",
      "     |  to the file from the root directory.\n",
      "     |  \n",
      "     |  A separate subclass is be defined for each corpus format.  These\n",
      "     |  subclasses define one or more methods that provide 'views' on the\n",
      "     |  corpus contents, such as ``words()`` (for a list of words) and\n",
      "     |  ``parsed_sents()`` (for a list of parsed sentences).  Called with\n",
      "     |  no arguments, these methods will return the contents of the entire\n",
      "     |  corpus.  For most corpora, these methods define one or more\n",
      "     |  selection arguments, such as ``fileids`` or ``categories``, which can\n",
      "     |  be used to select which portion of the corpus should be returned.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class DependencyCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      DependencyCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', word_tokenizer=<nltk.tokenize.simple.TabTokenizer object at 0x107b899b0>, sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=56), para_block_reader=<function read_blankline_block at 0x1078aac80>)\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class EuroparlCorpusReader(PlaintextCorpusReader)\n",
      "     |  Reader for Europarl corpora that consist of plaintext documents.\n",
      "     |  Documents are divided into chapters instead of paragraphs as\n",
      "     |  for regular plaintext documents. Chapters are separated using blank\n",
      "     |  lines. Everything is inherited from ``PlaintextCorpusReader`` except\n",
      "     |  that:\n",
      "     |    - Since the corpus is pre-processed and pre-tokenized, the\n",
      "     |      word tokenizer should just split the line at whitespaces.\n",
      "     |    - For the same reason, the sentence tokenizer should just\n",
      "     |      split the paragraph at line breaks.\n",
      "     |    - There is a new 'chapters()' method that returns chapters instead\n",
      "     |      instead of paragraphs.\n",
      "     |    - The 'paras()' method inherited from PlaintextCorpusReader is\n",
      "     |      made non-functional to remove any confusion between chapters\n",
      "     |      and paragraphs for Europarl.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EuroparlCorpusReader\n",
      "     |      PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  chapters(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          chapters, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=56), sent_tokenizer=<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x1078a1c50>, para_block_reader=<function read_blankline_block at 0x1078aac80>, encoding='utf8')\n",
      "     |      Construct a new plaintext corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n",
      "     |          >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param word_tokenizer: Tokenizer for breaking sentences or\n",
      "     |          paragraphs into words.\n",
      "     |      :param sent_tokenizer: Tokenizer for breaking paragraphs\n",
      "     |          into words.\n",
      "     |      :param para_block_reader: The block reader used to divide the\n",
      "     |          corpus into paragraph blocks.\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class FramenetCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  A corpus reader for the Framenet Corpus.\n",
      "     |  \n",
      "     |  >>> from nltk.corpus import framenet as fn\n",
      "     |  >>> fn.lu(3238).frame.lexUnit['glint.v'] is fn.lu(3238)\n",
      "     |  True\n",
      "     |  >>> fn.frame_by_name('Replacing') is fn.lus('replace.v')[0].frame\n",
      "     |  True\n",
      "     |  >>> fn.lus('prejudice.n')[0].frame.frameRelations == fn.frame_relations('Partiality')\n",
      "     |  True\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FramenetCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids)\n",
      "     |  \n",
      "     |  annotated_document(self, fn_docid)\n",
      "     |      Returns the annotated document whose id number is\n",
      "     |      ``fn_docid``. This id number can be obtained by calling the\n",
      "     |      Documents() function.\n",
      "     |      \n",
      "     |      The dict that is returned from this function will contain the\n",
      "     |      following keys:\n",
      "     |      \n",
      "     |      - '_type'      : 'fulltextannotation'\n",
      "     |      - 'sentence'   : a list of sentences in the document\n",
      "     |         - Each item in the list is a dict containing the following keys:\n",
      "     |            - 'ID'    : the ID number of the sentence\n",
      "     |            - '_type' : 'sentence'\n",
      "     |            - 'text'  : the text of the sentence\n",
      "     |            - 'paragNo' : the paragraph number\n",
      "     |            - 'sentNo'  : the sentence number\n",
      "     |            - 'docID'   : the document ID number\n",
      "     |            - 'corpID'  : the corpus ID number\n",
      "     |            - 'aPos'    : the annotation position\n",
      "     |            - 'annotationSet' : a list of annotation layers for the sentence\n",
      "     |               - Each item in the list is a dict containing the following keys:\n",
      "     |                  - 'ID'       : the ID number of the annotation set\n",
      "     |                  - '_type'    : 'annotationset'\n",
      "     |                  - 'status'   : either 'MANUAL' or 'UNANN'\n",
      "     |                  - 'luName'   : (only if status is 'MANUAL')\n",
      "     |                  - 'luID'     : (only if status is 'MANUAL')\n",
      "     |                  - 'frameID'  : (only if status is 'MANUAL')\n",
      "     |                  - 'frameName': (only if status is 'MANUAL')\n",
      "     |                  - 'layer' : a list of labels for the layer\n",
      "     |                     - Each item in the layer is a dict containing the\n",
      "     |                       following keys:\n",
      "     |                        - '_type': 'layer'\n",
      "     |                        - 'rank'\n",
      "     |                        - 'name'\n",
      "     |                        - 'label' : a list of labels in the layer\n",
      "     |                           - Each item is a dict containing the following keys:\n",
      "     |                              - 'start'\n",
      "     |                              - 'end'\n",
      "     |                              - 'name'\n",
      "     |                              - 'feID' (optional)\n",
      "     |      \n",
      "     |      :param fn_docid: The Framenet id number of the document\n",
      "     |      :type fn_docid: int\n",
      "     |      :return: Information about the annotated document\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  buildindexes(self)\n",
      "     |      Build the internal indexes to make look-ups faster.\n",
      "     |  \n",
      "     |  documents(self, name=None)\n",
      "     |      Return a list of the annotated documents in Framenet.\n",
      "     |      \n",
      "     |      Details for a specific annotated document can be obtained using this\n",
      "     |      class's annotated_document() function and pass it the value of the 'ID' field.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> len(fn.documents())\n",
      "     |      78\n",
      "     |      >>> set([x.corpname for x in fn.documents()])==set(['ANC', 'C-4', 'KBEval',                     'LUCorpus-v0.3', 'Miscellaneous', 'NTI', 'PropBank', 'QA', 'SemAnno'])\n",
      "     |      True\n",
      "     |      \n",
      "     |      :param name: A regular expression pattern used to search the\n",
      "     |          file name of each annotated document. The document's\n",
      "     |          file name contains the name of the corpus that the\n",
      "     |          document is from, followed by two underscores \"__\"\n",
      "     |          followed by the document name. So, for example, the\n",
      "     |          file name \"LUCorpus-v0.3__20000410_nyt-NEW.xml\" is\n",
      "     |          from the corpus named \"LUCorpus-v0.3\" and the\n",
      "     |          document name is \"20000410_nyt-NEW.xml\".\n",
      "     |      :type name: str\n",
      "     |      :return: A list of selected (or all) annotated documents\n",
      "     |      :rtype: list of dicts, where each dict object contains the following\n",
      "     |              keys:\n",
      "     |      \n",
      "     |              - 'name'\n",
      "     |              - 'ID'\n",
      "     |              - 'corpid'\n",
      "     |              - 'corpname'\n",
      "     |              - 'description'\n",
      "     |              - 'filename'\n",
      "     |  \n",
      "     |  fe_relations(self)\n",
      "     |      Obtain a list of frame element relations.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> ferels = fn.fe_relations()\n",
      "     |      >>> isinstance(ferels, list)\n",
      "     |      True\n",
      "     |      >>> len(ferels)\n",
      "     |      10020\n",
      "     |      >>> PrettyDict(ferels[0], breakLines=True)\n",
      "     |      {'ID': 14642,\n",
      "     |      '_type': 'ferelation',\n",
      "     |      'frameRelation': <Parent=Abounding_with -- Inheritance -> Child=Lively_place>,\n",
      "     |      'subFE': <fe ID=11370 name=Degree>,\n",
      "     |      'subFEName': 'Degree',\n",
      "     |      'subFrame': <frame ID=1904 name=Lively_place>,\n",
      "     |      'subID': 11370,\n",
      "     |      'supID': 2271,\n",
      "     |      'superFE': <fe ID=2271 name=Degree>,\n",
      "     |      'superFEName': 'Degree',\n",
      "     |      'superFrame': <frame ID=262 name=Abounding_with>,\n",
      "     |      'type': <framerelationtype ID=1 name=Inheritance>}\n",
      "     |      \n",
      "     |      :return: A list of all of the frame element relations in framenet\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  frame(self, fn_fid_or_fname, ignorekeys=[])\n",
      "     |      Get the details for the specified Frame using the frame's name\n",
      "     |      or id number.\n",
      "     |      \n",
      "     |      Usage examples:\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> f = fn.frame(256)\n",
      "     |      >>> f.name\n",
      "     |      'Medical_specialties'\n",
      "     |      >>> f = fn.frame('Medical_specialties')\n",
      "     |      >>> f.ID\n",
      "     |      256\n",
      "     |      >>> # ensure non-ASCII character in definition doesn't trigger an encoding error:\n",
      "     |      >>> fn.frame('Imposing_obligation')\n",
      "     |      frame (1494): Imposing_obligation...\n",
      "     |      \n",
      "     |      The dict that is returned from this function will contain the\n",
      "     |      following information about the Frame:\n",
      "     |      \n",
      "     |      - 'name'       : the name of the Frame (e.g. 'Birth', 'Apply_heat', etc.)\n",
      "     |      - 'definition' : textual definition of the Frame\n",
      "     |      - 'ID'         : the internal ID number of the Frame\n",
      "     |      - 'semTypes'   : a list of semantic types for this frame\n",
      "     |         - Each item in the list is a dict containing the following keys:\n",
      "     |            - 'name' : can be used with the semtype() function\n",
      "     |            - 'ID'   : can be used with the semtype() function\n",
      "     |      \n",
      "     |      - 'lexUnit'    : a dict containing all of the LUs for this frame.\n",
      "     |                       The keys in this dict are the names of the LUs and\n",
      "     |                       the value for each key is itself a dict containing\n",
      "     |                       info about the LU (see the lu() function for more info.)\n",
      "     |      \n",
      "     |      - 'FE' : a dict containing the Frame Elements that are part of this frame\n",
      "     |               The keys in this dict are the names of the FEs (e.g. 'Body_system')\n",
      "     |               and the values are dicts containing the following keys\n",
      "     |            - 'definition' : The definition of the FE\n",
      "     |            - 'name'       : The name of the FE e.g. 'Body_system'\n",
      "     |            - 'ID'         : The id number\n",
      "     |            - '_type'      : 'fe'\n",
      "     |            - 'abbrev'     : Abbreviation e.g. 'bod'\n",
      "     |            - 'coreType'   : one of \"Core\", \"Peripheral\", or \"Extra-Thematic\"\n",
      "     |            - 'semType'    : if not None, a dict with the following two keys:\n",
      "     |               - 'name' : name of the semantic type. can be used with\n",
      "     |                          the semtype() function\n",
      "     |               - 'ID'   : id number of the semantic type. can be used with\n",
      "     |                          the semtype() function\n",
      "     |            - 'requiresFE' : if not None, a dict with the following two keys:\n",
      "     |               - 'name' : the name of another FE in this frame\n",
      "     |               - 'ID'   : the id of the other FE in this frame\n",
      "     |            - 'excludesFE' : if not None, a dict with the following two keys:\n",
      "     |               - 'name' : the name of another FE in this frame\n",
      "     |               - 'ID'   : the id of the other FE in this frame\n",
      "     |      \n",
      "     |      - 'frameRelation'      : a list of objects describing frame relations\n",
      "     |      - 'FEcoreSets'  : a list of Frame Element core sets for this frame\n",
      "     |         - Each item in the list is a list of FE objects\n",
      "     |      \n",
      "     |      :param fn_fid_or_fname: The Framenet name or id number of the frame\n",
      "     |      :type fn_fid_or_fname: int or str\n",
      "     |      :param ignorekeys: The keys to ignore. These keys will not be\n",
      "     |          included in the output. (optional)\n",
      "     |      :type ignorekeys: list(str)\n",
      "     |      :return: Information about a frame\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  frame_by_id(self, fn_fid, ignorekeys=[])\n",
      "     |      Get the details for the specified Frame using the frame's id\n",
      "     |      number.\n",
      "     |      \n",
      "     |      Usage examples:\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> f = fn.frame_by_id(256)\n",
      "     |      >>> f.ID\n",
      "     |      256\n",
      "     |      >>> f.name\n",
      "     |      'Medical_specialties'\n",
      "     |      >>> f.definition\n",
      "     |      \"This frame includes words that name ...\"\n",
      "     |      \n",
      "     |      :param fn_fid: The Framenet id number of the frame\n",
      "     |      :type fn_fid: int\n",
      "     |      :param ignorekeys: The keys to ignore. These keys will not be\n",
      "     |          included in the output. (optional)\n",
      "     |      :type ignorekeys: list(str)\n",
      "     |      :return: Information about a frame\n",
      "     |      :rtype: dict\n",
      "     |      \n",
      "     |      Also see the ``frame()`` function for details about what is\n",
      "     |      contained in the dict that is returned.\n",
      "     |  \n",
      "     |  frame_by_name(self, fn_fname, ignorekeys=[], check_cache=True)\n",
      "     |      Get the details for the specified Frame using the frame's name.\n",
      "     |      \n",
      "     |      Usage examples:\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> f = fn.frame_by_name('Medical_specialties')\n",
      "     |      >>> f.ID\n",
      "     |      256\n",
      "     |      >>> f.name\n",
      "     |      'Medical_specialties'\n",
      "     |      >>> f.definition\n",
      "     |      \"This frame includes words that name ...\"\n",
      "     |      \n",
      "     |      :param fn_fname: The name of the frame\n",
      "     |      :type fn_fname: str\n",
      "     |      :param ignorekeys: The keys to ignore. These keys will not be\n",
      "     |          included in the output. (optional)\n",
      "     |      :type ignorekeys: list(str)\n",
      "     |      :return: Information about a frame\n",
      "     |      :rtype: dict\n",
      "     |      \n",
      "     |      Also see the ``frame()`` function for details about what is\n",
      "     |      contained in the dict that is returned.\n",
      "     |  \n",
      "     |  frame_ids_and_names(self, name=None)\n",
      "     |      Uses the frame index, which is much faster than looking up each frame definition\n",
      "     |      if only the names and IDs are needed.\n",
      "     |  \n",
      "     |  frame_relation_types(self)\n",
      "     |      Obtain a list of frame relation types.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> frts = list(fn.frame_relation_types())\n",
      "     |      >>> isinstance(frts, list)\n",
      "     |      True\n",
      "     |      >>> len(frts)\n",
      "     |      9\n",
      "     |      >>> PrettyDict(frts[0], breakLines=True)\n",
      "     |      {'ID': 1,\n",
      "     |       '_type': 'framerelationtype',\n",
      "     |       'frameRelations': [<Parent=Event -- Inheritance -> Child=Change_of_consistency>, <Parent=Event -- Inheritance -> Child=Rotting>, ...],\n",
      "     |       'name': 'Inheritance',\n",
      "     |       'subFrameName': 'Child',\n",
      "     |       'superFrameName': 'Parent'}\n",
      "     |      \n",
      "     |      :return: A list of all of the frame relation types in framenet\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  frame_relations(self, frame=None, frame2=None, type=None)\n",
      "     |      :param frame: (optional) frame object, name, or ID; only relations involving\n",
      "     |      this frame will be returned\n",
      "     |      :param frame2: (optional; 'frame' must be a different frame) only show relations\n",
      "     |      between the two specified frames, in either direction\n",
      "     |      :param type: (optional) frame relation type (name or object); show only relations\n",
      "     |      of this type\n",
      "     |      :type frame: int or str or AttrDict\n",
      "     |      :return: A list of all of the frame relations in framenet\n",
      "     |      :rtype: list(dict)\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> frels = fn.frame_relations()\n",
      "     |      >>> isinstance(frels, list)\n",
      "     |      True\n",
      "     |      >>> len(frels)\n",
      "     |      1676\n",
      "     |      >>> PrettyList(fn.frame_relations('Cooking_creation'), maxReprSize=0, breakLines=True)\n",
      "     |      [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>,\n",
      "     |       <Parent=Apply_heat -- Using -> Child=Cooking_creation>,\n",
      "     |       <MainEntry=Apply_heat -- See_also -> ReferringEntry=Cooking_creation>]\n",
      "     |      >>> PrettyList(fn.frame_relations(373), breakLines=True)\n",
      "     |      [<Parent=Topic -- Using -> Child=Communication>,\n",
      "     |       <Source=Discussion -- ReFraming_Mapping -> Target=Topic>, ...]\n",
      "     |      >>> PrettyList(fn.frame_relations(fn.frame('Cooking_creation')), breakLines=True)\n",
      "     |      [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>,\n",
      "     |       <Parent=Apply_heat -- Using -> Child=Cooking_creation>, ...]\n",
      "     |      >>> PrettyList(fn.frame_relations('Cooking_creation', type='Inheritance'))\n",
      "     |      [<Parent=Intentionally_create -- Inheritance -> Child=Cooking_creation>]\n",
      "     |      >>> PrettyList(fn.frame_relations('Cooking_creation', 'Apply_heat'), breakLines=True)\n",
      "     |      [<Parent=Apply_heat -- Using -> Child=Cooking_creation>,\n",
      "     |      <MainEntry=Apply_heat -- See_also -> ReferringEntry=Cooking_creation>]\n",
      "     |  \n",
      "     |  frames(self, name=None)\n",
      "     |      Obtain details for a specific frame.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> len(fn.frames())\n",
      "     |      1019\n",
      "     |      >>> PrettyList(fn.frames(r'(?i)medical'), maxReprSize=0, breakLines=True)\n",
      "     |      [<frame ID=256 name=Medical_specialties>,\n",
      "     |       <frame ID=257 name=Medical_instruments>,\n",
      "     |       <frame ID=255 name=Medical_professionals>,\n",
      "     |       <frame ID=239 name=Medical_conditions>]\n",
      "     |      \n",
      "     |      A brief intro to Frames (excerpted from \"FrameNet II: Extended\n",
      "     |      Theory and Practice\" by Ruppenhofer et. al., 2010):\n",
      "     |      \n",
      "     |      A Frame is a script-like conceptual structure that describes a\n",
      "     |      particular type of situation, object, or event along with the\n",
      "     |      participants and props that are needed for that Frame. For\n",
      "     |      example, the \"Apply_heat\" frame describes a common situation\n",
      "     |      involving a Cook, some Food, and a Heating_Instrument, and is\n",
      "     |      evoked by words such as bake, blanch, boil, broil, brown,\n",
      "     |      simmer, steam, etc.\n",
      "     |      \n",
      "     |      We call the roles of a Frame \"frame elements\" (FEs) and the\n",
      "     |      frame-evoking words are called \"lexical units\" (LUs).\n",
      "     |      \n",
      "     |      FrameNet includes relations between Frames. Several types of\n",
      "     |      relations are defined, of which the most important are:\n",
      "     |      \n",
      "     |         - Inheritance: An IS-A relation. The child frame is a subtype\n",
      "     |           of the parent frame, and each FE in the parent is bound to\n",
      "     |           a corresponding FE in the child. An example is the\n",
      "     |           \"Revenge\" frame which inherits from the\n",
      "     |           \"Rewards_and_punishments\" frame.\n",
      "     |      \n",
      "     |         - Using: The child frame presupposes the parent frame as\n",
      "     |           background, e.g the \"Speed\" frame \"uses\" (or presupposes)\n",
      "     |           the \"Motion\" frame; however, not all parent FEs need to be\n",
      "     |           bound to child FEs.\n",
      "     |      \n",
      "     |         - Subframe: The child frame is a subevent of a complex event\n",
      "     |           represented by the parent, e.g. the \"Criminal_process\" frame\n",
      "     |           has subframes of \"Arrest\", \"Arraignment\", \"Trial\", and\n",
      "     |           \"Sentencing\".\n",
      "     |      \n",
      "     |         - Perspective_on: The child frame provides a particular\n",
      "     |           perspective on an un-perspectivized parent frame. A pair of\n",
      "     |           examples consists of the \"Hiring\" and \"Get_a_job\" frames,\n",
      "     |           which perspectivize the \"Employment_start\" frame from the\n",
      "     |           Employer's and the Employee's point of view, respectively.\n",
      "     |      \n",
      "     |      :param name: A regular expression pattern used to match against\n",
      "     |          Frame names. If 'name' is None, then a list of all\n",
      "     |          Framenet Frames will be returned.\n",
      "     |      :type name: str\n",
      "     |      :return: A list of matching Frames (or all Frames).\n",
      "     |      :rtype: list(AttrDict)\n",
      "     |  \n",
      "     |  frames_by_lemma(self, pat)\n",
      "     |      Returns a list of all frames that contain LUs in which the\n",
      "     |      ``name`` attribute of the LU matchs the given regular expression\n",
      "     |      ``pat``. Note that LU names are composed of \"lemma.POS\", where\n",
      "     |      the \"lemma\" part can be made up of either a single lexeme\n",
      "     |      (e.g. 'run') or multiple lexemes (e.g. 'a little').\n",
      "     |      \n",
      "     |      Note: if you are going to be doing a lot of this type of\n",
      "     |      searching, you'd want to build an index that maps from lemmas to\n",
      "     |      frames because each time frames_by_lemma() is called, it has to\n",
      "     |      search through ALL of the frame XML files in the db.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> fn.frames_by_lemma(r'(?i)a little')\n",
      "     |      [<frame ID=189 name=Quantity>, <frame ID=2001 name=Degree>]\n",
      "     |      \n",
      "     |      :return: A list of frame objects.\n",
      "     |      :rtype: list(AttrDict)\n",
      "     |  \n",
      "     |  lu(self, fn_luid, ignorekeys=[])\n",
      "     |      Get information about a specific Lexical Unit using the id number\n",
      "     |      ``fn_luid``. This function reads the LU information from the xml\n",
      "     |      file on disk each time it is called. You may want to cache this\n",
      "     |      info if you plan to call this function with the same id number\n",
      "     |      multiple times.\n",
      "     |      \n",
      "     |      Usage examples:\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> fn.lu(256).name\n",
      "     |      'foresee.v'\n",
      "     |      >>> fn.lu(256).definition\n",
      "     |      'COD: be aware of beforehand; predict.'\n",
      "     |      >>> fn.lu(256).frame.name\n",
      "     |      'Expectation'\n",
      "     |      >>> pprint(list(map(PrettyDict, fn.lu(256).lexemes)))\n",
      "     |      [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}]\n",
      "     |      \n",
      "     |      The dict that is returned from this function will contain most of the\n",
      "     |      following information about the LU. Note that some LUs do not contain\n",
      "     |      all of these pieces of information - particularly 'totalAnnotated' and\n",
      "     |      'incorporatedFE' may be missing in some LUs:\n",
      "     |      \n",
      "     |      - 'name'       : the name of the LU (e.g. 'merger.n')\n",
      "     |      - 'definition' : textual definition of the LU\n",
      "     |      - 'ID'         : the internal ID number of the LU\n",
      "     |      - '_type'      : 'lu'\n",
      "     |      - 'status'     : e.g. 'Created'\n",
      "     |      - 'frame'      : Frame that this LU belongs to\n",
      "     |      - 'POS'        : the part of speech of this LU (e.g. 'N')\n",
      "     |      - 'totalAnnotated' : total number of examples annotated with this LU\n",
      "     |      - 'incorporatedFE' : FE that incorporates this LU (e.g. 'Ailment')\n",
      "     |      - 'sentenceCount'  : a dict with the following two keys:\n",
      "     |               - 'annotated': number of sentences annotated with this LU\n",
      "     |               - 'total'    : total number of sentences with this LU\n",
      "     |      \n",
      "     |      - 'lexemes'  : a list of dicts describing the lemma of this LU.\n",
      "     |         Each dict in the list contains these keys:\n",
      "     |         - 'POS'     : part of speech e.g. 'N'\n",
      "     |         - 'name'    : either single-lexeme e.g. 'merger' or\n",
      "     |                       multi-lexeme e.g. 'a little'\n",
      "     |         - 'order': the order of the lexeme in the lemma (starting from 1)\n",
      "     |         - 'headword': a boolean ('true' or 'false')\n",
      "     |         - 'breakBefore': Can this lexeme be separated from the previous lexeme?\n",
      "     |              Consider: \"take over.v\" as in:\n",
      "     |                       Germany took over the Netherlands in 2 days.\n",
      "     |                       Germany took the Netherlands over in 2 days.\n",
      "     |              In this case, 'breakBefore' would be \"true\" for the lexeme\n",
      "     |              \"over\". Contrast this with \"take after.v\" as in:\n",
      "     |                       Mary takes after her grandmother.\n",
      "     |                      *Mary takes her grandmother after.\n",
      "     |              In this case, 'breakBefore' would be \"false\" for the lexeme \"after\"\n",
      "     |      \n",
      "     |      - 'lemmaID'    : Can be used to connect lemmas in different LUs\n",
      "     |      - 'semTypes'   : a list of semantic type objects for this LU\n",
      "     |      - 'subCorpus'  : a list of subcorpora\n",
      "     |         - Each item in the list is a dict containing the following keys:\n",
      "     |            - 'name' :\n",
      "     |            - 'sentence' : a list of sentences in the subcorpus\n",
      "     |               - each item in the list is a dict with the following keys:\n",
      "     |                  - 'ID':\n",
      "     |                  - 'sentNo':\n",
      "     |                  - 'text': the text of the sentence\n",
      "     |                  - 'aPos':\n",
      "     |                  - 'annotationSet': a list of annotation sets\n",
      "     |                     - each item in the list is a dict with the following keys:\n",
      "     |                        - 'ID':\n",
      "     |                        - 'status':\n",
      "     |                        - 'layer': a list of layers\n",
      "     |                           - each layer is a dict containing the following keys:\n",
      "     |                              - 'name': layer name (e.g. 'BNC')\n",
      "     |                              - 'rank':\n",
      "     |                              - 'label': a list of labels for the layer\n",
      "     |                                 - each label is a dict containing the following keys:\n",
      "     |                                    - 'start': start pos of label in sentence 'text' (0-based)\n",
      "     |                                    - 'end': end pos of label in sentence 'text' (0-based)\n",
      "     |                                    - 'name': name of label (e.g. 'NN1')\n",
      "     |      \n",
      "     |      Under the hood, this implementation looks up the lexical unit information\n",
      "     |      in the *frame* definition file. That file does not contain\n",
      "     |      corpus annotations, so the LU files will be accessed on demand if those are\n",
      "     |      needed. In principle, valence patterns could be loaded here too,\n",
      "     |      though these are not currently supported.\n",
      "     |      \n",
      "     |      :param fn_luid: The id number of the lexical unit\n",
      "     |      :type fn_luid: int\n",
      "     |      :param ignorekeys: The keys to ignore. These keys will not be\n",
      "     |          included in the output. (optional)\n",
      "     |      :type ignorekeys: list(str)\n",
      "     |      :return: All information about the lexical unit\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  lu_basic(self, fn_luid)\n",
      "     |      Returns basic information about the LU whose id is\n",
      "     |      ``fn_luid``. This is basically just a wrapper around the\n",
      "     |      ``lu()`` function with \"subCorpus\" info excluded.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> PrettyDict(fn.lu_basic(256), breakLines=True)\n",
      "     |      {'ID': 256,\n",
      "     |       'POS': 'V',\n",
      "     |       '_type': 'lu',\n",
      "     |       'definition': 'COD: be aware of beforehand; predict.',\n",
      "     |       'frame': <frame ID=26 name=Expectation>,\n",
      "     |       'lemmaID': 15082,\n",
      "     |       'lexemes': [{'POS': 'V', 'breakBefore': 'false', 'headword': 'false', 'name': 'foresee', 'order': 1}],\n",
      "     |       'name': 'foresee.v',\n",
      "     |       'semTypes': [],\n",
      "     |       'sentenceCount': {'annotated': 44, 'total': 227},\n",
      "     |       'status': 'FN1_Sent'}\n",
      "     |      \n",
      "     |      :param fn_luid: The id number of the desired LU\n",
      "     |      :type fn_luid: int\n",
      "     |      :return: Basic information about the lexical unit\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  lu_ids_and_names(self, name=None)\n",
      "     |      Uses the LU index, which is much faster than looking up each LU definition\n",
      "     |      if only the names and IDs are needed.\n",
      "     |  \n",
      "     |  lus(self, name=None)\n",
      "     |      Obtain details for a specific lexical unit.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> len(fn.lus())\n",
      "     |      11829\n",
      "     |      >>> PrettyList(fn.lus(r'(?i)a little'), maxReprSize=0, breakLines=True)\n",
      "     |      [<lu ID=14744 name=a little bit.adv>,\n",
      "     |       <lu ID=14733 name=a little.n>,\n",
      "     |       <lu ID=14743 name=a little.adv>]\n",
      "     |      \n",
      "     |      A brief intro to Lexical Units (excerpted from \"FrameNet II:\n",
      "     |      Extended Theory and Practice\" by Ruppenhofer et. al., 2010):\n",
      "     |      \n",
      "     |      A lexical unit (LU) is a pairing of a word with a meaning. For\n",
      "     |      example, the \"Apply_heat\" Frame describes a common situation\n",
      "     |      involving a Cook, some Food, and a Heating Instrument, and is\n",
      "     |      _evoked_ by words such as bake, blanch, boil, broil, brown,\n",
      "     |      simmer, steam, etc. These frame-evoking words are the LUs in the\n",
      "     |      Apply_heat frame. Each sense of a polysemous word is a different\n",
      "     |      LU.\n",
      "     |      \n",
      "     |      We have used the word \"word\" in talking about LUs. The reality\n",
      "     |      is actually rather complex. When we say that the word \"bake\" is\n",
      "     |      polysemous, we mean that the lemma \"bake.v\" (which has the\n",
      "     |      word-forms \"bake\", \"bakes\", \"baked\", and \"baking\") is linked to\n",
      "     |      three different frames:\n",
      "     |      \n",
      "     |         - Apply_heat: \"Michelle baked the potatoes for 45 minutes.\"\n",
      "     |      \n",
      "     |         - Cooking_creation: \"Michelle baked her mother a cake for her birthday.\"\n",
      "     |      \n",
      "     |         - Absorb_heat: \"The potatoes have to bake for more than 30 minutes.\"\n",
      "     |      \n",
      "     |      These constitute three different LUs, with different\n",
      "     |      definitions.\n",
      "     |      \n",
      "     |      Multiword expressions such as \"given name\" and hyphenated words\n",
      "     |      like \"shut-eye\" can also be LUs. Idiomatic phrases such as\n",
      "     |      \"middle of nowhere\" and \"give the slip (to)\" are also defined as\n",
      "     |      LUs in the appropriate frames (\"Isolated_places\" and \"Evading\",\n",
      "     |      respectively), and their internal structure is not analyzed.\n",
      "     |      \n",
      "     |      Framenet provides multiple annotated examples of each sense of a\n",
      "     |      word (i.e. each LU).  Moreover, the set of examples\n",
      "     |      (approximately 20 per LU) illustrates all of the combinatorial\n",
      "     |      possibilities of the lexical unit.\n",
      "     |      \n",
      "     |      Each LU is linked to a Frame, and hence to the other words which\n",
      "     |      evoke that Frame. This makes the FrameNet database similar to a\n",
      "     |      thesaurus, grouping together semantically similar words.\n",
      "     |      \n",
      "     |      In the simplest case, frame-evoking words are verbs such as\n",
      "     |      \"fried\" in:\n",
      "     |      \n",
      "     |         \"Matilde fried the catfish in a heavy iron skillet.\"\n",
      "     |      \n",
      "     |      Sometimes event nouns may evoke a Frame. For example,\n",
      "     |      \"reduction\" evokes \"Cause_change_of_scalar_position\" in:\n",
      "     |      \n",
      "     |         \"...the reduction of debt levels to $665 million from $2.6 billion.\"\n",
      "     |      \n",
      "     |      Adjectives may also evoke a Frame. For example, \"asleep\" may\n",
      "     |      evoke the \"Sleep\" frame as in:\n",
      "     |      \n",
      "     |         \"They were asleep for hours.\"\n",
      "     |      \n",
      "     |      Many common nouns, such as artifacts like \"hat\" or \"tower\",\n",
      "     |      typically serve as dependents rather than clearly evoking their\n",
      "     |      own frames.\n",
      "     |      \n",
      "     |      :param name: A regular expression pattern used to search the LU\n",
      "     |          names. Note that LU names take the form of a dotted\n",
      "     |          string (e.g. \"run.v\" or \"a little.adv\") in which a\n",
      "     |          lemma preceeds the \".\" and a POS follows the\n",
      "     |          dot. The lemma may be composed of a single lexeme\n",
      "     |          (e.g. \"run\") or of multiple lexemes (e.g. \"a\n",
      "     |          little\"). If 'name' is not given, then all LUs will\n",
      "     |          be returned.\n",
      "     |      \n",
      "     |          The valid POSes are:\n",
      "     |      \n",
      "     |                 v    - verb\n",
      "     |                 n    - noun\n",
      "     |                 a    - adjective\n",
      "     |                 adv  - adverb\n",
      "     |                 prep - preposition\n",
      "     |                 num  - numbers\n",
      "     |                 intj - interjection\n",
      "     |                 art  - article\n",
      "     |                 c    - conjunction\n",
      "     |                 scon - subordinating conjunction\n",
      "     |      \n",
      "     |      :type name: str\n",
      "     |      :return: A list of selected (or all) lexical units\n",
      "     |      :rtype: list of LU objects (dicts). See the lu() function for info\n",
      "     |        about the specifics of LU objects.\n",
      "     |  \n",
      "     |  propagate_semtypes(self)\n",
      "     |      Apply inference rules to distribute semtypes over relations between FEs.\n",
      "     |      For FrameNet 1.5, this results in 1011 semtypes being propagated.\n",
      "     |      (Not done by default because it requires loading all frame files,\n",
      "     |      which takes several seconds. If this needed to be fast, it could be rewritten\n",
      "     |      to traverse the neighboring relations on demand for each FE semtype.)\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)\n",
      "     |      4241\n",
      "     |      >>> fn.propagate_semtypes()\n",
      "     |      >>> sum(1 for f in fn.frames() for fe in f.FE.values() if fe.semType)\n",
      "     |      5252\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README.txt (or README) file.\n",
      "     |  \n",
      "     |  semtype(self, key)\n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> fn.semtype(233).name\n",
      "     |      'Temperature'\n",
      "     |      >>> fn.semtype(233).abbrev\n",
      "     |      'Temp'\n",
      "     |      >>> fn.semtype('Temperature').ID\n",
      "     |      233\n",
      "     |      \n",
      "     |      :param key: The name, abbreviation, or id number of the semantic type\n",
      "     |      :type key: string or int\n",
      "     |      :return: Information about a semantic type\n",
      "     |      :rtype: dict\n",
      "     |  \n",
      "     |  semtype_inherits(self, st, superST)\n",
      "     |  \n",
      "     |  semtypes(self)\n",
      "     |      Obtain a list of semantic types.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import framenet as fn\n",
      "     |      >>> stypes = fn.semtypes()\n",
      "     |      >>> len(stypes)\n",
      "     |      73\n",
      "     |      >>> sorted(stypes[0].keys())\n",
      "     |      ['ID', '_type', 'abbrev', 'definition', 'name', 'rootType', 'subTypes', 'superType']\n",
      "     |      \n",
      "     |      :return: A list of all of the semantic types in framenet\n",
      "     |      :rtype: list(dict)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileid=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class IEERCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      IEERCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  docs(self, fileids=None)\n",
      "     |  \n",
      "     |  parsed_docs(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class IPIPANCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Corpus reader designed to work with corpus created by IPI PAN.\n",
      "     |  See http://korpus.pl/en/ for more details about IPI PAN corpus.\n",
      "     |  \n",
      "     |  The corpus includes information about text domain, channel and categories.\n",
      "     |  You can access possible values using ``domains()``, ``channels()`` and\n",
      "     |  ``categories()``. You can use also this metadata to filter files, e.g.:\n",
      "     |  ``fileids(channel='prasa')``, ``fileids(categories='publicystyczny')``.\n",
      "     |  \n",
      "     |  The reader supports methods: words, sents, paras and their tagged versions.\n",
      "     |  You can get part of speech instead of full tag by giving \"simplify_tags=True\"\n",
      "     |  parameter, e.g.: ``tagged_sents(simplify_tags=True)``.\n",
      "     |  \n",
      "     |  Also you can get all tags disambiguated tags specifying parameter\n",
      "     |  \"one_tag=False\", e.g.: ``tagged_paras(one_tag=False)``.\n",
      "     |  \n",
      "     |  You can get all tags that were assigned by a morphological analyzer specifying\n",
      "     |  parameter \"disamb_only=False\", e.g. ``tagged_words(disamb_only=False)``.\n",
      "     |  \n",
      "     |  The IPIPAN Corpus contains tags indicating if there is a space between two\n",
      "     |  tokens. To add special \"no space\" markers, you should specify parameter\n",
      "     |  \"append_no_space=True\", e.g. ``tagged_words(append_no_space=True)``.\n",
      "     |  As a result in place where there should be no space between two tokens new\n",
      "     |  pair ('', 'no-space') will be inserted (for tagged data) and just '' for\n",
      "     |  methods without tags.\n",
      "     |  \n",
      "     |  The corpus reader can also try to append spaces between words. To enable this\n",
      "     |  option, specify parameter \"append_space=True\", e.g. ``words(append_space=True)``.\n",
      "     |  As a result either ' ' or (' ', 'space') will be inserted between tokens.\n",
      "     |  \n",
      "     |  By default, xml entities like &quot; and &amp; are replaced by corresponding\n",
      "     |  characters. You can turn off this feature, specifying parameter\n",
      "     |  \"replace_xmlentities=False\", e.g. ``words(replace_xmlentities=False)``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IPIPANCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids)\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |  \n",
      "     |  channels(self, fileids=None)\n",
      "     |  \n",
      "     |  domains(self, fileids=None)\n",
      "     |  \n",
      "     |  fileids(self, channels=None, domains=None, categories=None)\n",
      "     |  \n",
      "     |  paras(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  words(self, fileids=None, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class IndianCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  List of words, one per line.  Blank lines are ignored.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IndianCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class KNBCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      "     |  This class implements:\n",
      "     |    - ``__init__``, which specifies the location of the corpus\n",
      "     |      and a method for detecting the sentence blocks in corpus files.\n",
      "     |    - ``_read_block``, which reads a block from the input stream.\n",
      "     |    - ``_word``, which takes a block and returns a list of list of words.\n",
      "     |    - ``_tag``, which takes a block and returns a list of list of tagged\n",
      "     |      words.\n",
      "     |    - ``_parse``, which takes a block and returns a list of parsed\n",
      "     |      sentences.\n",
      "     |  \n",
      "     |  The structure of tagged words:\n",
      "     |    tagged_word = (word(str), tags(tuple))\n",
      "     |    tags = (surface, reading, lemma, pos1, posid1, pos2, posid2, pos3, posid3, others ...)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KNBCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', morphs2str=<function <lambda> at 0x107ba2c80>)\n",
      "     |      Initialize KNBCorpusReader\n",
      "     |      morphs2str is a function to convert morphlist to str for tree representation\n",
      "     |      for _parse()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class LinThesaurusCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Wrapper for the LISP-formatted thesauruses distributed by Dekang Lin.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LinThesaurusCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, ngram)\n",
      "     |      Determines whether or not the given ngram is in the thesaurus.\n",
      "     |      \n",
      "     |      :param ngram: ngram to lookup\n",
      "     |      :type ngram: C{string}\n",
      "     |      :return: whether the given ngram is in the thesaurus.\n",
      "     |  \n",
      "     |  __init__(self, root, badscore=0.0)\n",
      "     |      Initialize the thesaurus.\n",
      "     |      \n",
      "     |      :param root: root directory containing thesaurus LISP files\n",
      "     |      :type root: C{string}\n",
      "     |      :param badscore: the score to give to words which do not appear in each other's sets of synonyms\n",
      "     |      :type badscore: C{float}\n",
      "     |  \n",
      "     |  scored_synonyms(self, ngram, fileid=None)\n",
      "     |      Returns a list of scored synonyms (tuples of synonyms and scores) for the current ngram\n",
      "     |      \n",
      "     |      :param ngram: ngram to lookup\n",
      "     |      :type ngram: C{string}\n",
      "     |      :param fileid: thesaurus fileid to search in. If None, search all fileids.\n",
      "     |      :type fileid: C{string}\n",
      "     |      :return: If fileid is specified, list of tuples of scores and synonyms; otherwise,\n",
      "     |               list of tuples of fileids and lists, where inner lists consist of tuples of\n",
      "     |               scores and synonyms.\n",
      "     |  \n",
      "     |  similarity(self, ngram1, ngram2, fileid=None)\n",
      "     |      Returns the similarity score for two ngrams.\n",
      "     |      \n",
      "     |      :param ngram1: first ngram to compare\n",
      "     |      :type ngram1: C{string}\n",
      "     |      :param ngram2: second ngram to compare\n",
      "     |      :type ngram2: C{string}\n",
      "     |      :param fileid: thesaurus fileid to search in. If None, search all fileids.\n",
      "     |      :type fileid: C{string}\n",
      "     |      :return: If fileid is specified, just the score for the two ngrams; otherwise,\n",
      "     |               list of tuples of fileids and scores.\n",
      "     |  \n",
      "     |  synonyms(self, ngram, fileid=None)\n",
      "     |      Returns a list of synonyms for the current ngram.\n",
      "     |      \n",
      "     |      :param ngram: ngram to lookup\n",
      "     |      :type ngram: C{string}\n",
      "     |      :param fileid: thesaurus fileid to search in. If None, search all fileids.\n",
      "     |      :type fileid: C{string}\n",
      "     |      :return: If fileid is specified, list of synonyms; otherwise, list of tuples of fileids and\n",
      "     |               lists, where inner lists contain synonyms.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class MacMorphoCorpusReader(TaggedCorpusReader)\n",
      "     |  A corpus reader for the MAC_MORPHO corpus.  Each line contains a\n",
      "     |  single tagged word, using '_' as a separator.  Sentence boundaries\n",
      "     |  are based on the end-sentence tag ('_.').  Paragraph information\n",
      "     |  is not included in the corpus, so each paragraph returned by\n",
      "     |  ``self.paras()`` and ``self.tagged_paras()`` contains a single\n",
      "     |  sentence.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MacMorphoCorpusReader\n",
      "     |      TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TaggedCorpusReader:\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(list(tuple(str,str))))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class NPSChatCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      NPSChatCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wrap_etree=False, tagset=None)\n",
      "     |  \n",
      "     |  posts(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_posts(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  xml_posts(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class NombankCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Corpus reader for the nombank corpus, which augments the Penn\n",
      "     |  Treebank with information about the predicate argument structure\n",
      "     |  of every noun instance.  The corpus consists of two parts: the\n",
      "     |  predicate-argument annotations themselves, and a set of \"frameset\n",
      "     |  files\" which define the argument labels used by the annotations,\n",
      "     |  on a per-noun basis.  Each \"frameset file\" contains one or more\n",
      "     |  predicates, such as ``'turn'`` or ``'turn_on'``, each of which is\n",
      "     |  divided into coarse-grained word senses called \"rolesets\".  For\n",
      "     |  each \"roleset\", the frameset file provides descriptions of the\n",
      "     |  argument roles, along with examples.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NombankCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, nomfile, framefiles='', nounsfile=None, parse_fileid_xform=None, parse_corpus=None, encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param nomfile: The name of the file containing the predicate-\n",
      "     |          argument annotations (relative to ``root``).\n",
      "     |      :param framefiles: A list or regexp specifying the frameset\n",
      "     |          fileids for this corpus.\n",
      "     |      :param parse_fileid_xform: A transform that should be applied\n",
      "     |          to the fileids in this corpus.  This should be a function\n",
      "     |          of one argument (a fileid) that returns a string (the new\n",
      "     |          fileid).\n",
      "     |      :param parse_corpus: The corpus containing the parse trees\n",
      "     |          corresponding to this corpus.  These parse trees are\n",
      "     |          necessary to resolve the tree pointers used by nombank.\n",
      "     |  \n",
      "     |  instances(self, baseform=None)\n",
      "     |      :return: a corpus view that acts as a list of\n",
      "     |      ``NombankInstance`` objects, one for each noun in the corpus.\n",
      "     |  \n",
      "     |  lines(self)\n",
      "     |      :return: a corpus view that acts as a list of strings, one for\n",
      "     |      each line in the predicate-argument annotation file.\n",
      "     |  \n",
      "     |  nouns(self)\n",
      "     |      :return: a corpus view that acts as a list of all noun lemmas\n",
      "     |      in this corpus (from the nombank.1.0.words file).\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the text contents of the given fileids, as a single string.\n",
      "     |  \n",
      "     |  roleset(self, roleset_id)\n",
      "     |      :return: the xml description for the given roleset.\n",
      "     |  \n",
      "     |  rolesets(self, baseform=None)\n",
      "     |      :return: list of xml descriptions for rolesets.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PPAttachmentCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  sentence_id verb noun1 preposition noun2 attachment\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PPAttachmentCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  attachments(self, fileids)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  tuples(self, fileids)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class Pl196xCorpusReader(nltk.corpus.reader.api.CategorizedCorpusReader, nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      Pl196xCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  decode_tag(self, tag)\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  textids(self, fileids=None, categories=None)\n",
      "     |      In the pl196x corpus each category is stored in single\n",
      "     |      file and thus both methods provide identical functionality. In order\n",
      "     |      to accommodate finer granularity, a non-standard textids() method was\n",
      "     |      implemented. All the main functions can be supplied with a list\n",
      "     |      of required chunks---giving much more control to the user.\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None, textids=None)\n",
      "     |  \n",
      "     |  xml(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  headLen = 2770\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PlaintextCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Reader for corpora that consist of plaintext documents.  Paragraphs\n",
      "     |  are assumed to be split using blank lines.  Sentences and words can\n",
      "     |  be tokenized using the default tokenizers, or by custom tokenizers\n",
      "     |  specificed as parameters to the constructor.\n",
      "     |  \n",
      "     |  This corpus reader can be customized (e.g., to skip preface\n",
      "     |  sections of specific document formats) by creating a subclass and\n",
      "     |  overriding the ``CorpusView`` class variable.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, word_tokenizer=WordPunctTokenizer(pattern='\\\\w+|[^\\\\w\\\\s]+', gaps=False, discard_empty=True, flags=56), sent_tokenizer=<nltk.tokenize.punkt.PunktSentenceTokenizer object at 0x1078a1c50>, para_block_reader=<function read_blankline_block at 0x1078aac80>, encoding='utf8')\n",
      "     |      Construct a new plaintext corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/usr/local/share/nltk_data/corpora/webtext/'\n",
      "     |          >>> reader = PlaintextCorpusReader(root, '.*\\.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param word_tokenizer: Tokenizer for breaking sentences or\n",
      "     |          paragraphs into words.\n",
      "     |      :param sent_tokenizer: Tokenizer for breaking paragraphs\n",
      "     |          into words.\n",
      "     |      :param para_block_reader: The block reader used to divide the\n",
      "     |          corpus into paragraph blocks.\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PortugueseCategorizedPlaintextCorpusReader(CategorizedPlaintextCorpusReader)\n",
      "     |  # is there a better way?\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PortugueseCategorizedPlaintextCorpusReader\n",
      "     |      CategorizedPlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CategorizedCorpusReader\n",
      "     |      PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CategorizedPlaintextCorpusReader:\n",
      "     |  \n",
      "     |  paras(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None, categories=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  categories(self, fileids=None)\n",
      "     |      Return a list of the categories that are defined for this corpus,\n",
      "     |      or for the file(s) if it is given.\n",
      "     |  \n",
      "     |  fileids(self, categories=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that make up the given category(s) if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CategorizedCorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class PropbankCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Corpus reader for the propbank corpus, which augments the Penn\n",
      "     |  Treebank with information about the predicate argument structure\n",
      "     |  of every verb instance.  The corpus consists of two parts: the\n",
      "     |  predicate-argument annotations themselves, and a set of \"frameset\n",
      "     |  files\" which define the argument labels used by the annotations,\n",
      "     |  on a per-verb basis.  Each \"frameset file\" contains one or more\n",
      "     |  predicates, such as ``'turn'`` or ``'turn_on'``, each of which is\n",
      "     |  divided into coarse-grained word senses called \"rolesets\".  For\n",
      "     |  each \"roleset\", the frameset file provides descriptions of the\n",
      "     |  argument roles, along with examples.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PropbankCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, propfile, framefiles='', verbsfile=None, parse_fileid_xform=None, parse_corpus=None, encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param propfile: The name of the file containing the predicate-\n",
      "     |          argument annotations (relative to ``root``).\n",
      "     |      :param framefiles: A list or regexp specifying the frameset\n",
      "     |          fileids for this corpus.\n",
      "     |      :param parse_fileid_xform: A transform that should be applied\n",
      "     |          to the fileids in this corpus.  This should be a function\n",
      "     |          of one argument (a fileid) that returns a string (the new\n",
      "     |          fileid).\n",
      "     |      :param parse_corpus: The corpus containing the parse trees\n",
      "     |          corresponding to this corpus.  These parse trees are\n",
      "     |          necessary to resolve the tree pointers used by propbank.\n",
      "     |  \n",
      "     |  instances(self, baseform=None)\n",
      "     |      :return: a corpus view that acts as a list of\n",
      "     |      ``PropBankInstance`` objects, one for each noun in the corpus.\n",
      "     |  \n",
      "     |  lines(self)\n",
      "     |      :return: a corpus view that acts as a list of strings, one for\n",
      "     |      each line in the predicate-argument annotation file.\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the text contents of the given fileids, as a single string.\n",
      "     |  \n",
      "     |  roleset(self, roleset_id)\n",
      "     |      :return: the xml description for the given roleset.\n",
      "     |  \n",
      "     |  rolesets(self, baseform=None)\n",
      "     |      :return: list of xml descriptions for rolesets.\n",
      "     |  \n",
      "     |  verbs(self)\n",
      "     |      :return: a corpus view that acts as a list of all verb lemmas\n",
      "     |      in this corpus (from the verbs.txt file).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class RTECorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  Corpus reader for corpora in RTE challenges.\n",
      "     |  \n",
      "     |  This is just a wrapper around the XMLCorpusReader. See module docstring above for the expected\n",
      "     |  structure of input documents.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RTECorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  pairs(self, fileids)\n",
      "     |      Build a list of RTEPairs from a RTE corpus.\n",
      "     |      \n",
      "     |      :param fileids: a list of RTE corpus fileids\n",
      "     |      :type: list\n",
      "     |      :rtype: list(RTEPair)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wrap_etree=False)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileid=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SemcorCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  Corpus reader for the SemCor Corpus.\n",
      "     |  For access to the complete XML data structure, use the ``xml()``\n",
      "     |  method.  For access to simple word lists and tagged word lists, use\n",
      "     |  ``words()``, ``sents()``, ``tagged_words()``, and ``tagged_sents()``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SemcorCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wordnet, lazy=True)\n",
      "     |  \n",
      "     |  chunk_sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of sentences, each encoded\n",
      "     |          as a list of chunks.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  chunks(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of chunks,\n",
      "     |          each of which is a list of words and punctuation symbols\n",
      "     |          that form a unit.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of sentences, each encoded\n",
      "     |          as a list of word strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_chunks(self, fileids=None, tag='pos')\n",
      "     |      :return: the given file(s) as a list of tagged chunks, represented\n",
      "     |          in tree form.\n",
      "     |      :rtype: list(Tree)\n",
      "     |      \n",
      "     |      :param tag: `'pos'` (part of speech), `'sem'` (semantic), or `'both'`\n",
      "     |          to indicate the kind of tags to include.  Semantic tags consist of\n",
      "     |          WordNet lemma IDs, plus an `'NE'` node if the chunk is a named entity\n",
      "     |          without a specific entry in WordNet.  (Named entities of type 'other'\n",
      "     |          have no lemma.  Other chunks not in WordNet have no semantic tag.\n",
      "     |          Punctuation tokens have `None` for their part of speech tag.)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tag='pos')\n",
      "     |      :return: the given file(s) as a list of sentences. Each sentence\n",
      "     |          is represented as a list of tagged chunks (in tree form).\n",
      "     |      :rtype: list(list(Tree))\n",
      "     |      \n",
      "     |      :param tag: `'pos'` (part of speech), `'sem'` (semantic), or `'both'`\n",
      "     |          to indicate the kind of tags to include.  Semantic tags consist of\n",
      "     |          WordNet lemma IDs, plus an `'NE'` node if the chunk is a named entity\n",
      "     |          without a specific entry in WordNet.  (Named entities of type 'other'\n",
      "     |          have no lemma.  Other chunks not in WordNet have no semantic tag.\n",
      "     |          Punctuation tokens have `None` for their part of speech tag.)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SensevalCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      SensevalCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  instances(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the text contents of the given fileids, as a single string.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SentiSynset(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, pos_score, neg_score, synset)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __str__(self)\n",
      "     |      Prints just the Pos/Neg scores for now.\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self)\n",
      "     |  \n",
      "     |  neg_score(self)\n",
      "     |  \n",
      "     |  obj_score(self)\n",
      "     |  \n",
      "     |  pos_score(self)\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SentiWordNetCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      SentiWordNetCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf-8')\n",
      "     |      Construct a new SentiWordNet Corpus Reader, using data from\n",
      "     |      the specified file.\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  all_senti_synsets(self)\n",
      "     |  \n",
      "     |  senti_synset(self, *vals)\n",
      "     |  \n",
      "     |  senti_synsets(self, string, pos=None)\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SinicaTreebankCorpusReader(nltk.corpus.reader.api.SyntaxCorpusReader)\n",
      "     |  Reader for the sinica treebank.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SinicaTreebankCorpusReader\n",
      "     |      nltk.corpus.reader.api.SyntaxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from nltk.corpus.reader.api.SyntaxCorpusReader:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class StringCategoryCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  # [xx] Should the order of the tuple be reversed -- in most other places\n",
      "     |  # in nltk, we use the form (data, tag) -- e.g., tagged words and\n",
      "     |  # labeled texts for classifiers.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StringCategoryCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, delimiter=' ', encoding='utf8')\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |      :param delimiter: Field delimiter\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the text contents of the given fileids, as a single string.\n",
      "     |  \n",
      "     |  tuples(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SwadeshCorpusReader(WordListCorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      SwadeshCorpusReader\n",
      "     |      WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  entries(self, fileids=None)\n",
      "     |      :return: a tuple of words for the specified fileids.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from WordListCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SwitchboardCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      SwitchboardCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, tagset=None)\n",
      "     |  \n",
      "     |  discourses(self)\n",
      "     |  \n",
      "     |  tagged_discourses(self, tagset=False)\n",
      "     |  \n",
      "     |  tagged_turns(self, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, tagset=None)\n",
      "     |  \n",
      "     |  turns(self)\n",
      "     |  \n",
      "     |  words(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class SyntaxCorpusReader(CorpusReader)\n",
      "     |  An abstract base class for reading corpora consisting of\n",
      "     |  syntactically parsed text.  Subclasses should define:\n",
      "     |  \n",
      "     |    - ``__init__``, which specifies the location of the corpus\n",
      "     |      and a method for detecting the sentence blocks in corpus files.\n",
      "     |    - ``_read_block``, which reads a block from the input stream.\n",
      "     |    - ``_word``, which takes a block and returns a list of list of words.\n",
      "     |    - ``_tag``, which takes a block and returns a list of list of tagged\n",
      "     |      words.\n",
      "     |    - ``_parse``, which takes a block and returns a list of parsed\n",
      "     |      sentences.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SyntaxCorpusReader\n",
      "     |      CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  parsed_sents(self, fileids=None)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class TEICorpusView(nltk.corpus.reader.util.StreamBackedCorpusView)\n",
      "     |  Method resolution order:\n",
      "     |      TEICorpusView\n",
      "     |      nltk.corpus.reader.util.StreamBackedCorpusView\n",
      "     |      nltk.util.AbstractLazySequence\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, corpus_file, tagged, group_by_sent, group_by_para, tagset=None, headLen=0, textids=None)\n",
      "     |  \n",
      "     |  read_block(self, stream)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.util.StreamBackedCorpusView:\n",
      "     |  \n",
      "     |  __add__(self, other)\n",
      "     |      # Use concat for these, so we can use a ConcatenatedCorpusView\n",
      "     |      # when possible.\n",
      "     |  \n",
      "     |  __getitem__(self, i)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __mul__(self, count)\n",
      "     |  \n",
      "     |  __radd__(self, other)\n",
      "     |  \n",
      "     |  __rmul__(self, count)\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Close the file stream associated with this corpus view.  This\n",
      "     |      can be useful if you are worried about running out of file\n",
      "     |      handles (although the stream should automatically be closed\n",
      "     |      upon garbage collection of the corpus view).  If the corpus\n",
      "     |      view is accessed after it is closed, it will be automatically\n",
      "     |      re-opened.\n",
      "     |  \n",
      "     |  iterate_from(self, start_tok)\n",
      "     |      # If we wanted to be thread-safe, then this method would need to\n",
      "     |      # do some locking.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.util.StreamBackedCorpusView:\n",
      "     |  \n",
      "     |  fileid\n",
      "     |      The fileid of the file that is accessed by this view.\n",
      "     |      \n",
      "     |      :type: str or PathPointer\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.util.AbstractLazySequence:\n",
      "     |  \n",
      "     |  __contains__(self, value)\n",
      "     |      Return true if this list contains ``value``.\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      :raise ValueError: Corpus view objects are unhashable.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return an iterator that generates the tokens in the corpus\n",
      "     |      file underlying this corpus view.\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return a string representation for this corpus view that is\n",
      "     |      similar to a list's representation; but if it would be more\n",
      "     |      than 60 characters long, it is truncated.\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  count(self, value)\n",
      "     |      Return the number of times this list contains ``value``.\n",
      "     |  \n",
      "     |  index(self, value, start=None, stop=None)\n",
      "     |      Return the index of the first occurrence of ``value`` in this\n",
      "     |      list that is greater than or equal to ``start`` and less than\n",
      "     |      ``stop``.  Negative start and stop values are treated like negative\n",
      "     |      slice bounds -- i.e., they count from the end of the list.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |      Return a string representation for this corpus view that is\n",
      "     |      similar to a list's representation; but if it would be more\n",
      "     |      than 60 characters long, it is truncated.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.util.AbstractLazySequence:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TaggedCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Reader for simple part-of-speech tagged corpora.  Paragraphs are\n",
      "     |  assumed to be split using blank lines.  Sentences and words can be\n",
      "     |  tokenized using the default tokenizers, or by custom tokenizers\n",
      "     |  specified as parameters to the constructor.  Words are parsed\n",
      "     |  using ``nltk.tag.str2tuple``.  By default, ``'/'`` is used as the\n",
      "     |  separator.  I.e., words should have the form::\n",
      "     |  \n",
      "     |     word1/tag1 word2/tag2 word3/tag3 ...\n",
      "     |  \n",
      "     |  But custom separators may be specified as parameters to the\n",
      "     |  constructor.  Part of speech tags are case-normalized to upper\n",
      "     |  case.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, sep='/', word_tokenizer=WhitespaceTokenizer(pattern='\\\\s+', gaps=True, discard_empty=True, flags=56), sent_tokenizer=RegexpTokenizer(pattern='\\n', gaps=True, discard_empty=True, flags=56), para_block_reader=<function read_blankline_block at 0x1078aac80>, encoding='utf8', tagset=None)\n",
      "     |      Construct a new Tagged Corpus reader for a set of documents\n",
      "     |      located at the given root directory.  Example usage:\n",
      "     |      \n",
      "     |          >>> root = '/...path to corpus.../'\n",
      "     |          >>> reader = TaggedCorpusReader(root, '.*', '.txt') # doctest: +SKIP\n",
      "     |      \n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |      :param fileids: A list or regexp specifying the fileids in this corpus.\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_paras(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of ``(word,tag)`` tuples.\n",
      "     |      :rtype: list(list(list(tuple(str,str))))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class TimitCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Reader for the TIMIT corpus (or any other corpus with the same\n",
      "     |  file layout and use of file formats).  The corpus root directory\n",
      "     |  should contain the following files:\n",
      "     |  \n",
      "     |    - timitdic.txt: dictionary of standard transcriptions\n",
      "     |    - spkrinfo.txt: table of speaker information\n",
      "     |  \n",
      "     |  In addition, the root directory should contain one subdirectory\n",
      "     |  for each speaker, containing three files for each utterance:\n",
      "     |  \n",
      "     |    - <utterance-id>.txt: text content of utterances\n",
      "     |    - <utterance-id>.wrd: tokenized text content of utterances\n",
      "     |    - <utterance-id>.phn: phonetic transcription of utterances\n",
      "     |    - <utterance-id>.wav: utterance sound file\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimitCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, encoding='utf8')\n",
      "     |      Construct a new TIMIT corpus reader in the given directory.\n",
      "     |      :param root: The root directory for this corpus.\n",
      "     |  \n",
      "     |  audiodata(self, utterance, start=0, end=None)\n",
      "     |  \n",
      "     |  fileids(self, filetype=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus.\n",
      "     |      \n",
      "     |      :param filetype: If specified, then ``filetype`` indicates that\n",
      "     |          only the files that have the given type should be\n",
      "     |          returned.  Accepted values are: ``txt``, ``wrd``, ``phn``,\n",
      "     |          ``wav``, or ``metadata``,\n",
      "     |  \n",
      "     |  phone_times(self, utterances=None)\n",
      "     |      offset is represented as a number of 16kHz samples!\n",
      "     |  \n",
      "     |  phone_trees(self, utterances=None)\n",
      "     |  \n",
      "     |  phones(self, utterances=None)\n",
      "     |  \n",
      "     |  play(self, utterance, start=0, end=None)\n",
      "     |      Play the given audio sample.\n",
      "     |      \n",
      "     |      :param utterance: The utterance id of the sample to play\n",
      "     |  \n",
      "     |  sent_times(self, utterances=None)\n",
      "     |  \n",
      "     |  sentid(self, utterance)\n",
      "     |  \n",
      "     |  sents(self, utterances=None)\n",
      "     |  \n",
      "     |  spkrid(self, utterance)\n",
      "     |  \n",
      "     |  spkrinfo(self, speaker)\n",
      "     |      :return: A dictionary mapping .. something.\n",
      "     |  \n",
      "     |  spkrutteranceids(self, speaker)\n",
      "     |      :return: A list of all utterances associated with a given\n",
      "     |      speaker.\n",
      "     |  \n",
      "     |  transcription_dict(self)\n",
      "     |      :return: A dictionary giving the 'standard' transcription for\n",
      "     |      each word.\n",
      "     |  \n",
      "     |  utterance(self, spkrid, sentid)\n",
      "     |  \n",
      "     |  utteranceids(self, dialect=None, sex=None, spkrid=None, sent_type=None, sentid=None)\n",
      "     |      :return: A list of the utterance identifiers for all\n",
      "     |      utterances in this corpus, or for the given speaker, dialect\n",
      "     |      region, gender, sentence type, or sentence number, if\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  wav(self, utterance, start=0, end=None)\n",
      "     |      # [xx] NOTE: This is currently broken -- we're assuming that the\n",
      "     |      # fileids are WAV fileids (aka RIFF), but they're actually NIST SPHERE\n",
      "     |      # fileids.\n",
      "     |  \n",
      "     |  word_times(self, utterances=None)\n",
      "     |  \n",
      "     |  words(self, utterances=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class TimitTaggedCorpusReader(TaggedCorpusReader)\n",
      "     |  A corpus reader for tagged sentences that are included in the TIMIT corpus.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimitTaggedCorpusReader\n",
      "     |      TaggedCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  paras(self)\n",
      "     |  \n",
      "     |  tagged_paras(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TaggedCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  tagged_sents(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences, each encoded as a list of ``(word,tag)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(list(tuple(str,str)))\n",
      "     |  \n",
      "     |  tagged_words(self, fileids=None, tagset=None)\n",
      "     |      :return: the given file(s) as a list of tagged\n",
      "     |          words and punctuation symbols, encoded as tuples\n",
      "     |          ``(word,tag)``.\n",
      "     |      :rtype: list(tuple(str,str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class ToolboxCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      ToolboxCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  entries(self, fileids, **kwargs)\n",
      "     |      # should probably be done lazily:\n",
      "     |  \n",
      "     |  fields(self, fileids, strip=True, unwrap=True, encoding='utf8', errors='strict', unicode_fields=None)\n",
      "     |  \n",
      "     |  raw(self, fileids)\n",
      "     |  \n",
      "     |  words(self, fileids, key='lx')\n",
      "     |  \n",
      "     |  xml(self, fileids, key=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class UdhrCorpusReader(nltk.corpus.reader.plaintext.PlaintextCorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      UdhrCorpusReader\n",
      "     |      nltk.corpus.reader.plaintext.PlaintextCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root='udhr')\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ENCODINGS = [('.*-Latin1$', 'latin-1'), ('.*-Hebrew$', 'hebrew'), ('.*...\n",
      "     |  \n",
      "     |  SKIP = set(['Amharic-Afenegus6..60375', 'Armenian-DallakHelv', 'Azeri_...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.plaintext.PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  paras(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          paragraphs, each encoded as a list of sentences, which are\n",
      "     |          in turn encoded as lists of word strings.\n",
      "     |      :rtype: list(list(list(str)))\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |      :return: the given file(s) as a single string.\n",
      "     |      :rtype: str\n",
      "     |  \n",
      "     |  sents(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of\n",
      "     |          sentences or utterances, each encoded as a list of word\n",
      "     |          strings.\n",
      "     |      :rtype: list(list(str))\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |      :return: the given file(s) as a list of words\n",
      "     |          and punctuation symbols.\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from nltk.corpus.reader.plaintext.PlaintextCorpusReader:\n",
      "     |  \n",
      "     |  CorpusView = <class 'nltk.corpus.reader.util.StreamBackedCorpusView'>\n",
      "     |      A 'view' of a corpus file, which acts like a sequence of tokens:\n",
      "     |      it can be accessed by index, iterated over, etc.  However, the\n",
      "     |      tokens are only constructed as-needed -- the entire corpus is\n",
      "     |      never stored in memory at once.\n",
      "     |      \n",
      "     |      The constructor to ``StreamBackedCorpusView`` takes two arguments:\n",
      "     |      a corpus fileid (specified as a string or as a ``PathPointer``);\n",
      "     |      and a block reader.  A \"block reader\" is a function that reads\n",
      "     |      zero or more tokens from a stream, and returns them as a list.  A\n",
      "     |      very simple example of a block reader is:\n",
      "     |      \n",
      "     |          >>> def simple_block_reader(stream):\n",
      "     |          ...     return stream.readline().split()\n",
      "     |      \n",
      "     |      This simple block reader reads a single line at a time, and\n",
      "     |      returns a single token (consisting of a string) for each\n",
      "     |      whitespace-separated substring on the line.\n",
      "     |      \n",
      "     |      When deciding how to define the block reader for a given\n",
      "     |      corpus, careful consideration should be given to the size of\n",
      "     |      blocks handled by the block reader.  Smaller block sizes will\n",
      "     |      increase the memory requirements of the corpus view's internal\n",
      "     |      data structures (by 2 integers per block).  On the other hand,\n",
      "     |      larger block sizes may decrease performance for random access to\n",
      "     |      the corpus.  (But note that larger block sizes will *not*\n",
      "     |      decrease performance for iteration.)\n",
      "     |      \n",
      "     |      Internally, ``CorpusView`` maintains a partial mapping from token\n",
      "     |      index to file position, with one entry per block.  When a token\n",
      "     |      with a given index *i* is requested, the ``CorpusView`` constructs\n",
      "     |      it as follows:\n",
      "     |      \n",
      "     |        1. First, it searches the toknum/filepos mapping for the token\n",
      "     |           index closest to (but less than or equal to) *i*.\n",
      "     |      \n",
      "     |        2. Then, starting at the file position corresponding to that\n",
      "     |           index, it reads one block at a time using the block reader\n",
      "     |           until it reaches the requested token.\n",
      "     |      \n",
      "     |      The toknum/filepos mapping is created lazily: it is initially\n",
      "     |      empty, but every time a new block is read, the block's\n",
      "     |      initial token is added to the mapping.  (Thus, the toknum/filepos\n",
      "     |      map has one entry per block.)\n",
      "     |      \n",
      "     |      In order to increase efficiency for random access patterns that\n",
      "     |      have high degrees of locality, the corpus view may cache one or\n",
      "     |      more blocks.\n",
      "     |      \n",
      "     |      :note: Each ``CorpusView`` object internally maintains an open file\n",
      "     |          object for its underlying corpus file.  This file should be\n",
      "     |          automatically closed when the ``CorpusView`` is garbage collected,\n",
      "     |          but if you wish to close it manually, use the ``close()``\n",
      "     |          method.  If you access a ``CorpusView``'s items after it has been\n",
      "     |          closed, the file object will be automatically re-opened.\n",
      "     |      \n",
      "     |      :warning: If the contents of the file are modified during the\n",
      "     |          lifetime of the ``CorpusView``, then the ``CorpusView``'s behavior\n",
      "     |          is undefined.\n",
      "     |      \n",
      "     |      :warning: If a unicode encoding is specified when constructing a\n",
      "     |          ``CorpusView``, then the block reader may only call\n",
      "     |          ``stream.seek()`` with offsets that have been returned by\n",
      "     |          ``stream.tell()``; in particular, calling ``stream.seek()`` with\n",
      "     |          relative offsets, or with offsets based on string lengths, may\n",
      "     |          lead to incorrect behavior.\n",
      "     |      \n",
      "     |      :ivar _block_reader: The function used to read\n",
      "     |          a single block from the underlying file stream.\n",
      "     |      :ivar _toknum: A list containing the token index of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          token index of the first token in block ``i``.  Together\n",
      "     |          with ``_filepos``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _filepos: A list containing the file position of each block\n",
      "     |          that has been processed.  In particular, ``_toknum[i]`` is the\n",
      "     |          file position of the first character in block ``i``.  Together\n",
      "     |          with ``_toknum``, this forms a partial mapping between token\n",
      "     |          indices and file positions.\n",
      "     |      :ivar _stream: The stream used to access the underlying corpus file.\n",
      "     |      :ivar _len: The total number of tokens in the corpus, if known;\n",
      "     |          or None, if the number of tokens is not yet known.\n",
      "     |      :ivar _eofpos: The character position of the last character in the\n",
      "     |          file.  This is calculated when the corpus view is initialized,\n",
      "     |          and is used to decide when the end of file has been reached.\n",
      "     |      :ivar _cache: A cache of the most recently read block.  It\n",
      "     |         is encoded as a tuple (start_toknum, end_toknum, tokens), where\n",
      "     |         start_toknum is the token index of the first token in the block;\n",
      "     |         end_toknum is the token index of the first token not in the\n",
      "     |         block; and tokens is a list of the tokens in the block.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class VerbnetCorpusReader(nltk.corpus.reader.xmldocs.XMLCorpusReader)\n",
      "     |  Method resolution order:\n",
      "     |      VerbnetCorpusReader\n",
      "     |      nltk.corpus.reader.xmldocs.XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wrap_etree=False)\n",
      "     |      # No unicode encoding param, since the data files are all XML.\n",
      "     |  \n",
      "     |  classids(self, lemma=None, wordnetid=None, fileid=None, classid=None)\n",
      "     |      Return a list of the verbnet class identifiers.  If a file\n",
      "     |      identifier is specified, then return only the verbnet class\n",
      "     |      identifiers for classes (and subclasses) defined by that file.\n",
      "     |      If a lemma is specified, then return only verbnet class\n",
      "     |      identifiers for classes that contain that lemma as a member.\n",
      "     |      If a wordnetid is specified, then return only identifiers for\n",
      "     |      classes that contain that wordnetid as a member.  If a classid\n",
      "     |      is specified, then return only identifiers for subclasses of\n",
      "     |      the specified verbnet class.\n",
      "     |  \n",
      "     |  fileids(self, vnclass_ids=None)\n",
      "     |      Return a list of fileids that make up this corpus.  If\n",
      "     |      ``vnclass_ids`` is specified, then return the fileids that make\n",
      "     |      up the specified verbnet class(es).\n",
      "     |  \n",
      "     |  lemmas(self, classid=None)\n",
      "     |      Return a list of all verb lemmas that appear in any class, or\n",
      "     |      in the ``classid`` if specified.\n",
      "     |  \n",
      "     |  longid(self, shortid)\n",
      "     |      Given a short verbnet class identifier (eg '37.10'), map it\n",
      "     |      to a long id (eg 'confess-37.10').  If ``shortid`` is already a\n",
      "     |      long id, then return it as-is\n",
      "     |  \n",
      "     |  pprint(self, vnclass)\n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given verbnet class.\n",
      "     |      \n",
      "     |      :param vnclass: A verbnet class identifier; or an ElementTree\n",
      "     |      containing the xml contents of a verbnet class.\n",
      "     |  \n",
      "     |  pprint_description(self, vnframe, indent='')\n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given verbnet frame description.\n",
      "     |      \n",
      "     |      :param vnframe: An ElementTree containing the xml contents of\n",
      "     |          a verbnet frame.\n",
      "     |  \n",
      "     |  pprint_frame(self, vnframe, indent='')\n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given verbnet frame.\n",
      "     |      \n",
      "     |      :param vnframe: An ElementTree containing the xml contents of\n",
      "     |          a verbnet frame.\n",
      "     |  \n",
      "     |  pprint_members(self, vnclass, indent='')\n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given verbnet class's member verbs.\n",
      "     |      \n",
      "     |      :param vnclass: A verbnet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a verbnet class.\n",
      "     |  \n",
      "     |  pprint_semantics(self, vnframe, indent='')\n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given verbnet frame semantics.\n",
      "     |      \n",
      "     |      :param vnframe: An ElementTree containing the xml contents of\n",
      "     |          a verbnet frame.\n",
      "     |  \n",
      "     |  pprint_subclasses(self, vnclass, indent='')\n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given verbnet class's subclasses.\n",
      "     |      \n",
      "     |      :param vnclass: A verbnet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a verbnet class.\n",
      "     |  \n",
      "     |  pprint_syntax(self, vnframe, indent='')\n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given verbnet frame syntax.\n",
      "     |      \n",
      "     |      :param vnframe: An ElementTree containing the xml contents of\n",
      "     |          a verbnet frame.\n",
      "     |  \n",
      "     |  pprint_themroles(self, vnclass, indent='')\n",
      "     |      Return a string containing a pretty-printed representation of\n",
      "     |      the given verbnet class's thematic roles.\n",
      "     |      \n",
      "     |      :param vnclass: A verbnet class identifier; or an ElementTree\n",
      "     |          containing the xml contents of a verbnet class.\n",
      "     |  \n",
      "     |  shortid(self, longid)\n",
      "     |      Given a long verbnet class identifier (eg 'confess-37.10'),\n",
      "     |      map it to a short id (eg '37.10').  If ``longid`` is already a\n",
      "     |      short id, then return it as-is.\n",
      "     |  \n",
      "     |  vnclass(self, fileid_or_classid)\n",
      "     |      Return an ElementTree containing the xml for the specified\n",
      "     |      verbnet class.\n",
      "     |      \n",
      "     |      :param fileid_or_classid: An identifier specifying which class\n",
      "     |          should be returned.  Can be a file identifier (such as\n",
      "     |          ``'put-9.1.xml'``), or a verbnet class identifier (such as\n",
      "     |          ``'put-9.1'``) or a short verbnet class identifier (such as\n",
      "     |          ``'9.1'``).\n",
      "     |  \n",
      "     |  wordnetids(self, classid=None)\n",
      "     |      Return a list of all wordnet identifiers that appear in any\n",
      "     |      class, or in ``classid`` if specified.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.xmldocs.XMLCorpusReader:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileid=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class WordListCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  List of words, one per line.  Blank lines are ignored.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WordListCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileids=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, encoding='utf8', tagset=None)\n",
      "     |      :type root: PathPointer or str\n",
      "     |      :param root: A path pointer identifying the root directory for\n",
      "     |          this corpus.  If a string is specified, then it will be\n",
      "     |          converted to a ``PathPointer`` automatically.\n",
      "     |      :param fileids: A list of the files that make up this corpus.\n",
      "     |          This list can either be specified explicitly, as a list of\n",
      "     |          strings; or implicitly, as a regular expression over file\n",
      "     |          paths.  The absolute path for each file will be constructed\n",
      "     |          by joining the reader's root to each file name.\n",
      "     |      :param encoding: The default unicode encoding for the files\n",
      "     |          that make up the corpus.  The value of ``encoding`` can be any\n",
      "     |          of the following:\n",
      "     |          - A string: ``encoding`` is the encoding name for all files.\n",
      "     |          - A dictionary: ``encoding[file_id]`` is the encoding\n",
      "     |            name for the file whose identifier is ``file_id``.  If\n",
      "     |            ``file_id`` is not in ``encoding``, then the file\n",
      "     |            contents will be processed using non-unicode byte strings.\n",
      "     |          - A list: ``encoding`` should be a list of ``(regexp, encoding)``\n",
      "     |            tuples.  The encoding for a file whose identifier is ``file_id``\n",
      "     |            will be the ``encoding`` value for the first tuple whose\n",
      "     |            ``regexp`` matches the ``file_id``.  If no tuple's ``regexp``\n",
      "     |            matches the ``file_id``, the file contents will be processed\n",
      "     |            using non-unicode byte strings.\n",
      "     |          - None: the file contents of all files will be\n",
      "     |            processed using non-unicode byte strings.\n",
      "     |      :param tagset: The name of the tagset used by this corpus, to be used\n",
      "     |            for normalizing or converting the POS tags returned by the\n",
      "     |            tagged_...() methods.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class WordNetCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  A corpus reader used to access wordnet or its variants.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WordNetCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, omw_reader)\n",
      "     |      Construct a new wordnet corpus reader, with the given root\n",
      "     |      directory.\n",
      "     |  \n",
      "     |  all_lemma_names(self, pos=None, lang='en')\n",
      "     |      Return all lemma names for all synsets for the given\n",
      "     |      part of speech tag and langauge or languages. If pos is not specified, all synsets\n",
      "     |      for all parts of speech will be used.\n",
      "     |  \n",
      "     |  all_synsets(self, pos=None)\n",
      "     |      Iterate over all synsets with a given part of speech tag.\n",
      "     |      If no pos is specified, all synsets for all parts of speech\n",
      "     |      will be loaded.\n",
      "     |  \n",
      "     |  get_version(self)\n",
      "     |  \n",
      "     |  ic(self, corpus, weight_senses_equally=False, smoothing=1.0)\n",
      "     |      Creates an information content lookup dictionary from a corpus.\n",
      "     |      \n",
      "     |      :type corpus: CorpusReader\n",
      "     |      :param corpus: The corpus from which we create an information\n",
      "     |      content dictionary.\n",
      "     |      :type weight_senses_equally: bool\n",
      "     |      :param weight_senses_equally: If this is True, gives all\n",
      "     |      possible senses equal weight rather than dividing by the\n",
      "     |      number of possible senses.  (If a word has 3 synses, each\n",
      "     |      sense gets 0.3333 per appearance when this is False, 1.0 when\n",
      "     |      it is true.)\n",
      "     |      :param smoothing: How much do we smooth synset counts (default is 1.0)\n",
      "     |      :type smoothing: float\n",
      "     |      :return: An information content dictionary\n",
      "     |  \n",
      "     |  jcn_similarity(self, synset1, synset2, ic, verbose=False)\n",
      "     |      Jiang-Conrath Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      "     |      ancestor node) and that of the two input Synsets. The relationship is\n",
      "     |      given by the equation 1 / (IC(s1) + IC(s2) - 2 * IC(lcs)).\n",
      "     |      \n",
      "     |      :type  other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type  ic: dict\n",
      "     |      :param ic: an information content object (as returned by ``nltk.corpus.wordnet_ic.ic()``).\n",
      "     |      :return: A float score denoting the similarity of the two ``Synset`` objects.\n",
      "     |  \n",
      "     |  langs(self)\n",
      "     |      return a list of languages supported by Multilingual Wordnet\n",
      "     |  \n",
      "     |  lch_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      "     |      Leacock Chodorow Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      shortest path that connects the senses (as above) and the maximum depth\n",
      "     |      of the taxonomy in which the senses occur. The relationship is given as\n",
      "     |      -log(p/2d) where p is the shortest path length and d is the taxonomy\n",
      "     |      depth.\n",
      "     |      \n",
      "     |      :type  other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type simulate_root: bool\n",
      "     |      :param simulate_root: The various verb taxonomies do not\n",
      "     |          share a single root which disallows this metric from working for\n",
      "     |          synsets that are not connected. This flag (True by default)\n",
      "     |          creates a fake root that connects all the taxonomies. Set it\n",
      "     |          to false to disable this behavior. For the noun taxonomy,\n",
      "     |          there is usually a default root except for WordNet version 1.6.\n",
      "     |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      "     |          as well.\n",
      "     |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      "     |          normally greater than 0. None is returned if no connecting path\n",
      "     |          could be found. If a ``Synset`` is compared with itself, the\n",
      "     |          maximum score is returned, which varies depending on the taxonomy\n",
      "     |          depth.\n",
      "     |  \n",
      "     |  lemma(self, name, lang='en')\n",
      "     |      Return lemma object that matches the name\n",
      "     |  \n",
      "     |  lemma_count(self, lemma)\n",
      "     |      Return the frequency count for this Lemma\n",
      "     |  \n",
      "     |  lemma_from_key(self, key)\n",
      "     |  \n",
      "     |  lemmas(self, lemma, pos=None, lang='en')\n",
      "     |      Return all Lemma objects with a name matching the specified lemma\n",
      "     |      name and part of speech tag. Matches any part of speech tag if none is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  lin_similarity(self, synset1, synset2, ic, verbose=False)\n",
      "     |      Lin Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      "     |      ancestor node) and that of the two input Synsets. The relationship is\n",
      "     |      given by the equation 2 * IC(lcs) / (IC(s1) + IC(s2)).\n",
      "     |      \n",
      "     |      :type other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type ic: dict\n",
      "     |      :param ic: an information content object (as returned by ``nltk.corpus.wordnet_ic.ic()``).\n",
      "     |      :return: A float score denoting the similarity of the two ``Synset`` objects,\n",
      "     |          in the range 0 to 1.\n",
      "     |  \n",
      "     |  morphy(self, form, pos=None)\n",
      "     |      Find a possible base form for the given form, with the given\n",
      "     |      part of speech, by checking WordNet's list of exceptional\n",
      "     |      forms, and by recursively stripping affixes for this part of\n",
      "     |      speech until a form in WordNet is found.\n",
      "     |      \n",
      "     |      >>> from nltk.corpus import wordnet as wn\n",
      "     |      >>> print(wn.morphy('dogs'))\n",
      "     |      dog\n",
      "     |      >>> print(wn.morphy('churches'))\n",
      "     |      church\n",
      "     |      >>> print(wn.morphy('aardwolves'))\n",
      "     |      aardwolf\n",
      "     |      >>> print(wn.morphy('abaci'))\n",
      "     |      abacus\n",
      "     |      >>> wn.morphy('hardrock', wn.ADV)\n",
      "     |      >>> print(wn.morphy('book', wn.NOUN))\n",
      "     |      book\n",
      "     |      >>> wn.morphy('book', wn.ADJ)\n",
      "     |  \n",
      "     |  of2ss(self, of)\n",
      "     |      take an id and return the synsets\n",
      "     |  \n",
      "     |  path_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      "     |      Path Distance Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      shortest path that connects the senses in the is-a (hypernym/hypnoym)\n",
      "     |      taxonomy. The score is in the range 0 to 1, except in those cases where\n",
      "     |      a path cannot be found (will only be true for verbs as there are many\n",
      "     |      distinct verb taxonomies), in which case None is returned. A score of\n",
      "     |      1 represents identity i.e. comparing a sense with itself will return 1.\n",
      "     |      \n",
      "     |      :type other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type simulate_root: bool\n",
      "     |      :param simulate_root: The various verb taxonomies do not\n",
      "     |          share a single root which disallows this metric from working for\n",
      "     |          synsets that are not connected. This flag (True by default)\n",
      "     |          creates a fake root that connects all the taxonomies. Set it\n",
      "     |          to false to disable this behavior. For the noun taxonomy,\n",
      "     |          there is usually a default root except for WordNet version 1.6.\n",
      "     |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      "     |          as well.\n",
      "     |      :return: A score denoting the similarity of the two ``Synset`` objects,\n",
      "     |          normally between 0 and 1. None is returned if no connecting path\n",
      "     |          could be found. 1 is returned if a ``Synset`` is compared with\n",
      "     |          itself.\n",
      "     |  \n",
      "     |  res_similarity(self, synset1, synset2, ic, verbose=False)\n",
      "     |      Resnik Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      Information Content (IC) of the Least Common Subsumer (most specific\n",
      "     |      ancestor node).\n",
      "     |      \n",
      "     |      :type  other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type ic: dict\n",
      "     |      :param ic: an information content object (as returned by ``nltk.corpus.wordnet_ic.ic()``).\n",
      "     |      :return: A float score denoting the similarity of the two ``Synset`` objects.\n",
      "     |          Synsets whose LCS is the root node of the taxonomy will have a\n",
      "     |          score of 0 (e.g. N['dog'][0] and N['table'][0]).\n",
      "     |  \n",
      "     |  ss2of(self, ss)\n",
      "     |      return the ILI of the synset\n",
      "     |  \n",
      "     |  synset(self, name)\n",
      "     |      #////////////////////////////////////////////////////////////\n",
      "     |      # Loading Synsets\n",
      "     |      #////////////////////////////////////////////////////////////\n",
      "     |  \n",
      "     |  synsets(self, lemma, pos=None, lang='en')\n",
      "     |      Load all synsets with a given lemma and part of speech tag.\n",
      "     |      If no pos is specified, all synsets for all parts of speech\n",
      "     |      will be loaded. \n",
      "     |      If lang is specified, all the synsets associated with the lemma name\n",
      "     |      of that language will be returned.\n",
      "     |  \n",
      "     |  wup_similarity(self, synset1, synset2, verbose=False, simulate_root=True)\n",
      "     |      Wu-Palmer Similarity:\n",
      "     |      Return a score denoting how similar two word senses are, based on the\n",
      "     |      depth of the two senses in the taxonomy and that of their Least Common\n",
      "     |      Subsumer (most specific ancestor node). Previously, the scores computed\n",
      "     |      by this implementation did _not_ always agree with those given by\n",
      "     |      Pedersen's Perl implementation of WordNet Similarity. However, with\n",
      "     |      the addition of the simulate_root flag (see below), the score for\n",
      "     |      verbs now almost always agree but not always for nouns.\n",
      "     |      \n",
      "     |      The LCS does not necessarily feature in the shortest path connecting\n",
      "     |      the two senses, as it is by definition the common ancestor deepest in\n",
      "     |      the taxonomy, not closest to the two senses. Typically, however, it\n",
      "     |      will so feature. Where multiple candidates for the LCS exist, that\n",
      "     |      whose shortest path to the root node is the longest will be selected.\n",
      "     |      Where the LCS has multiple paths to the root, the longer path is used\n",
      "     |      for the purposes of the calculation.\n",
      "     |      \n",
      "     |      :type  other: Synset\n",
      "     |      :param other: The ``Synset`` that this ``Synset`` is being compared to.\n",
      "     |      :type simulate_root: bool\n",
      "     |      :param simulate_root: The various verb taxonomies do not\n",
      "     |          share a single root which disallows this metric from working for\n",
      "     |          synsets that are not connected. This flag (True by default)\n",
      "     |          creates a fake root that connects all the taxonomies. Set it\n",
      "     |          to false to disable this behavior. For the noun taxonomy,\n",
      "     |          there is usually a default root except for WordNet version 1.6.\n",
      "     |          If you are using wordnet 1.6, a fake root will be added for nouns\n",
      "     |          as well.\n",
      "     |      :return: A float score denoting the similarity of the two ``Synset`` objects,\n",
      "     |          normally greater than zero. If no connecting path between the two\n",
      "     |          senses can be found, None is returned.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ADJ = 'a'\n",
      "     |  \n",
      "     |  ADJ_SAT = 's'\n",
      "     |  \n",
      "     |  ADV = 'r'\n",
      "     |  \n",
      "     |  MORPHOLOGICAL_SUBSTITUTIONS = {'a': [('er', ''), ('est', ''), ('er', '...\n",
      "     |  \n",
      "     |  NOUN = 'n'\n",
      "     |  \n",
      "     |  VERB = 'v'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class WordNetICCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  A corpus reader for the WordNet information content corpus.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WordNetICCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids)\n",
      "     |  \n",
      "     |  ic(self, icfile)\n",
      "     |      Load an information content file from the wordnet_ic corpus\n",
      "     |      and return a dictionary.  This dictionary has just two keys,\n",
      "     |      NOUN and VERB, whose values are dictionaries that map from\n",
      "     |      synsets to information content values.\n",
      "     |      \n",
      "     |      :type icfile: str\n",
      "     |      :param icfile: The name of the wordnet_ic file (e.g. \"ic-brown.dat\")\n",
      "     |      :return: An information content dictionary\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class XMLCorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Corpus reader for corpora whose documents are xml files.\n",
      "     |  \n",
      "     |  Note that the ``XMLCorpusReader`` constructor does not take an\n",
      "     |  ``encoding`` argument, because the unicode encoding is specified by\n",
      "     |  the XML files themselves.  See the XML specs for more info.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XMLCorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, fileids, wrap_etree=False)\n",
      "     |  \n",
      "     |  raw(self, fileids=None)\n",
      "     |  \n",
      "     |  words(self, fileid=None)\n",
      "     |      Returns all of the words and punctuation symbols in the specified file\n",
      "     |      that were in text nodes -- ie, tags are ignored. Like the xml() method,\n",
      "     |      fileid can only specify one file.\n",
      "     |      \n",
      "     |      :return: the given file's text nodes as a list of words and punctuation symbols\n",
      "     |      :rtype: list(str)\n",
      "     |  \n",
      "     |  xml(self, fileid=None)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  fileids(self)\n",
      "     |      Return a list of file identifiers for the fileids that make up\n",
      "     |      this corpus.\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "    \n",
      "    class YCOECorpusReader(nltk.corpus.reader.api.CorpusReader)\n",
      "     |  Corpus reader for the York-Toronto-Helsinki Parsed Corpus of Old\n",
      "     |  English Prose (YCOE), a 1.5 million word syntactically-annotated\n",
      "     |  corpus of Old English prose texts.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      YCOECorpusReader\n",
      "     |      nltk.corpus.reader.api.CorpusReader\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root, encoding='utf8')\n",
      "     |  \n",
      "     |  documents(self, fileids=None)\n",
      "     |      Return a list of document identifiers for all documents in\n",
      "     |      this corpus, or for the documents with the given file(s) if\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  fileids(self, documents=None)\n",
      "     |      Return a list of file identifiers for the files that make up\n",
      "     |      this corpus, or that store the given document(s) if specified.\n",
      "     |  \n",
      "     |  paras(self, documents=None)\n",
      "     |  \n",
      "     |  parsed_sents(self, documents=None)\n",
      "     |  \n",
      "     |  sents(self, documents=None)\n",
      "     |  \n",
      "     |  tagged_paras(self, documents=None)\n",
      "     |  \n",
      "     |  tagged_sents(self, documents=None)\n",
      "     |  \n",
      "     |  tagged_words(self, documents=None)\n",
      "     |  \n",
      "     |  words(self, documents=None)\n",
      "     |      # Delegate to one of our two sub-readers:\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |  \n",
      "     |  __unicode__ = __str__(self, /)\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  abspath(self, fileid)\n",
      "     |      Return the absolute path for the given file.\n",
      "     |      \n",
      "     |      :type file: str\n",
      "     |      :param file: The file identifier for the file whose path\n",
      "     |          should be returned.\n",
      "     |      :rtype: PathPointer\n",
      "     |  \n",
      "     |  abspaths(self, fileids=None, include_encoding=False, include_fileid=False)\n",
      "     |      Return a list of the absolute paths for all fileids in this corpus;\n",
      "     |      or for the given list of fileids, if specified.\n",
      "     |      \n",
      "     |      :type fileids: None or str or list\n",
      "     |      :param fileids: Specifies the set of fileids for which paths should\n",
      "     |          be returned.  Can be None, for all fileids; a list of\n",
      "     |          file identifiers, for a specified set of fileids; or a single\n",
      "     |          file identifier, for a single file.  Note that the return\n",
      "     |          value is always a list of paths, even if ``fileids`` is a\n",
      "     |          single file identifier.\n",
      "     |      \n",
      "     |      :param include_encoding: If true, then return a list of\n",
      "     |          ``(path_pointer, encoding)`` tuples.\n",
      "     |      \n",
      "     |      :rtype: list(PathPointer)\n",
      "     |  \n",
      "     |  encoding(self, file)\n",
      "     |      Return the unicode encoding for the given corpus file, if known.\n",
      "     |      If the encoding is unknown, or if the given file should be\n",
      "     |      processed using byte strings (str), then return None.\n",
      "     |  \n",
      "     |  ensure_loaded(self)\n",
      "     |      Load this corpus (if it has not already been loaded).  This is\n",
      "     |      used by LazyCorpusLoader as a simple method that can be used to\n",
      "     |      make sure a corpus is loaded -- e.g., in case a user wants to\n",
      "     |      do help(some_corpus).\n",
      "     |  \n",
      "     |  open(self, file)\n",
      "     |      Return an open stream that can be used to read the given file.\n",
      "     |      If the file's encoding is not None, then the stream will\n",
      "     |      automatically decode the file's contents into unicode.\n",
      "     |      \n",
      "     |      :param file: The file identifier of the file to read.\n",
      "     |  \n",
      "     |  readme(self)\n",
      "     |      Return the contents of the corpus README file, if it exists.\n",
      "     |  \n",
      "     |  unicode_repr = __repr__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from nltk.corpus.reader.api.CorpusReader:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  root\n",
      "     |      The directory where this corpus is stored.\n",
      "     |      \n",
      "     |      :type: PathPointer\n",
      "\n",
      "FUNCTIONS\n",
      "    find_corpus_fileids(root, regexp)\n",
      "    \n",
      "    tagged_treebank_para_block_reader(stream)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['CorpusReader', 'CategorizedCorpusReader', 'PlaintextCorpus...\n",
      "\n",
      "FILE\n",
      "    /Users/benkaufman/py3venv/lib/python3.4/site-packages/nltk/corpus/reader/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.corpus.reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading your own corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#test_output.txt#',\n",
       " '.DS_Store',\n",
       " '.ex.swp',\n",
       " '.git/COMMIT_EDITMSG',\n",
       " '.git/FETCH_HEAD',\n",
       " '.git/HEAD',\n",
       " '.git/ORIG_HEAD',\n",
       " '.git/config',\n",
       " '.git/description',\n",
       " '.git/hooks/applypatch-msg.sample',\n",
       " '.git/hooks/commit-msg.sample',\n",
       " '.git/hooks/post-update.sample',\n",
       " '.git/hooks/pre-applypatch.sample',\n",
       " '.git/hooks/pre-commit.sample',\n",
       " '.git/hooks/pre-push.sample',\n",
       " '.git/hooks/pre-rebase.sample',\n",
       " '.git/hooks/prepare-commit-msg.sample',\n",
       " '.git/hooks/update.sample',\n",
       " '.git/index',\n",
       " '.git/info/exclude',\n",
       " '.git/logs/HEAD',\n",
       " '.git/logs/refs/heads/dev-Spaces_upload_file',\n",
       " '.git/logs/refs/heads/dev-a-lot-of-tests',\n",
       " '.git/logs/refs/heads/dev-ansible-log-table-fix',\n",
       " '.git/logs/refs/heads/dev-async-more-tests',\n",
       " '.git/logs/refs/heads/dev-async-tests',\n",
       " '.git/logs/refs/heads/dev-bugs_20150713',\n",
       " '.git/logs/refs/heads/dev-cleanup',\n",
       " '.git/logs/refs/heads/dev-client-library',\n",
       " '.git/logs/refs/heads/dev-client-library-file-transaction',\n",
       " '.git/logs/refs/heads/dev-client-tests',\n",
       " '.git/logs/refs/heads/dev-file-fileinbox-tests',\n",
       " '.git/logs/refs/heads/dev-general-test-updates',\n",
       " '.git/logs/refs/heads/dev-json-schema-validation',\n",
       " '.git/logs/refs/heads/dev-manifest-tests',\n",
       " '.git/logs/refs/heads/dev-new-client-tests',\n",
       " '.git/logs/refs/heads/dev-quick-logger-fix',\n",
       " '.git/logs/refs/heads/dev-quick-script-change',\n",
       " '.git/logs/refs/heads/dev-role-perm-fix',\n",
       " '.git/logs/refs/heads/dev-rwh6-client-merge',\n",
       " '.git/logs/refs/heads/dev-rwh6_bugfixes',\n",
       " '.git/logs/refs/heads/dev-update_client',\n",
       " '.git/logs/refs/heads/master',\n",
       " '.git/logs/refs/heads/message_handler_unittests',\n",
       " '.git/logs/refs/remotes/origin/HEAD',\n",
       " '.git/logs/refs/remotes/origin/dev-JSON_schema',\n",
       " '.git/logs/refs/remotes/origin/dev-OAP_support',\n",
       " '.git/logs/refs/remotes/origin/dev-Spaces_auth0',\n",
       " '.git/logs/refs/remotes/origin/dev-Spaces_greenplum',\n",
       " '.git/logs/refs/remotes/origin/dev-Spaces_upload_file',\n",
       " '.git/logs/refs/remotes/origin/dev-a-lot-of-tests',\n",
       " '.git/logs/refs/remotes/origin/dev-add_setup_py',\n",
       " '.git/logs/refs/remotes/origin/dev-ansible',\n",
       " '.git/logs/refs/remotes/origin/dev-ansible-log-table-fix',\n",
       " '.git/logs/refs/remotes/origin/dev-async-more-tests',\n",
       " '.git/logs/refs/remotes/origin/dev-async-tests',\n",
       " '.git/logs/refs/remotes/origin/dev-auth_simple',\n",
       " '.git/logs/refs/remotes/origin/dev-aws',\n",
       " '.git/logs/refs/remotes/origin/dev-bugs_20150713',\n",
       " '.git/logs/refs/remotes/origin/dev-cleanup',\n",
       " '.git/logs/refs/remotes/origin/dev-client-library',\n",
       " '.git/logs/refs/remotes/origin/dev-client-library-file-transaction',\n",
       " '.git/logs/refs/remotes/origin/dev-client-tests',\n",
       " '.git/logs/refs/remotes/origin/dev-config',\n",
       " '.git/logs/refs/remotes/origin/dev-db_testing',\n",
       " '.git/logs/refs/remotes/origin/dev-file-fileinbox-tests',\n",
       " '.git/logs/refs/remotes/origin/dev-fix_imports',\n",
       " '.git/logs/refs/remotes/origin/dev-fix_script',\n",
       " '.git/logs/refs/remotes/origin/dev-fix_tests',\n",
       " '.git/logs/refs/remotes/origin/dev-fixes_20150701',\n",
       " '.git/logs/refs/remotes/origin/dev-general-test-updates',\n",
       " '.git/logs/refs/remotes/origin/dev-json-schema-validation',\n",
       " '.git/logs/refs/remotes/origin/dev-manifest-tests',\n",
       " '.git/logs/refs/remotes/origin/dev-mtu-1500',\n",
       " '.git/logs/refs/remotes/origin/dev-new-client-tests',\n",
       " '.git/logs/refs/remotes/origin/dev-new-client-tests-ferrants',\n",
       " '.git/logs/refs/remotes/origin/dev-oap_manifest_bugs',\n",
       " '.git/logs/refs/remotes/origin/dev-pylint',\n",
       " '.git/logs/refs/remotes/origin/dev-quick-logger-fix',\n",
       " '.git/logs/refs/remotes/origin/dev-quick-script-change',\n",
       " '.git/logs/refs/remotes/origin/dev-rds',\n",
       " '.git/logs/refs/remotes/origin/dev-role-perm-fix',\n",
       " '.git/logs/refs/remotes/origin/dev-rwh6-client-merge',\n",
       " '.git/logs/refs/remotes/origin/dev-rwh6_bugfixes',\n",
       " '.git/logs/refs/remotes/origin/dev-update_client',\n",
       " '.git/logs/refs/remotes/origin/master',\n",
       " '.git/logs/refs/remotes/origin/message_handler_unittests',\n",
       " '.git/logs/refs/remotes/origin/rel-0.1',\n",
       " '.git/logs/refs/remotes/origin/review_master',\n",
       " '.git/logs/refs/stash',\n",
       " '.git/objects/00/1934d2456241e49994638f48426b2fb255be52',\n",
       " '.git/objects/00/1a136af84bf0539d1df7752785df8e7dba287a',\n",
       " '.git/objects/00/2f1e7d7d524564a0fc6189872cea5e7894372e',\n",
       " '.git/objects/00/46cff0dbf28b3b3b4cd75f094cdc1cee06db3c',\n",
       " '.git/objects/00/4db17f902970b505a3f7e91250ce6451c73b41',\n",
       " '.git/objects/00/4f5a6c8a124e42f5ec57679c1f14c41bddde55',\n",
       " '.git/objects/00/56baed9aced5cee5d466bdf878a52ec6f4102b',\n",
       " '.git/objects/00/6074e0b16a5868d37f589a53ff77b52b776f67',\n",
       " '.git/objects/00/718a2ac0abe958e9081fd51ab0215e98ca679b',\n",
       " '.git/objects/00/87802bd59b069cbe7a2ca5569475b4ca224fa5',\n",
       " '.git/objects/00/9db6972aa045b0c3466b2b38508c23c3d04815',\n",
       " '.git/objects/00/c8b8a88f5ff2df4d699e8045e2ebf6d85f29a1',\n",
       " '.git/objects/00/cac18dbb09971f5b22710160bede4fdb42431d',\n",
       " '.git/objects/00/de33c8ac719fc3a34d267433bcda4888848a69',\n",
       " '.git/objects/00/e0ca7b0911838a5f8f4e8ca72152734b76679d',\n",
       " '.git/objects/00/e575684a2882630c99f9f659a7017681f2c540',\n",
       " '.git/objects/00/e8821c2a27dc3e6faf658549c5517897e44ca3',\n",
       " '.git/objects/00/ee40c2e37a6bbbab19259cb5b2970be8dbcef1',\n",
       " '.git/objects/00/eeeb53521c0f7254e0733972c3edf236849ba8',\n",
       " '.git/objects/01/03908419507d223a8480d95b03c638ac00dcb5',\n",
       " '.git/objects/01/03bfa917e1370dd215c7fd4c11ec362856aa22',\n",
       " '.git/objects/01/088260a70a2ae6bbf4b2eec4d39690968a01e5',\n",
       " '.git/objects/01/1fa8e374fb9f69e5f4936160eb04e7a6c228f5',\n",
       " '.git/objects/01/25eaee223b391fa0d78a46bcadd19377d49cfc',\n",
       " '.git/objects/01/4e58816132fee0ec934e2ed73fbb82bc13b1a5',\n",
       " '.git/objects/01/4fa9a9f885aff7d7589036601df145db4ba950',\n",
       " '.git/objects/01/5eb4ea2f0c8d03b048e6ac1c99aea2943e526c',\n",
       " '.git/objects/01/61acf74d869ae0b17e76dbd1956ca235e9d21e',\n",
       " '.git/objects/01/624ce3bcc3cc6d1fba41f0cd942c7e9233e06a',\n",
       " '.git/objects/01/87481d7b197701c9e14fa48400ff29b6fe8f58',\n",
       " '.git/objects/01/90f046b392e4cd43ce04f779056ee563ce42e4',\n",
       " '.git/objects/01/92082167f8a68954d8db5f41e9427fe0293c13',\n",
       " '.git/objects/01/a157b158eceb79393dc17f9d7938acd45dad6a',\n",
       " '.git/objects/01/a50bbdb49b80b076bc3daa079cc5985186d617',\n",
       " '.git/objects/01/a927d061ff7d83bbb6d86f039c0c437cdc499a',\n",
       " '.git/objects/01/b05b43eb8d54392d4994de4ff93dd5ef99ee42',\n",
       " '.git/objects/01/bf0fac82a426b1e3db1d215e65427750989d04',\n",
       " '.git/objects/01/c4dd1cccdd5c18345124163959c50d5d24d362',\n",
       " '.git/objects/01/ccf13fe7f7cab5b47fcc4e0642c66b4a203e98',\n",
       " '.git/objects/01/d04a47fb46eac1eae2f698cf629799eba17f5f',\n",
       " '.git/objects/01/d27f7fdd187f17dd62f10c4b58f8742b4fe3fb',\n",
       " '.git/objects/01/dd003b006963e447dbda16aefa8e55afbf5fbc',\n",
       " '.git/objects/01/e04333d65e031d611dce71b73c1f23e62fe221',\n",
       " '.git/objects/01/e9eb0768653b9ff0c059354785fd1bb6e4215c',\n",
       " '.git/objects/01/edcdf41871e4c8f8e441dabeeb6408dbfea5a2',\n",
       " '.git/objects/01/efb2bc04e576d68750a836309126d1aad90c45',\n",
       " '.git/objects/01/f15420ca3b939439fb5bba09cb0f423c1a3b30',\n",
       " '.git/objects/01/f29824c84b93567de344ef23ccc5f9aa31c532',\n",
       " '.git/objects/01/f44d99e22751eb6e934700c98810e457347528',\n",
       " '.git/objects/02/0db89a3e242593e40bea0c152382cb874ff986',\n",
       " '.git/objects/02/11e77faf9e96a782c79ec23439229d0feba722',\n",
       " '.git/objects/02/19638abff01cbd46b54ee2b01a0a3c639725fc',\n",
       " '.git/objects/02/3e2486393fc0091cbfbcde0670b33032e66792',\n",
       " '.git/objects/02/4dd15ff5f6dfd9cbe10babf6f977dd85e9a906',\n",
       " '.git/objects/02/5661cbaa0e3a75693888cf6411a5339e45bb13',\n",
       " '.git/objects/02/61c4221ba5b8f0882a9d29fc30f6db5c4f5101',\n",
       " '.git/objects/02/79a1fa2820e2ebffe9b1c377d92a461a232c50',\n",
       " '.git/objects/02/84d02d9e91154ab770174e98c0bf62641cace7',\n",
       " '.git/objects/02/949f4d9719e1393a12eea770afd325ab434ce9',\n",
       " '.git/objects/02/9ef50a2bfce37df8a2ccb2aed6f72aff019739',\n",
       " '.git/objects/02/b2e396b4e9f45201b1ad6c3f450f1d71e07ee8',\n",
       " '.git/objects/02/b8623b7c0062b6057a0647a702cee212e6a79e',\n",
       " '.git/objects/02/c0834a6cf0165e1287705c91318a787bbaed3b',\n",
       " '.git/objects/02/d4292da3e8723ea5b8f564eef08918cf8515bd',\n",
       " '.git/objects/02/ddc21c850d3c7f118911b03633925c991a4698',\n",
       " '.git/objects/02/ee480fa94440376e269f4eecf79f084e7c654e',\n",
       " '.git/objects/03/02722a202d9cd45fe4fe54577e0188acf61b8d',\n",
       " '.git/objects/03/223fb0c2409967b94e2b5070501be835528a25',\n",
       " '.git/objects/03/2d27bac44b799f79f8a4250f4f5238cbc00f82',\n",
       " '.git/objects/03/317d9ce559d1fb6831181d2f94e799f6557644',\n",
       " '.git/objects/03/36e468839c25fe62e34a10777109971d9a786a',\n",
       " '.git/objects/03/4560ddeb42492d7a37f9c9bdf286e2e92a28c4',\n",
       " '.git/objects/03/6158a38165836d69134af8ee5ce8031c1adf5a',\n",
       " '.git/objects/03/62628017ff2c8bde077cf16891c6f29092f3dd',\n",
       " '.git/objects/03/6470ddf452ad4f6f16a7bad1fe7ed4af6401cd',\n",
       " '.git/objects/03/6dcef6a6d197b17bea8885b53a5405fa4319c5',\n",
       " '.git/objects/03/75ba53e278cc41e49ee189c920ba3ab35a093d',\n",
       " '.git/objects/03/94cfd75c306f908bb75719fd7e8e461d996c43',\n",
       " '.git/objects/03/b8b006c113b07570a315b9343bae0465f9fdec',\n",
       " '.git/objects/03/bd611944db3120db7b310b80ff95bd1f6873d2',\n",
       " '.git/objects/03/c1247df0809a839d1f19099e8f0e6452d6c61d',\n",
       " '.git/objects/03/c3fa5e4b296a9f952cca2c2b0b1c87a5b495a6',\n",
       " '.git/objects/03/c6e0ac15363c9bb5f46e3a7766ad1961876d28',\n",
       " '.git/objects/03/d7d09439a99e89942c0d7fc11eccb076ffaab4',\n",
       " '.git/objects/03/db10829f714d283e8b471ec0c08909a2d419b8',\n",
       " '.git/objects/03/f621b8bb3f4fda84250b75b404ac74c5dadb45',\n",
       " '.git/objects/04/0caa2fe71676a6bc2f066f05cc1e4491e379d2',\n",
       " '.git/objects/04/0f6b0414b050805099d17fae5c4832b6f72f68',\n",
       " '.git/objects/04/11205650e36572a69b6f2d7180a188b77c708a',\n",
       " '.git/objects/04/1c978691a47c9f44be30e124b27b17c5001e53',\n",
       " '.git/objects/04/30e549eea6e0a482117c17e9065e5aa332dc6d',\n",
       " '.git/objects/04/3278d034f884db0aa9b72b8779a7528f6290a2',\n",
       " '.git/objects/04/3b1c86ddf6933965f9f47d4c9090e5c610b045',\n",
       " '.git/objects/04/4332a67dd6508c0010adc95b01abdfc27c9572',\n",
       " '.git/objects/04/50647503c428dd24da9f529b8725db8db26d46',\n",
       " '.git/objects/04/5123c7a3f873ce64fad057fedd4832582c231e',\n",
       " '.git/objects/04/598eb5a822b498ea65dfd973ec40cc08510b56',\n",
       " '.git/objects/04/5afec123d57571c0d66a54e92306d48fda9cf7',\n",
       " '.git/objects/04/63ad78c4a9cbd25edb0450bf509e9b3eaa4b1e',\n",
       " '.git/objects/04/730644dfb8fae0aee3cef2daa56f2a7fe7d639',\n",
       " '.git/objects/04/8b322931f8c6c879ee852a3fbf9ce15151b148',\n",
       " '.git/objects/04/9193517aaa13eb5361690c441ecb7a0dd9b009',\n",
       " '.git/objects/04/9bc1cc7e3f684199857cad89df622096ed2ed2',\n",
       " '.git/objects/04/a0f9b2c8de1c6a92cc7abf3d7af585e540249c',\n",
       " '.git/objects/04/b32ebe070d5a6718ec53fb9ec7bb7f80b249fd',\n",
       " '.git/objects/04/bc1677fec23e486ac2d92540b668858e02cda6',\n",
       " '.git/objects/04/d4de80ed7523bb8ce83ebb36caa5fbf215586a',\n",
       " '.git/objects/05/017c24ad4534d6ef29647e80b4fe9aac273690',\n",
       " '.git/objects/05/1f0ec3f23375021d770af3cfd60a3d8a3f3d0a',\n",
       " '.git/objects/05/225e658d4b3c2c3b17d5ad31eac0d768c09cb4',\n",
       " '.git/objects/05/2e663c8553ba43d9e9ec241b9f32d38dca79fc',\n",
       " '.git/objects/05/2e82b021b5be6081b352ea2cfdd9564e39598b',\n",
       " '.git/objects/05/338263740070006c3afc7f1b4130466bba6a8d',\n",
       " '.git/objects/05/3bc2ee396625faa0245106f4c33269c3c27591',\n",
       " '.git/objects/05/4d491befb28ccbfd5a757c5614343f5b6c9564',\n",
       " '.git/objects/05/77947672809693bba7b8fa62238d2d666db893',\n",
       " '.git/objects/05/8c7a4f6d605a4a09a368f7708683b2b6fd4c76',\n",
       " '.git/objects/05/a08684ea813b7a9b15ad19bd8ad81a744d8d51',\n",
       " '.git/objects/05/a628513784b151dbccac421cb8a88b92fcd67f',\n",
       " '.git/objects/05/b2056172f67b1985bcad43b7f80fab37aaa5c9',\n",
       " '.git/objects/05/b83c7e1fb74352c85bdc42e0d97ee29ee72a8d',\n",
       " '.git/objects/05/b907b61ee07e7fd6519ea485b92f090b5daf99',\n",
       " '.git/objects/05/cb6457a531e8dd5a8f44f773f8a79693cb23a2',\n",
       " '.git/objects/05/d187f8b68823eaf7b525d8dca0db9d4893728b',\n",
       " '.git/objects/05/d33c9df7ae3f03ca4cc46f940c70754d925aa2',\n",
       " '.git/objects/05/d8841f4883a8e786f2b262114aef48a33012d3',\n",
       " '.git/objects/05/d95315d444ab9b4c67f14f4b4e0e7ba7249f3b',\n",
       " '.git/objects/05/ddd8687affc9c1297357901c28ba68d920f923',\n",
       " '.git/objects/05/ecf63019109ddcf332dd6255a8fd1f5c605ecc',\n",
       " '.git/objects/05/eda07e1fa2edca68f726c0c2e7062077e95256',\n",
       " '.git/objects/05/eeaff84269b3c1b067d424d5962a13d087ad48',\n",
       " '.git/objects/05/fcb581af512d6530116f245c0e8036e6ce9fad',\n",
       " '.git/objects/06/1441ff3401bb21e92de038618365a2d612ebf8',\n",
       " '.git/objects/06/21c261616d3c41b6c3199faef5cdc2a49cb10d',\n",
       " '.git/objects/06/234a9c01fd52334f4de4f62b890d095302821a',\n",
       " '.git/objects/06/35aced7922aee317481cec216370e01ac8ae0d',\n",
       " '.git/objects/06/3e9dab70b604929645cf192fc83c5920101cb4',\n",
       " '.git/objects/06/458086ff5bd890cedbbed101305e485b72276a',\n",
       " '.git/objects/06/573f54edb2dfd597b8fde022b20a3f0e63f0b7',\n",
       " '.git/objects/06/64e454ea9f16e12e7a390b2b6e8e57761ccbf3',\n",
       " '.git/objects/06/67a2a9b30c8af3b68b103e46615daf4545ef85',\n",
       " '.git/objects/06/69865cce1133689cce2f0a948237c5181bf36c',\n",
       " '.git/objects/06/6e4e146e3f35186919188a3b8fa02b06fe3e07',\n",
       " '.git/objects/06/770cc0039cad818b0234d8a02a872d9f65f690',\n",
       " '.git/objects/06/79f8f27da1695194b7c5a71dff4335665bca07',\n",
       " '.git/objects/06/7e82312b80a9c7183fdcb6176e5d689a89a449',\n",
       " '.git/objects/06/86e6e0943f4919da3e68927fa79cfda6c6b563',\n",
       " '.git/objects/06/8f3b8fc91abc437e69486e0d19a4748ef33522',\n",
       " '.git/objects/06/a7685dc8e23a519c8e1d8ea335cb162d6b250c',\n",
       " '.git/objects/06/a8a183884bf68974dc0e5e6a83c96a74b5f318',\n",
       " '.git/objects/06/b379378a4a30b7627f3dbbef7933fae375232e',\n",
       " '.git/objects/06/bfeaa12b195248fa0f15b2c48f088ddda07043',\n",
       " '.git/objects/06/d3c19b36cc02ceeb644e9dcbadfc561d0c1e84',\n",
       " '.git/objects/06/d5f66e4f5c92677b9e4189ed783701db459469',\n",
       " '.git/objects/06/e36229440b9db3c4fa2a4767b75a0e0550021b',\n",
       " '.git/objects/06/e49010fe0471116319664d3ae4875261c7f367',\n",
       " '.git/objects/06/fe7b379f6d57d9f32516bb704fcfe9598617e2',\n",
       " '.git/objects/07/0631e85bfd21aa6ed1f1e429325cfe17f58f01',\n",
       " '.git/objects/07/1f135b9573175ec562e9307f523a5e9eff5ccc',\n",
       " '.git/objects/07/2a60345df44bf72cb189eb7db7d453bd1e8adb',\n",
       " '.git/objects/07/308b5495ba48e9cbe7de177abd944f76dd4c87',\n",
       " '.git/objects/07/34514fbf56330d587e962f0d5e11fba1c92108',\n",
       " '.git/objects/07/351dd611d82ed8c3e5f59a6d69a817b0f514a5',\n",
       " '.git/objects/07/39304dc2d2bb3a1e448e549801ad37ff5b9988',\n",
       " '.git/objects/07/527a65a05608230992cbe31ab4b4a46fadf91c',\n",
       " '.git/objects/07/56e746455d2c9670be674f7b87f8dcd64543d7',\n",
       " '.git/objects/07/57b8da9c6dd9811ce67bd77b26f33b971b93b6',\n",
       " '.git/objects/07/810570d5cd5b516916be8aa9e53fdcc1fad30b',\n",
       " '.git/objects/07/888986cb28c75f22f04189296ac99a68bdf9f9',\n",
       " '.git/objects/07/947733c5cf13f72d0968a5d04353be6bd5128c',\n",
       " '.git/objects/07/96cd9cd7c11129adb7cbd15ca4f4a0415ad5d5',\n",
       " '.git/objects/07/9c8a055ad3e030a3162727d97c4991d9fc728f',\n",
       " '.git/objects/07/a6cbae2c6b53727f23cc454997d227a34481bf',\n",
       " '.git/objects/07/b1c097a279f223e1e9437f0ffef3e4535d8ace',\n",
       " '.git/objects/07/b82fa5bc39e75f88c562399c7f6eee2a01ac24',\n",
       " '.git/objects/07/c51af4712edaab6af987237e6eb91475e71b8d',\n",
       " '.git/objects/07/c65af852f49f89091b3dad57b51d1584d2e7fc',\n",
       " '.git/objects/07/d2e4d86916da8aea6382bf6df938ab8811b94f',\n",
       " '.git/objects/07/dab8043b86f4c63aef872249c0d23a4745436b',\n",
       " '.git/objects/07/f006e92a4bfc001cbe7e316355e9a3b7f94f1b',\n",
       " '.git/objects/07/f66747c19724da7cb4c2ebbe6c81acf9d621fb',\n",
       " '.git/objects/07/f786082089b8881e78270851ab07d02a889d52',\n",
       " '.git/objects/08/37a775b731c89fa6575ab14ca7483b8c0528e8',\n",
       " '.git/objects/08/6d016c97c87b871dde443ba28c623b8f9ff33e',\n",
       " '.git/objects/08/98b6bbd84b35e5adde818889f3f59b02c7d617',\n",
       " '.git/objects/08/9ea00da34f3bb9a1dc5305ac021d83f1d5eb7e',\n",
       " '.git/objects/08/a81643a2f293873e2e6c984fd9b93a0cfa58f7',\n",
       " '.git/objects/08/b7a12c1a76dd65cf1f618bd420a4144ba3739b',\n",
       " '.git/objects/08/c18ae2f28604d5c40d13e9c1dcd4e85f85172b',\n",
       " '.git/objects/08/c2d75b2698c9e65d344e9ce88e2c74c579c68e',\n",
       " '.git/objects/08/ceb78adaec399f11d6706f9631a3d990e5ca7a',\n",
       " '.git/objects/08/eed914da2860d5c9ac955a46cec4a3ab760a45',\n",
       " '.git/objects/08/fa3ab44842fc38021d111b22e8d3e0a7a307c1',\n",
       " '.git/objects/09/101b90ccf1c95ca690974099e792b0094fcc45',\n",
       " '.git/objects/09/105c121f871d11b63b7b8fed729161cd8ad825',\n",
       " '.git/objects/09/1b80297cc6ddfb32c1a8a057e82a63d70bcecd',\n",
       " '.git/objects/09/1c0e4eddd1933fb133e325290da9caebb1e7a3',\n",
       " '.git/objects/09/4763afdfd9f24b1e72e81d1796b00df493bc3c',\n",
       " '.git/objects/09/4a07ee8e6015201d4b1b87d45bdb1e26260db8',\n",
       " '.git/objects/09/53dfd51f4f143621ac31dd3c7424b6d2e912a5',\n",
       " '.git/objects/09/562835096d01fff7c92a434fdb60ff077790c9',\n",
       " '.git/objects/09/5eccf3ba4ff9fac4cf279c7ac205765192c04e',\n",
       " '.git/objects/09/65941f3be7c8e90a15d6a10cb1f4d499e16b95',\n",
       " '.git/objects/09/6b4ac08a78021229b6b0293c946fd7d528eb13',\n",
       " '.git/objects/09/80320c1970bc13a791c9b17deefd871a779dee',\n",
       " '.git/objects/09/8057263d792b6ad9c9bd57f24fc4e317a61a71',\n",
       " '.git/objects/09/8caec834fe16682bedc759812cd4bde320a820',\n",
       " '.git/objects/09/9415e358ca2a08db538d68b7e2915521f0a8fd',\n",
       " '.git/objects/09/a01ad0bf0dca3466a5ec3644c461c8d7bfe213',\n",
       " '.git/objects/09/ab7b2a8a8a868ce01e4fa52cb4b933e82abcf8',\n",
       " '.git/objects/09/b745676de593b3eea0e89cfe8e82d5bc49a54c',\n",
       " '.git/objects/09/c37617a98c61357d739b01d137652a4e8f602b',\n",
       " '.git/objects/09/c4393bd44db40706966f30f7abc78e91b1bfb5',\n",
       " '.git/objects/09/cdd0a9a81ecc790fe10c5f51ecc5be64a67acf',\n",
       " '.git/objects/09/dc32a85795989fa0619d0e8ad4184ac181d249',\n",
       " '.git/objects/09/f00cccc1ddb0f6a2b6a4b5d73b5c11cb3a97f5',\n",
       " '.git/objects/09/f56e6d2414c9e5dea49153330bb11ea5cf1d1f',\n",
       " '.git/objects/09/f7b9022012f1c5effc2b28971cc2fb3fe48611',\n",
       " '.git/objects/0a/001518dba5532d5581d9c0c594bae358de69d8',\n",
       " '.git/objects/0a/019e0736166f7598b1194de0fdea04c7490f5b',\n",
       " '.git/objects/0a/05acdf6da1b9cbb063150ed264c104240db5ef',\n",
       " '.git/objects/0a/4871c1ce3a026bcb12d5bf56a4b9bcc78421e7',\n",
       " '.git/objects/0a/4c406990c35a9e56eaf7e94030a600dea17373',\n",
       " '.git/objects/0a/5d1db9a4dde7a590dbcde230a2f7cb94865317',\n",
       " '.git/objects/0a/7cf9888c48f46db850c5b17fcfb133a11efbf3',\n",
       " '.git/objects/0a/8f3f009fe501e8b20219b1f996cff6cabe9e69',\n",
       " '.git/objects/0a/9e1777a824d8f3a444ed84560d22be575f1654',\n",
       " '.git/objects/0a/cb659255359d73dfd8b053b08bbf02db390fae',\n",
       " '.git/objects/0a/ce6652abf087130c0216b561cbcc265a46e66e',\n",
       " '.git/objects/0a/d56f7635e024070ec8f107753a4f955277a054',\n",
       " '.git/objects/0a/ec2f9ea6e5d3f30114300f39fc0d404a42e5fc',\n",
       " '.git/objects/0b/04a91c4446c039c0d2ad3931fb282a6dee24b9',\n",
       " '.git/objects/0b/1e8ba7320785abfb427fa67f86c5af2af5309e',\n",
       " '.git/objects/0b/2a8e35cfd4bd58362ce1eefcb54230738e17f2',\n",
       " '.git/objects/0b/39f5c84f686a405418a0c45efed23a0e542ba2',\n",
       " '.git/objects/0b/40047c29545b884d452b65fd70a439bce8bedc',\n",
       " '.git/objects/0b/44c42cc315106afe96ef1130cd53911af0f069',\n",
       " '.git/objects/0b/4cbfecc286f374e8d832846489047ce935367e',\n",
       " '.git/objects/0b/712eb2a22fcd749612a8cbe6262765ed3363c2',\n",
       " '.git/objects/0b/798f049f6abefa64738204e045bbe155a3dfe6',\n",
       " '.git/objects/0b/7a2105aa5bcafe5f9562eb44b9407804c55d86',\n",
       " '.git/objects/0b/95c53f1818a8c9437ab90e56af206413ca0908',\n",
       " '.git/objects/0b/b5e1a0dddfe92a2d036881c5a956f538d35604',\n",
       " '.git/objects/0b/d75d8d395c1151cd83453f973f3e44f381bdcf',\n",
       " '.git/objects/0b/d878f010b14151a1b34bdd06338b9c643a2c92',\n",
       " '.git/objects/0b/deb03442df63455daea076bab7e0cabadfa9d2',\n",
       " '.git/objects/0b/defe6bdacdff0c2ffd5607f6fa0bc2e88c733e',\n",
       " '.git/objects/0b/e54944f61d246dd293597b9668340b4eda7277',\n",
       " '.git/objects/0b/f90cf7fbde914be86ca90b07bdf8d247abdadd',\n",
       " '.git/objects/0b/fd4159206c6124ea50235ad85b9a99da357272',\n",
       " '.git/objects/0c/27bd4956f76aad9430807e763ee077492ce917',\n",
       " '.git/objects/0c/59198e789281e36150604eb6e36a20d00aa95b',\n",
       " '.git/objects/0c/6d207db5e83f83fe8c58114fdd9b537f59d7c2',\n",
       " '.git/objects/0c/81ce732d531db49f608a3cdb58de380f8f3f90',\n",
       " '.git/objects/0c/a4a9a5c37812aa588fc3467ee3287a1224acc1',\n",
       " '.git/objects/0c/b4c7b448cdee511884dcee61cd45d432f5b1a3',\n",
       " '.git/objects/0c/bc2cd4df673b13eeda3a8d77f136c7b288631a',\n",
       " '.git/objects/0c/c2793f19b93cb6df2ed11d07fa008e4383b4a6',\n",
       " '.git/objects/0c/d1debcf9e63ba29376e5ebf5156635eb6b54aa',\n",
       " '.git/objects/0c/d2fe9f4333814257bf928f7bdaa390c84d4666',\n",
       " '.git/objects/0c/d3e3c0d547283ffdaa974de9eef66cda7dcfd9',\n",
       " '.git/objects/0c/e9d52f9ab39501fb1112fe62fe55aa923d6650',\n",
       " '.git/objects/0c/f0abe80e711f7c30030c1fd2b636f4253484fe',\n",
       " '.git/objects/0c/fbed25e3e032c0e77746f508b14484d5be9fd3',\n",
       " '.git/objects/0d/0722c6b2c628eb33fd6662564540700a8f1a1f',\n",
       " '.git/objects/0d/140d1a2494553b27e1c03af2fb37377eb1f9ec',\n",
       " '.git/objects/0d/144a96dea1919c3d051a9e8b1cfb1b09bf8be4',\n",
       " '.git/objects/0d/1b56b0b1a01c342cc0f5807dffa16ebad8b6f6',\n",
       " '.git/objects/0d/23d7b5ab194ac71caf0222bd260a51b28955d7',\n",
       " '.git/objects/0d/384a5c44e49a42b2a8c9a01c561ec050ea0a79',\n",
       " '.git/objects/0d/4278d291852c654163d55cf65c88626de16d73',\n",
       " '.git/objects/0d/436cec9d647316c8b44da99b46c9ddc691d354',\n",
       " '.git/objects/0d/5b89b01ab6862218136c4a5247929146136e0b',\n",
       " '.git/objects/0d/7a55f60c1c272af9ad765ef535035a42dd7fde',\n",
       " '.git/objects/0d/83f0fa779e8a65312c72dbb1a688f747c5178a',\n",
       " '.git/objects/0d/89c92cc9ca73a93bbbb5a59631126206d2f09b',\n",
       " '.git/objects/0d/954962f700e0a0cdae325f310aca274b672f09',\n",
       " '.git/objects/0d/9add5b8318640f18c86b786824aff9f7032b60',\n",
       " '.git/objects/0d/9e47f41a3f0a3b0dcc70fd8ad947c7207fa614',\n",
       " '.git/objects/0d/a1f3aff922c2de7bb66af5076d75962315b43a',\n",
       " '.git/objects/0d/b40b39fe1c543b7dd57c25498615780609f3d2',\n",
       " '.git/objects/0d/bdff972abf9a74816d465c1faa7a85ab7611bb',\n",
       " '.git/objects/0d/c10fed340bf7dc0acf64e7d7c782f215cddc13',\n",
       " '.git/objects/0d/c2e395ebee6cfb615b3629b097c5d314403498',\n",
       " '.git/objects/0d/cb07409ea66e2762f02672e02e0202607616f0',\n",
       " '.git/objects/0d/ce918d074c2bec761481d3acf7b2da2d41511e',\n",
       " '.git/objects/0d/d6d4eb59b61dd965c097385baeaf00de4cef39',\n",
       " '.git/objects/0d/e45c3415f6855243743c62dbe201aa7ecade04',\n",
       " '.git/objects/0d/e754c32a820270e5e826a21f6beb841f14e08c',\n",
       " '.git/objects/0d/eb5104a7473be2f257aa2a78f7d63f72dae6db',\n",
       " '.git/objects/0d/edc1a5067a5915b642fd6a715b6e979387e49c',\n",
       " '.git/objects/0e/04f53b90f6ab61a08a16f20d99c623fadfd93d',\n",
       " '.git/objects/0e/05eaa1d2a80c649e62b94b11171c8dafe0d60f',\n",
       " '.git/objects/0e/06795352d052c160b218da1d7c5e90d9689916',\n",
       " '.git/objects/0e/07e1532f5a384513c4a3edbd0a7873a999f43c',\n",
       " '.git/objects/0e/23e8a197f647f90d1842eae64589e6953fedb7',\n",
       " '.git/objects/0e/28ddd2c293c392b012c5f4d59b2db3552f5bcf',\n",
       " '.git/objects/0e/2c8399fd2f0aa0120bdfe5134fd704c5122277',\n",
       " '.git/objects/0e/36be8632b7d7d1101aa87e587e4c82b41a297d',\n",
       " '.git/objects/0e/5b2aedaa4c8aaaf818d34b4897b7a2a0aa7081',\n",
       " '.git/objects/0e/5c8e5900ae68e8989e62078aac12fec4665bb9',\n",
       " '.git/objects/0e/75229ba05c3a004f4003a51f0e43880c5389cc',\n",
       " '.git/objects/0e/8a28a5ac6cfbbd527e076b6a099cada4f0274e',\n",
       " '.git/objects/0e/8e30ad4ee814c76fa547b4a24a4221c34397dd',\n",
       " '.git/objects/0e/97fa613e4afe50c7c0d2ec7c19f9629b2c2205',\n",
       " '.git/objects/0e/a146b3ab5a78e2ae771dcfd24abd4631dd4a17',\n",
       " '.git/objects/0e/a7d605108d14617e9cccf19e9e19c3559a3ee7',\n",
       " '.git/objects/0e/b6ada72c09308466836d98cf40b9634a6916d1',\n",
       " '.git/objects/0e/cc3ac4e7d38ea2d84d6acae9cf342f19d008ae',\n",
       " '.git/objects/0e/cf538648c5f3dba0e052a773f5566845d024e7',\n",
       " '.git/objects/0e/ec39fc45f26116973867f8a9335b56e6355f13',\n",
       " '.git/objects/0e/f1299f48a3bbe8ddeb2b1bad7ca8bb7b4ac10a',\n",
       " '.git/objects/0e/fd5b9988690b2df3f6160bbec41e05920e6b14',\n",
       " '.git/objects/0f/1092b17e46fe4836658408034dbda37b2823bf',\n",
       " '.git/objects/0f/1d35ab926f1229068a371c7a0765008996f9e3',\n",
       " '.git/objects/0f/295165d59f40115f5099fb979b4de4a3e347b2',\n",
       " '.git/objects/0f/2a2888c4ac50343be389cc8c1e6e1c41026136',\n",
       " '.git/objects/0f/38f98f5c9c4cf6a85d781e90c3aca0cb0c4671',\n",
       " '.git/objects/0f/3e8bceca51fb687cc38b3a30b13f25856d9f4c',\n",
       " '.git/objects/0f/43177d4c947f4bab9821a594cf8e87221433aa',\n",
       " '.git/objects/0f/4c91c48f0a6f0ad11142c5dd2f9138d66e3ffd',\n",
       " '.git/objects/0f/4fc33b641d6fcf30f7b98a4255e2bbd16badb6',\n",
       " '.git/objects/0f/53fbaed11b42450fb79425aabae92c43ebd143',\n",
       " '.git/objects/0f/5b71ebdb13f43831e876fae0ca7177029b51d5',\n",
       " '.git/objects/0f/660f09f3819bc76611f6ed359f383d972ae4f4',\n",
       " '.git/objects/0f/699f259ebd83010b747ed79f8da7ef9c9a27d0',\n",
       " '.git/objects/0f/7412c6f069b250f0c6816aeea6359eca281a6f',\n",
       " '.git/objects/0f/7cee5ed6a646ae249a66a35a20c6ea3a2db572',\n",
       " '.git/objects/0f/7e76b049962845e4494d81ed28f83a0f9e6da4',\n",
       " '.git/objects/0f/8358275ed2fd56ed0f71e70fa2e45769ce11c3',\n",
       " '.git/objects/0f/8cbdacb17b12852b6272c1e933c851571dbcd9',\n",
       " '.git/objects/0f/924424fad1278e60f967774dcafd81662a2380',\n",
       " '.git/objects/0f/989387c62898a000352520a6e3c326d51d7f5d',\n",
       " '.git/objects/0f/9e1ebad9a7403b13b1c50f8b1f8339213e6ceb',\n",
       " '.git/objects/0f/a2f28f5f370d5aec228e663fc40cbb5df16fab',\n",
       " '.git/objects/0f/ca63199c7605e3095e1ca47fb08bb89f15432e',\n",
       " '.git/objects/0f/cba99cb548b04f846f07a9801564f499314248',\n",
       " '.git/objects/0f/de1243b0fc36fd5dfdc4b01ede7f6bb32e2f9a',\n",
       " '.git/objects/0f/ee6b7e334a369ebc4dc3d80daac381449eb270',\n",
       " '.git/objects/0f/efe6453022ba283f22f768e1c534dfb047996e',\n",
       " '.git/objects/0f/ff9c9a095f73c9b57b72f52691e412d1364ad4',\n",
       " '.git/objects/10/180eb3157bd7e5acb087b2246ba11132606547',\n",
       " '.git/objects/10/30fb155e3cbc79beed37aaf2ae14fc9d575d5f',\n",
       " '.git/objects/10/3c1f995a4762672f09beb7736cc0c774539aac',\n",
       " '.git/objects/10/46fb91cbdd4436fc897674479611c1708d2135',\n",
       " '.git/objects/10/4845845e5350c6668ff118c2ad77769a1bec28',\n",
       " '.git/objects/10/53b583d9d3658a6c54e3b1bf7f3db7fffb0a93',\n",
       " '.git/objects/10/541357ced288a6362c964fb9e7b3ce3791a251',\n",
       " '.git/objects/10/60d052c85df07bc3a4a76475293614e3157816',\n",
       " '.git/objects/10/716696fad94871cfd81f9a5ec0acc8a2ca6891',\n",
       " '.git/objects/10/78a524d66d9c66050974e37d4f528cdcfc1255',\n",
       " '.git/objects/10/7d7381e1d4dff60ab74fcea1a6c21d0add0be0',\n",
       " '.git/objects/10/8ed1249a7ffa291f07a4d0cbea72b338ecdf5f',\n",
       " '.git/objects/10/9ffc432f77a7f6e6c56a28635139d3678010a6',\n",
       " '.git/objects/10/b3496391fe0fb7e3c55ccbc622f26c56a3dfc7',\n",
       " '.git/objects/10/bd44176a4f95d0dd9ce904ff7ac4a60eb09575',\n",
       " '.git/objects/10/c2c92b7ee291569d183685e86ca3f040d5a88b',\n",
       " '.git/objects/10/c71579ec7d41f34331913de9848ed599b51956',\n",
       " '.git/objects/10/d0c8f34fd396df6dd7801430952c70bba53443',\n",
       " '.git/objects/11/1b9c783016bd643c40eb5e2dd2fc9d845a57a8',\n",
       " '.git/objects/11/3d5280df6339944da4161b0c7551a3ae53d39d',\n",
       " '.git/objects/11/42d9a35ac547abd5360aac4fbdb7f5389a8667',\n",
       " '.git/objects/11/450a3feb8d05b6662f927eb2f6fbc7bd2f35da',\n",
       " '.git/objects/11/49d99f14c82ebdba45878a146dc56d934340cc',\n",
       " '.git/objects/11/4d14aa640185dfb141051bf97e281b4216247b',\n",
       " '.git/objects/11/50010de679b9356551d255bb0eec2527298691',\n",
       " '.git/objects/11/653ccb986f25818a28e172fbd764f08d215e45',\n",
       " '.git/objects/11/6850aca5bfcf000d8c32d8d306520b9cc64511',\n",
       " '.git/objects/11/6c45eb018f2d3dcc801e40175999b03d745c8c',\n",
       " '.git/objects/11/744e629f73e25c0359cd73682637f7c97dcb04',\n",
       " '.git/objects/11/7fcfb3f02e2b635a9d43d7eafea8ac3be0b828',\n",
       " '.git/objects/11/9689860cf51fc783623727bfe7cdfddc75bfab',\n",
       " '.git/objects/11/a52461416125cd8f8bc6c681c9185a0d0a723f',\n",
       " '.git/objects/11/c7abed34130b991809b66140f4a9699b08b247',\n",
       " '.git/objects/11/d5f14cad7fd4773bd9b592990e80830603fc08',\n",
       " '.git/objects/11/e0ae6dd685486c6872de62dfee18dd7a11fdb7',\n",
       " '.git/objects/11/e27d8b9ba7cc775ac06be4fe6a33af6ad74b42',\n",
       " '.git/objects/11/e371ad22c00d225e6469ce593ca2e4d16857b1',\n",
       " '.git/objects/11/eaaaf91e38634bd3b3e78b8f62d7cd777687ba',\n",
       " '.git/objects/11/f047d7a681638bad2dd92fdbbe901a5c673c7f',\n",
       " '.git/objects/11/f76dd1ede7c1df509ed1f3ef7b3933ce2f9d1b',\n",
       " '.git/objects/12/1005f46e9c0f7810cfa8c02dd81b05bbae2773',\n",
       " '.git/objects/12/125e5fa07160eb2838385c160f766e1bec9b5e',\n",
       " '.git/objects/12/1afee05ce4985e821f9aac0f4060da20f22c9f',\n",
       " '.git/objects/12/3fbbe7e0f650851ce0633b67f1e5551b647142',\n",
       " '.git/objects/12/653064b16321b9734ec5cc3bf0336bf4b51f53',\n",
       " '.git/objects/12/6641b2322bf953f04e605da019a756fd91fa6b',\n",
       " '.git/objects/12/741e3b312a314153a38111c70b96d778896557',\n",
       " '.git/objects/12/87d724de667b3e0f5e542041a6c225864b04ca',\n",
       " '.git/objects/12/98ffbd16d536323288321e0bf3097e144bfb59',\n",
       " '.git/objects/12/99efaff54a8a3c576b1673aad26b76e9cf8575',\n",
       " '.git/objects/12/a03a02b348274b5dd1cb46a7c5844240880964',\n",
       " '.git/objects/12/b6d670ba1b57e280e3c97264733870b0f464e7',\n",
       " '.git/objects/12/bf104625a32a53f767c4b9441e3456233ed0eb',\n",
       " '.git/objects/12/e8249e403c3d8ecabcf92e1a2598ccfe1ece93',\n",
       " '.git/objects/13/044325af12ee9e2e8e2e10b90756b6c338df51',\n",
       " '.git/objects/13/048c9abbace2d7aec841bb64b08e6ae7a72ed1',\n",
       " '.git/objects/13/255218fd072cbe8e4f049c4e487b35024c557d',\n",
       " '.git/objects/13/3b973690ac9e830cbd24d60a1306cd716654e0',\n",
       " '.git/objects/13/3e7b12c9bed009a724a150dd5e2981c5f78ecb',\n",
       " '.git/objects/13/4138d9e21d5e1819378d2c96fb7376caf90392',\n",
       " '.git/objects/13/5dd3a40860f5de796bbee115b0b194e29a5449',\n",
       " '.git/objects/13/65ac389c504e828d544da01c675c03975a8341',\n",
       " '.git/objects/13/6a9262cc02ac997ed4627cfcfc5f29ba31de15',\n",
       " '.git/objects/13/6aaae1127c454d6cb572f5db5d5d83f2532930',\n",
       " '.git/objects/13/78b714d730ec05902495589fb193c7e8cc3646',\n",
       " '.git/objects/13/8d06135033d3775fa3c6d968a903d083b30138',\n",
       " '.git/objects/13/9de5da9992d7a682a74d17574afd8f83479691',\n",
       " '.git/objects/13/acc06e83b45fd7382937d945991eb0f58eae2d',\n",
       " '.git/objects/13/d2fc2069ab7855fd1009f7948f5cabae1c90d0',\n",
       " '.git/objects/13/d4672a14c8a2732211ba3b84ee9cd6ab75914f',\n",
       " '.git/objects/13/d76d713c9916445d3dbed625c6863c16081ffa',\n",
       " '.git/objects/13/df7439aa7246b4f8c52b97e8ca8b80534dac72',\n",
       " '.git/objects/14/02e61ed3a96c137c32b8154dc0b3fb182b98c9',\n",
       " '.git/objects/14/03b0d8890bd32a34be37f1c7a7396d3040f089',\n",
       " '.git/objects/14/0407f8af49e09d7628704ae2852c3c61c70823',\n",
       " '.git/objects/14/1e74cd743d687b7eaac0f89bb423d896772908',\n",
       " '.git/objects/14/33a1c78316e56b83e8c64bd36cf446ce275860',\n",
       " '.git/objects/14/3fdde0a71f9855ed0bcbfec3e4724f573bef53',\n",
       " '.git/objects/14/423f1e2f794e82f7a5322d3eb24f2d363bbf94',\n",
       " '.git/objects/14/4dc25ab7a5e211ee0738c31076fd181d32d438',\n",
       " '.git/objects/14/614341078af54873fd630f2b3f7d5fe5900948',\n",
       " '.git/objects/14/89e1bc9f94df16268e29d3107c8ff8568b4be9',\n",
       " '.git/objects/14/98034b0bbcbf255c6ca2baa26161bad1711ad3',\n",
       " '.git/objects/14/a10c1f5c0afb215798b83f023aa31930937a46',\n",
       " '.git/objects/14/a54c109942a1debdbbbe456cc8d3bf989e42f1',\n",
       " '.git/objects/14/b091842e2a3af0b55bc1d370f5c08b990ee26a',\n",
       " '.git/objects/14/c2a795081fc406ba05868317464200db190717',\n",
       " '.git/objects/14/d67da54af9391345d8bf255be308095e8913e8',\n",
       " '.git/objects/14/e53e6eab8e1b166dcdcdc9c1cf8f18713af266',\n",
       " '.git/objects/14/ee62b70f450a708981257c3f697c618778c801',\n",
       " '.git/objects/14/f449383c17f9d7b683f5d20f0bffebd0bb006b',\n",
       " '.git/objects/14/f5e402b20aea48c84b5dcd964376304205065c',\n",
       " '.git/objects/14/fd839a13ecdf01764143b6f5d339de5f53174d',\n",
       " '.git/objects/15/086c4250a19dce0a3ec7839df0770bc7446f08',\n",
       " '.git/objects/15/0a71cb149df3bf1ea328660676a9aa88a2cf4c',\n",
       " '.git/objects/15/18eee952bfd3f14911e0dafa6db4787002a30c',\n",
       " '.git/objects/15/24c841969b6a2737b9e7ea9fb7bcfae32d234e',\n",
       " '.git/objects/15/2dbb5dabf42d8a742b33512377bc4e828fe3ea',\n",
       " '.git/objects/15/3791dc2a40e4a755774764e22b8f0439513e27',\n",
       " '.git/objects/15/54945fd35627e1a823fc42c3e84db78dce8910',\n",
       " '.git/objects/15/686a7e63fa6feb8d05e4e9852200cc2b2232e3',\n",
       " '.git/objects/15/6a52c31084bdc17b0920756f2c85826d5defde',\n",
       " '.git/objects/15/6d921771d14f689af85148550e23135134bc17',\n",
       " '.git/objects/15/7fbf38e72cac9a7a5e4bf57b027d1bd686f45d',\n",
       " '.git/objects/15/9708e559e26072351d92b7cda486ca9b2e51df',\n",
       " '.git/objects/15/a2db916c1f8bbc64eee22e5e9a62740893d152',\n",
       " '.git/objects/15/a8fc9ffbaf5af8cae25b7485a76c7eadb342c7',\n",
       " '.git/objects/15/acfd43115ea50a1afd94922e9a384d053b54e1',\n",
       " '.git/objects/15/ada2b24bba363b1190589acdd57eb11371c291',\n",
       " '.git/objects/15/b128db20a907a16534074ee9520d76532f22da',\n",
       " '.git/objects/15/b33285997aee561b622fd3734a2a0f2d2ea3a9',\n",
       " '.git/objects/15/b62230944fccd8025ad1b60633e208a1a1a6f2',\n",
       " '.git/objects/15/b6d5edc7d9e6b262a3e2d080d817072de6b038',\n",
       " '.git/objects/15/bbb11a9b3c4af71e4f1f2a54c0685ad251de87',\n",
       " '.git/objects/15/c9535ac2057e5fb9ac9286668d17a5433b0815',\n",
       " '.git/objects/15/d0b3007e798affbfe8407a740475001db6c3a4',\n",
       " '.git/objects/15/d1716431653a79a38f5fdfa0533e78e1f1927e',\n",
       " '.git/objects/15/d9ffc60515ff539b12bc6c2c4d10414707455b',\n",
       " '.git/objects/15/dbbd97b3e72651cc207ded6b6e375173d60779',\n",
       " '.git/objects/15/ea7732f10228a3f29643bd4644021157eb362d',\n",
       " '.git/objects/15/f56d944efe4063ecd0421b618f26e59bd3e34e',\n",
       " '.git/objects/15/f6105472f4992abb532ec7b7023aacdb9cf269',\n",
       " '.git/objects/15/fdc82377a9008b3ac72c5f2520468b81f145e5',\n",
       " '.git/objects/16/121949206a7cd0628432ff445b21d415b15902',\n",
       " '.git/objects/16/1887a5e46ac2069ff88957d7d8d8a646f31e2d',\n",
       " '.git/objects/16/23f69f2994dac62e705423a28584f0b358f216',\n",
       " '.git/objects/16/39e25ba4e3724af79f4faebef182237252fdfe',\n",
       " '.git/objects/16/57d03c27f1d53cfb20c27251615bda332a4887',\n",
       " '.git/objects/16/5dc97c6c6b020e413dbb6ed7214c66299a6d5b',\n",
       " '.git/objects/16/5e74e843a8d7a7e9ecb28f094b5cdb0fe38242',\n",
       " '.git/objects/16/712f9cb0d4e660efabebc1687ff25a729cd295',\n",
       " '.git/objects/16/719d7d9204d77e412134a63dd3dbb9de4b11c9',\n",
       " '.git/objects/16/8453345feed745d37ba0c55f741f8a0cbb7778',\n",
       " '.git/objects/16/9dc9017462a5322656dd31600a96b103ce7aee',\n",
       " '.git/objects/16/9dfea974d9278fac97c9e5f095d64041af1349',\n",
       " '.git/objects/16/b1c4a6703a040ac7619a57ece22777559755e2',\n",
       " '.git/objects/16/b319fd61df955f26af919e58895f17e5c66433',\n",
       " '.git/objects/16/c9721d4ae32654221967d2928acfe904633830',\n",
       " '.git/objects/16/e48162e2d7d90cde8e54cf1c513faec6b47865',\n",
       " '.git/objects/16/f879d70c9b295cc536dbbe51245f607dd2bc37',\n",
       " '.git/objects/17/0a4ce14efa360645f909c5697ec8c37197bada',\n",
       " '.git/objects/17/2584423c3bb2159adf5ad9afd3e12991700bab',\n",
       " '.git/objects/17/355445b1dd9c1e4a253242579aad94a8d18db9',\n",
       " '.git/objects/17/49fe52bad258f37b821674123136ac59e5bd12',\n",
       " '.git/objects/17/560a3e40dd28c2d88975afb2e1556bb9fd0fe6',\n",
       " '.git/objects/17/8324bc92841702e69c1b57b0bee8026cacbbd5',\n",
       " '.git/objects/17/8687e781b0967a4509b068b4ac51e76f7baead',\n",
       " '.git/objects/17/8bed113cbd0829a6750e3896b163021f49459b',\n",
       " '.git/objects/17/a1ac4e672eac7934c772627f2d5a7ae9f274fb',\n",
       " '.git/objects/17/b58f68d48fab59d3e1035744f6b82b7d674a3f',\n",
       " '.git/objects/17/c21c9e7a96ca347a597c2de80bb100d9f5335d',\n",
       " '.git/objects/17/c7b87268e8f44df0e68b5c2051419b478b5cdc',\n",
       " '.git/objects/17/d072072d89f4ab1934e7c98dc4e27b4ca6ca9a',\n",
       " '.git/objects/17/da887831eff23a161508c7536dd40d534ed0c0',\n",
       " '.git/objects/17/e7d9d124f03ebf363f88cebb708f57cdfc3630',\n",
       " '.git/objects/18/064a3eeb60ccf80e7ed3f164989b961477b1de',\n",
       " '.git/objects/18/277c203ac1261944ee0e5f1cd0f19e42d9ed30',\n",
       " '.git/objects/18/34cea25b6a13554b60eaab1952f628993c654c',\n",
       " '.git/objects/18/3dac92f38f1a1a3b4707f6fd3400f1be6246ba',\n",
       " '.git/objects/18/50a3bba187705e78f5e1b79c24ce34d1abb5bd',\n",
       " '.git/objects/18/58595fb3b71ce91cfeb7478d4caf82baf60795',\n",
       " '.git/objects/18/658c9c075ed73bdef0cbf7280cf3971160dba2',\n",
       " '.git/objects/18/7ec5f10d52c258b330a944542a83cfe97b8671',\n",
       " '.git/objects/18/8abe6c2933658249cc88bc24129f1d50c21b77',\n",
       " '.git/objects/18/95f6ff8fb76d3df717a62eaffaa9719d8dcf72',\n",
       " '.git/objects/18/b7fc992ca3d8afbae9f1d128f39bef6d8b7b1e',\n",
       " '.git/objects/18/bc0e4820b7acca2ef7247702d519f072d8fff3',\n",
       " '.git/objects/18/c03beb2c8ebe2398a7c56b500ffefe2031c7cd',\n",
       " '.git/objects/18/d7773d8231ca8ff7b95f4977c10086ee570a4a',\n",
       " '.git/objects/19/28922def3d861a13555d25da36b6f6e1a49424',\n",
       " '.git/objects/19/2c0ca4c1e247074d274f344401612abbff07e4',\n",
       " '.git/objects/19/383df0c78761fecc8acc86a62d1e60c026c5d0',\n",
       " '.git/objects/19/52ec1d0344f44d93146a7c657846e3f02e9316',\n",
       " '.git/objects/19/6fe63847c4e1f70edc10cd8c5bb075b297b1d3',\n",
       " '.git/objects/19/7f99d4addd70645c643bf73fdd153c1c2e5e75',\n",
       " '.git/objects/19/8164f3c4d0b330ba35df46e49548cd7b20910a',\n",
       " '.git/objects/19/8d07a206f77a0d49ac2d29630054468780fcbb',\n",
       " '.git/objects/19/8e92aa0d1a9d12951cacffa352ea18821d7d56',\n",
       " '.git/objects/19/94c78bd019cbea4d3d155ec9af6afd5266ba1d',\n",
       " '.git/objects/19/9c10c008f841341694ae96fb70236a711a7afd',\n",
       " '.git/objects/19/b14e66d703aef427f5ee361479ff44621c654c',\n",
       " '.git/objects/1a/1316cc4c2c327e20c74de5a2613528406fbcf2',\n",
       " '.git/objects/1a/2010795d5990d0ddfe9d21547769544c80cb4a',\n",
       " '.git/objects/1a/32a9f6afac44dd4d028c780dc5e51c735915ff',\n",
       " '.git/objects/1a/3df0d39d7dc58c826095e59048adf544b3180e',\n",
       " '.git/objects/1a/5252e4f59b059477f90e87d30cf776fac9e577',\n",
       " '.git/objects/1a/60c73ae882e675278a9a102c6ca2caa20e6f0b',\n",
       " '.git/objects/1a/7304612f8fd25b99b9422c67dae3d82dc923bf',\n",
       " '.git/objects/1a/7346bc2eafe34d13db853c4f237fdf6119d6e8',\n",
       " '.git/objects/1a/7ee8e9238f85670c13a968e7bc1ae4af696d63',\n",
       " '.git/objects/1a/82078e05629c52c1ad443db039fff19fb3cab4',\n",
       " '.git/objects/1a/85f7b187efd7c09d7de616514d9b7d1745f756',\n",
       " '.git/objects/1a/93323e6a4a9c5e2003fb6e1bc2b17a52bd5016',\n",
       " '.git/objects/1a/a6eb79cf04741a046e8355adcaf31321a70cdc',\n",
       " '.git/objects/1a/afe895dae4359a56a9d1079f614ce455b530c6',\n",
       " '.git/objects/1a/b75bf937b3ecc1c6543528363589cd52416a85',\n",
       " '.git/objects/1a/cf9817e1a3390ec36f6e3925f29c7d56f72c9f',\n",
       " '.git/objects/1a/d081688d263f4406a7cbd397e792992a52cce7',\n",
       " '.git/objects/1a/dc37c1c03a5c120ba20545d2009fac18944fc9',\n",
       " '.git/objects/1a/e378253d020159613bcca34a1128c9820995a2',\n",
       " '.git/objects/1a/f82d10a04b73fd3fbca248419c1107d656e40c',\n",
       " '.git/objects/1b/2700f58bb7a0af02e0539cb1a3e99166a5cbcd',\n",
       " '.git/objects/1b/3f7b580780e390333cbcce19963ff39e5f8734',\n",
       " '.git/objects/1b/449c134c3874c9de0a21faf6802e1e786a811a',\n",
       " '.git/objects/1b/4987e9e8c4fc3bb3cc39a2d380a56555fe6e46',\n",
       " '.git/objects/1b/4bebadd8738b2bbf33146493d6243bc6560874',\n",
       " '.git/objects/1b/7ee0300290198a7055ad4009c81888fee7652b',\n",
       " '.git/objects/1b/7f22841b21a6ceb74b5256203f37a8aa35305a',\n",
       " '.git/objects/1b/839a197506c1c9fffd086f1430ab92521cd78a',\n",
       " '.git/objects/1b/850d3f15dc1e143eedd6c41e518c7d12590702',\n",
       " '.git/objects/1b/85b33aeccdc0f526bc8b5e6d6b876bda6fb0ff',\n",
       " '.git/objects/1b/9b10a8344169206b22b4afee67c9ceb04022ca',\n",
       " '.git/objects/1b/a12b666565deed1b70c67a06c68ec20792ec57',\n",
       " '.git/objects/1b/a7a4a4d4e4ee8e90fe4835bd0a495117a15640',\n",
       " '.git/objects/1b/ae42e071dc433e97cb492dbd14b5e87dcef037',\n",
       " '.git/objects/1b/c39150db00a92cc26eba293e17b35c35e08682',\n",
       " '.git/objects/1b/cd9b41fe2e5291345cd06f9fdcbf556c2c0623',\n",
       " '.git/objects/1b/df66054c9a06721c9d52bb8f70017142bf7ad9',\n",
       " '.git/objects/1b/e53d329fc23abc37adf8b86d68b389170d5027',\n",
       " '.git/objects/1b/e65b6a80bfbd9681f65cdd6d4825c08099f4ef',\n",
       " '.git/objects/1c/37529ec7e3283e3f47f2329c2f337f3e18e4ae',\n",
       " '.git/objects/1c/426349c8175e669e306cf98087bfe2e6bafe1d',\n",
       " '.git/objects/1c/533506a1e62d1bcdfc5a0d02fccaf7544672d5',\n",
       " '.git/objects/1c/8016bfac54079a6bcd9dde032189aa7b07a546',\n",
       " '.git/objects/1c/81bc2ff782a1087f39aafa5c2aa753c6d402ca',\n",
       " '.git/objects/1c/84cef01ff0fbdeca382d84221c4d721c805605',\n",
       " '.git/objects/1c/99f1773a6bfbed50c2c7cce0b0925e0f2ea7eb',\n",
       " '.git/objects/1c/a80cde916380cd0c3e3039bed32e309f0e2d75',\n",
       " '.git/objects/1c/a999db076dff4b25027a0abf56d56bd614b9f8',\n",
       " '.git/objects/1c/acb57bade7aa26d6deab750050cada6fc74a33',\n",
       " '.git/objects/1c/d5a7c87ca0bef0cbae64c17b9259e0741e3154',\n",
       " '.git/objects/1c/da3745a8c7aa1fdf5fb97398496e2f4745c1b8',\n",
       " '.git/objects/1c/dc53affbfad03cd1d4f1fb94fcb236130bddbb',\n",
       " '.git/objects/1c/e3f5f7103372ed54112e4bcbe4c15d97598c52',\n",
       " '.git/objects/1c/f8feca2208c5011130af584c9e8f4c8d8bc26c',\n",
       " '.git/objects/1c/fb60c4ce9e199c0dad55e97cb8fb64de8861fe',\n",
       " '.git/objects/1d/0d66771b7c37ee642b8f8373297ee657b8de0f',\n",
       " '.git/objects/1d/24ba4ea19682187a9c9ab47032357aa7600896',\n",
       " '.git/objects/1d/2b68443d85d02c034a3bcf8ec40628c205908f',\n",
       " '.git/objects/1d/2bfd02827bc2ce3b0b1e0a14228b52ad8eae72',\n",
       " '.git/objects/1d/4116b45b5886e1c2fff03840b25df86bf09513',\n",
       " '.git/objects/1d/43c31b74fe9af7233b0bfb84fdbdccedf96823',\n",
       " '.git/objects/1d/4e50a2d66dd69e867db1bd908d1f399db141e7',\n",
       " '.git/objects/1d/52978e2b7690fe5d1033da5ee3c986dbb96a31',\n",
       " '.git/objects/1d/6da10dbf32e0105fdccdfcf29f4d8a923c7664',\n",
       " '.git/objects/1d/6e0876d5b00ec361d123def3aea3f76f565a17',\n",
       " '.git/objects/1d/70e1e56991d51615b1b865997c037a872e0829',\n",
       " '.git/objects/1d/81a75c56182d01c6197b877cb3ef491c2b5409',\n",
       " '.git/objects/1d/824e2df9b54c4f821f26b07c7a6f48ec0b84ef',\n",
       " '.git/objects/1d/a354117c28020f6f1a55028b21ac4570307925',\n",
       " '.git/objects/1d/b18288b6617314c79fbbee24f8244be30e375c',\n",
       " '.git/objects/1d/c18b93b6ce66dbf6a0fbf356c7f71b4b4738f7',\n",
       " '.git/objects/1d/df9c885e68901444d2e4fed1893799632b9953',\n",
       " '.git/objects/1d/ea7bcfba5be46b3fc1d8e0a34173f3ffebc003',\n",
       " '.git/objects/1e/0bca38c27a7faa76748adfd28afcdfb8a1681b',\n",
       " '.git/objects/1e/0f67a41f9a918fc8c27ed9efae06d0be2a932e',\n",
       " '.git/objects/1e/1828208d69213b6d3d0a6a5442f1b6f39d1a14',\n",
       " '.git/objects/1e/225b012a63b72c7677adbacebb98cb6f1d3b78',\n",
       " '.git/objects/1e/24078766e8c66695f6fac5ebe4e17150fc64fb',\n",
       " '.git/objects/1e/28a86aff515909034d4e985fc0327ca56c19a4',\n",
       " '.git/objects/1e/292a13a036e22805abc4b2be807ddf660d680b',\n",
       " '.git/objects/1e/4a46ed2b8c6066c82e57b67b1a0113060318ae',\n",
       " '.git/objects/1e/5ac7bde1d7ac33025e519c5a81528f0f118966',\n",
       " '.git/objects/1e/62b2b8888d21f805dee65df105a365076d6c1c',\n",
       " '.git/objects/1e/78834d636df9c61309eda95af7b4fb1d7ec223',\n",
       " '.git/objects/1e/7cc83ef0d892251f5c9b00946333e8f91918a9',\n",
       " '.git/objects/1e/889ad7e719a14ffa362f57bcef0ebf57a2d2f9',\n",
       " '.git/objects/1e/8be835c516df7017afbf3e99317cab9ac12eeb',\n",
       " '.git/objects/1e/98720c19c4eef45d31afcd8173a7d32ccbc705',\n",
       " '.git/objects/1e/a9087d9cd5f6e63171c1dc1012a65a81132539',\n",
       " '.git/objects/1e/b81e6ae0afe71cb70b89e7200b2a6d935c4562',\n",
       " '.git/objects/1e/c894c6a76f6c3225c850e68c446ea60de716cc',\n",
       " '.git/objects/1e/ca8f2366a8f273e7f6abf7700fba64fc62bfe0',\n",
       " '.git/objects/1e/d59ce31b3dab312b13bea290c6fc62b93ef359',\n",
       " '.git/objects/1e/f476a23b8c7196fa3290144d614c6794ce1df0',\n",
       " '.git/objects/1f/00c7bac54250ec77066a801a6868445b3a8799',\n",
       " '.git/objects/1f/034e628085eb1ae4ccc1296a204cb1d20dca2c',\n",
       " '.git/objects/1f/0408b1f71e28920387afa2b0644067a926ff87',\n",
       " '.git/objects/1f/2cd94f48085764798e4dfce751690d02fe71cd',\n",
       " '.git/objects/1f/3a9b7f59e3c1e021856b5d16f8767206324aa0',\n",
       " '.git/objects/1f/4816a9f798af8742db799169615a59553620e0',\n",
       " '.git/objects/1f/793d5905b44a980837f736e43df29fef753163',\n",
       " '.git/objects/1f/79f07b3b95d91e43cf630ccd91c9bb0045ade1',\n",
       " '.git/objects/1f/83a631e840891c379f8f5118b0636af2cf56bd',\n",
       " '.git/objects/1f/919c359294438ff9212807b2c8c2a3454c17d8',\n",
       " '.git/objects/1f/9d6d572f4da21d9ff5c629a543b7e1aa85c9d9',\n",
       " '.git/objects/1f/9f33d6c5ba9aaaa1136d853589b358d826ae43',\n",
       " '.git/objects/1f/a3489a685baa9cca7c0137a5d8ba9f5665ce63',\n",
       " '.git/objects/1f/a9bd4de0a569c855a8da1ab3350935e0fd85b3',\n",
       " '.git/objects/1f/ab291e0d5d67899b2331fbeead693be7752f09',\n",
       " '.git/objects/1f/c92d444c0bff499da57571eccd736e30c5154c',\n",
       " '.git/objects/1f/d59a6fe93314b55a27e5fe455a36d3ae34b7c7',\n",
       " '.git/objects/1f/d9e3d70909fb47f232fc4fda9aeb3731f1738d',\n",
       " '.git/objects/1f/df325ef749e19e646d3081a7807b20e783d8be',\n",
       " '.git/objects/1f/e7d810dc8e8c2f30527a037fa59947983f327a',\n",
       " '.git/objects/1f/f45cac1c297a1b6311118758924f373c00c0c7',\n",
       " '.git/objects/1f/f8e6f86d3ad04d8f30be81047d1ed745e792c0',\n",
       " '.git/objects/20/198b4920b07a2fd1500c6e57c9f6fd37800f19',\n",
       " '.git/objects/20/19accabfe0fe3742ca26499c7627e685b81c19',\n",
       " '.git/objects/20/1aeaf5e00020c9309d1176da67f0a7d09aa48a',\n",
       " '.git/objects/20/43e5d420b0e91f04c9852bab408b4178180559',\n",
       " '.git/objects/20/6180f057140f389211e96790ec720d9292143a',\n",
       " '.git/objects/20/66efce7c05f284fef7c7c8dcd8048bd1540425',\n",
       " '.git/objects/20/6a89dc331d45b6268fc319e4ca4e620d21242c',\n",
       " '.git/objects/20/87ebb73c736b9322611dab9f95366071ddc940',\n",
       " '.git/objects/20/8d5e3151158b50ae1176886117c93643a1e8a7',\n",
       " '.git/objects/20/9ea353a1587ac92f16515b302b45a4740f98ae',\n",
       " '.git/objects/20/dd23d61ed46457ada34f401d3f80cc15c2549f',\n",
       " '.git/objects/21/0ec400e3892586017f816fb924960a82053d78',\n",
       " '.git/objects/21/130f870e2c4fd857742e938139ae55fc3e84d0',\n",
       " '.git/objects/21/16f6412bd0208668a916626f7a22878b8967af',\n",
       " '.git/objects/21/208d551bfbacb55a59308ddd32b522d59c5913',\n",
       " '.git/objects/21/3e72f2aec3e7267e688ede9729cabf63fc11e7',\n",
       " '.git/objects/21/51982cee0f7dfc9c62a927abbe1cded09bb4d8',\n",
       " '.git/objects/21/681f3c911c1a47487287afa6e699cd1f22708d',\n",
       " '.git/objects/21/6febf753d1eba4ad51a2b6bfba0ac157a02c62',\n",
       " '.git/objects/21/8f08f13d001703825c4c59ed0acd2b4e1a5d96',\n",
       " '.git/objects/21/aa6c91e487448d47f3065ddee27589065e91d5',\n",
       " '.git/objects/21/b2ea07ab7f9e76570eff2f08c095223f3558b8',\n",
       " '.git/objects/21/c39e76f1ac453983715097eeea278cd322b4d6',\n",
       " '.git/objects/21/c44851c28a8eb53b2a0bb8fe287bec023b37fe',\n",
       " '.git/objects/21/c8035e169ce49dc2a0c486c176e63a16213256',\n",
       " '.git/objects/21/dcde80d7c5564273f7695fe6d3981e9055901d',\n",
       " '.git/objects/21/fab8e704d28b0eb968653ca3cdccdeda1f12c5',\n",
       " '.git/objects/22/0ac1ed8b788594fede2ddcebbbd5a350eccbf1',\n",
       " '.git/objects/22/2d3cde0954214b2d851f2cec0073772e40c67f',\n",
       " '.git/objects/22/2e0ecc0dda61da01b122f93f395d5cb1be8e69',\n",
       " '.git/objects/22/31d6ffbdf1a747ebb6cf6f1b29aea5e951b46b',\n",
       " '.git/objects/22/4432d466914390546ced60411d8dc9b583eda3',\n",
       " '.git/objects/22/517092a19b9974855f1be242c7739b19808746',\n",
       " '.git/objects/22/565e0dd8a7e001d8a463723c92531ed4a8ef57',\n",
       " '.git/objects/22/5dab15846023bf12faa11dc03c3f6cdb2fa760',\n",
       " '.git/objects/22/66368504f9c6ab639f890f10a0b4ce9cf33f84',\n",
       " '.git/objects/22/b30e4ee7792e77ba27956c552f20a400b8dbf8',\n",
       " '.git/objects/22/befc9629d696d7bb11ae6f9ea2805583ee6c88',\n",
       " '.git/objects/22/c1a5668662bc555a5c256978e8cd68c34db262',\n",
       " '.git/objects/22/ca1d0e04de8fa3907c856f5b02dcfd3388d47d',\n",
       " '.git/objects/22/d45e5796cee9f3dd5bb5805250dc40e582ed82',\n",
       " '.git/objects/22/db338434fbd65ed32195023f43fb3e89e301cf',\n",
       " '.git/objects/22/eeff5757ff758be4c8c16d5253968fef0bd15f',\n",
       " '.git/objects/22/fd18292a427bfc6026d4f6ca43dd46cc3fb915',\n",
       " '.git/objects/23/2a82034cfa06d3568410ef08968d4728cdc2bd',\n",
       " '.git/objects/23/3e06d50b58892d57be034367f258c692575dee',\n",
       " '.git/objects/23/50e92af31cfad08f584464ed7b37f1631fa01b',\n",
       " '.git/objects/23/5151cb16b6793001261d0e1a65f2192c6032c0',\n",
       " '.git/objects/23/587da9e2e4664c015d7314809869931e92bcae',\n",
       " '.git/objects/23/6d82778655d03b88102c06adcbd305ef473266',\n",
       " '.git/objects/23/70bba435d2e39d5ffd7cb1ba4862e3bca900dd',\n",
       " '.git/objects/23/723c374218d6fea377b311a72802b4cba2b70d',\n",
       " '.git/objects/23/91afc9c0cbfa3b2cca78f65045c6f5c65f2c15',\n",
       " '.git/objects/23/9881850327b6bf4b1d6780703ba150213fef05',\n",
       " '.git/objects/23/a146d53e8e0115e18ee167540b4019fa642fbc',\n",
       " '.git/objects/23/a5df666a78c93bc260d6f0e442f8ab5dfcf029',\n",
       " '.git/objects/23/bc6bf7958c5acaad1f793b192383f744f5e4ce',\n",
       " '.git/objects/23/c705e8c11ef0f12f9e4bea21d0c570dbeb4ff4',\n",
       " '.git/objects/23/d9551b832721427af6ea99f749c0413ac7acf1',\n",
       " '.git/objects/23/e11081f611e1ffbddc5d507cf0a0dc19eabd3a',\n",
       " '.git/objects/23/ed6ac5af0d5df298f65179719353c6290f340f',\n",
       " '.git/objects/23/f8f290f048989239411f3a1f18bc6ecf49ae09',\n",
       " '.git/objects/24/0873b8ae09e9a10f0758d48ebbceb97100c8a1',\n",
       " '.git/objects/24/446a10b092a4de4c73824ab532f792cbd15aea',\n",
       " '.git/objects/24/478a8f361fe5694017e78333908013c87b3b9b',\n",
       " '.git/objects/24/4de7bdc9901a924ecf40ec437c2dd9c6d03cde',\n",
       " '.git/objects/24/5cd4578cfd4230b070e31b83af3f67ed2c6bf8',\n",
       " '.git/objects/24/62fa35871a0144d8a3180a72f119e5ca5b92a8',\n",
       " '.git/objects/24/8a61318ac0f77cec8892ed3c2a3df9b664fb9f',\n",
       " '.git/objects/24/8f5e85bdadad94c17b823ae6e8545f1cc7384b',\n",
       " '.git/objects/24/9766590460d4fb82d2ea0b04dcf070e2cc50c7',\n",
       " '.git/objects/24/a8ee14899d1277193df0310ae12347b54e2d5e',\n",
       " '.git/objects/24/be5edd5739535fc2904697e3a902d4493bb7b4',\n",
       " '.git/objects/24/c5c1da08395d3a3d633d617cd2817e9a15489c',\n",
       " '.git/objects/24/e560be5cd436ce067bd8cc8d241bbb62df871a',\n",
       " '.git/objects/24/ed8a283653ac335800aeb9076949ce4ae9ccc8',\n",
       " '.git/objects/24/f472b3e4655a0a71b5a044451f0c8bc895802b',\n",
       " '.git/objects/24/faf3101f8dbcb1bf8f4990121bd9b99df6b1f0',\n",
       " '.git/objects/25/0f0b40f2a703ffc67889a6f59d2cf4ae6b4b71',\n",
       " '.git/objects/25/18b60cbba261a642985ecd0e2836e5a1068e9a',\n",
       " '.git/objects/25/1a9e63ec05227ff0d8eb3893400bc15fe26d5c',\n",
       " '.git/objects/25/233b97909edffe5dd7f0a680d7309975e89f00',\n",
       " '.git/objects/25/5ee1cb5d101d1aad196dcf0be15369485dc09e',\n",
       " '.git/objects/25/7818b62502c897f04f5e5d55c3761dba1731f4',\n",
       " '.git/objects/25/7f03a72949fa9ca95787b92a24529a59547299',\n",
       " '.git/objects/25/835cc1f64006fd435534ef5366176f607a9f28',\n",
       " '.git/objects/25/a2906455d81b1390d31657e0909e609f001d3f',\n",
       " '.git/objects/25/cae60a6140f59d9efc552faacdda4caa07449c',\n",
       " '.git/objects/25/e4353c84ab3563b3a1451d16d500a2b1fea1f4',\n",
       " '.git/objects/25/e900116fe5cc975bb32fac730ed4ca0067b282',\n",
       " '.git/objects/25/f2badee0580265bce63452cf07426c671db16f',\n",
       " '.git/objects/25/f8bc07c2d91f5af2494021d5973de1ddbc3f23',\n",
       " '.git/objects/26/02ba6a483378b461c4c54fa8b03963088d65bd',\n",
       " '.git/objects/26/0b88112514ebcc7a01a0fb9aad2e9e0a29f531',\n",
       " '.git/objects/26/0bd3ee1e721b06bf07f7c5a3f7d227df49a0a7',\n",
       " '.git/objects/26/0c2d1461f4e692f0dcb36c006d7b277dc65f5f',\n",
       " '.git/objects/26/1a6e35981493e6d3f28d7dc08673fa8abb73c7',\n",
       " '.git/objects/26/201b917a74721b6b56ae655f2dd669dd586a41',\n",
       " '.git/objects/26/2a179bdad9f3f21808281833250d282f3d6574',\n",
       " '.git/objects/26/3d9bb06af6822c52133e462a4728b24d9e70f8',\n",
       " '.git/objects/26/4b0a5d70bd31232a58eaa9733a795f879f2736',\n",
       " '.git/objects/26/4e9ead7258dca931f0842dcfddda9a2216ac29',\n",
       " '.git/objects/26/5640a3ba09c1540495838dcc7c7d2adfe31ecb',\n",
       " '.git/objects/26/7a68343d3c97bbdc72af1820313dc9c29473f9',\n",
       " '.git/objects/26/88deac46c673a9fd5dac1c9eb86cdf7197d1d1',\n",
       " '.git/objects/26/9631194fc7928852d8b1a4cc50e6794b85ce13',\n",
       " '.git/objects/26/af1757c6f0d8c7fcf8fae6f46bbc05e57631ec',\n",
       " '.git/objects/26/c90b813d40df140efe3e16b2b405e3da12f850',\n",
       " '.git/objects/26/cdf655de6d385cf1d6a21194d48f617cc95a5d',\n",
       " '.git/objects/27/07ea0205371b6e1b0a141f56f706f19aed7dac',\n",
       " '.git/objects/27/0a71731819cdbfc6f763b10a136d55651c97dd',\n",
       " '.git/objects/27/15b74a1874c3ece1a24a18856c1f91f80933c4',\n",
       " '.git/objects/27/1e3d41756886e43799da7b6c32177c7e805182',\n",
       " '.git/objects/27/345c420ba3bf852cc41d0687e62019386de629',\n",
       " '.git/objects/27/373c8530c94128c344d50040fe4b6374ab8167',\n",
       " '.git/objects/27/51191d40bf795849ed922019304801bf327a72',\n",
       " '.git/objects/27/7e8b79653e23760b8d575f0f652e7de7d07a72',\n",
       " '.git/objects/27/848ef75c82acf8f6373c49fd5ce76c33849b0c',\n",
       " '.git/objects/27/a05e5aa99e02696fa0990f008e193095b0365a',\n",
       " '.git/objects/27/a8e8bc37254d92b4686bec72927c4e55135043',\n",
       " '.git/objects/27/b0ff864d52f69afc7441a850ceb4a9c6613df1',\n",
       " '.git/objects/27/df2ef436d5b5c82b824989301255fe80401ce8',\n",
       " '.git/objects/28/09739823d451ba87e086c680788506d53dfe1e',\n",
       " '.git/objects/28/156186d035d00ee2f5a2fad6a48fc856cbb69a',\n",
       " '.git/objects/28/15811d3cf2c991c5678891730da485afb455cc',\n",
       " '.git/objects/28/25b06324390454a7c224111b8e0e72a41f80ed',\n",
       " '.git/objects/28/40a72631cd34640ca2b226eae624ff02fb4078',\n",
       " '.git/objects/28/43394f253ea61bd24e677044185b995bd0efef',\n",
       " '.git/objects/28/57b34d88f64ced11e1e6187eb7f6c837efc584',\n",
       " '.git/objects/28/7b80f3ddb90b367dd909eca199bf9776f60610',\n",
       " '.git/objects/28/919c62c0641279b6073124b609791659f83100',\n",
       " '.git/objects/28/a07035cb1ae15ca26fe7b4248b3de76555feba',\n",
       " '.git/objects/28/a5eeedeb446c820a29e53c49b0fb913e2824b0',\n",
       " '.git/objects/28/afca31a38aae2a6a50657f84fd51530093c867',\n",
       " '.git/objects/28/b19aaaea2643d0ad188e075c93a075ff3241a4',\n",
       " '.git/objects/28/b1cd00cc82112d30861db099cba72591b4dd68',\n",
       " '.git/objects/28/b7f7441c9ab0865effbcf627ba2f3cad23b64a',\n",
       " '.git/objects/28/b888d12bc0c6ab7f6f2e2aaf10567bc95f9d0f',\n",
       " '.git/objects/28/b9baa2f6e7777f19092699a541bd93f6de2cf0',\n",
       " '.git/objects/28/bef02a21f8932a4cfbaa3e59379a1c6707b751',\n",
       " '.git/objects/28/ecd40561e75a8c03728002f6b086a1ae8aaa80',\n",
       " '.git/objects/29/00bc2fae2d0aa9d5f46b3d2a062618f0ba4153',\n",
       " '.git/objects/29/122cf1aec05e82b3d69aa4c81d90ae67e65ca1',\n",
       " '.git/objects/29/16546dd6026f3d27f36c64993d275beb41d125',\n",
       " '.git/objects/29/227a02fc99700699b85e95eb9b87889496deef',\n",
       " '.git/objects/29/22fe193a538602c899cb788a0b3cb7d3debf4c',\n",
       " '.git/objects/29/4c51e47d0902be5ab7a097600b228bffe08a46',\n",
       " '.git/objects/29/626c9068e1d92b6b5019b03148a96e57070f31',\n",
       " '.git/objects/29/7970df63a6faad1e9f381d25ae88b5ee087c44',\n",
       " '.git/objects/29/966e6fd81faecbd17df5e615d91edc60fb905f',\n",
       " '.git/objects/29/9ddc725481e9e5f81db144304873d3ea4c7812',\n",
       " '.git/objects/29/a31ab914d91d800025e98e8ef1adceefcb53e6',\n",
       " '.git/objects/29/abdcb2a8c590bc37e4d45f46586d994494cf9d',\n",
       " '.git/objects/29/b5383286ef4075766f0fc0e1ffbfe7060bf896',\n",
       " '.git/objects/29/cd0b1358abef2225e54c3669352bf40ce0752a',\n",
       " '.git/objects/29/ceaf28caed37ad45bec5e8ec9ca53bd29da739',\n",
       " '.git/objects/29/d701ace991a795d6cf4d1884a1bcc21324e70d',\n",
       " '.git/objects/29/d810b7b0b2092783039976bd3098d1ead29596',\n",
       " '.git/objects/29/e37b6d714b2523bd962b549ffcd6cb7e133818',\n",
       " '.git/objects/29/eee87528953dccb031a2ba24b8900f304e92f1',\n",
       " '.git/objects/29/fb99a7854a220cb0de37f98a0304934f46be75',\n",
       " '.git/objects/2a/002a4f7814e51b72dd299bba8dea7eb55072a2',\n",
       " '.git/objects/2a/0b3db0e4a67f4c22a35c555dfd68064caaeca6',\n",
       " '.git/objects/2a/22496c2ac30b765e89b08827e1846a5e15b740',\n",
       " '.git/objects/2a/29d7d78fef53d03842373b39defd892d63a62b',\n",
       " '.git/objects/2a/3263b20d4c35848b741c2755cc0aa930747b3c',\n",
       " '.git/objects/2a/54a2509ce87a3dab3d2a9fe40f2710299711d7',\n",
       " '.git/objects/2a/57e686107b836578ebb5de8d5191fe40420499',\n",
       " '.git/objects/2a/64469d37be9ccc7b65633364d2d57f388df48e',\n",
       " '.git/objects/2a/7e2d35c5e7d9192ea11735431b95524c028dc2',\n",
       " '.git/objects/2a/85ed6b83d164f5ae058b3e5e9d401319da213d',\n",
       " '.git/objects/2a/9c3022abb31ffe2cfb501ff392309f36a80c6d',\n",
       " '.git/objects/2a/c1bb148c6df349465b93f3a2865f3b38c4b67d',\n",
       " '.git/objects/2a/c757686381e7a70afc0b0c9b3264ae8af69977',\n",
       " '.git/objects/2a/e0335f1bdf5cd1f44a83c7735e286c5eddf861',\n",
       " '.git/objects/2a/eeac88baacffa4c6a3549033fc97c1c7649f4e',\n",
       " '.git/objects/2a/fd4d8aac752c8657cb100fc163b5b186519c80',\n",
       " '.git/objects/2b/056f6a099606d06fa349b81af0ea73b3848104',\n",
       " '.git/objects/2b/0bdcd118527fbd7cde500b90f0b259299fd80b',\n",
       " '.git/objects/2b/0c8537f506bb65d69b24266786908e4be3f793',\n",
       " '.git/objects/2b/135e64d583befef52f0f5bc8b1cc74a6a27a87',\n",
       " '.git/objects/2b/28b37395dfb88760edab6b10973d64bb710dc7',\n",
       " '.git/objects/2b/2e9c09d2ac1c0fb8ea55919736a153270d967a',\n",
       " '.git/objects/2b/5e5a2d9ab45ca8d7efb01fa2a8a3beb36cf57c',\n",
       " '.git/objects/2b/654e14f0b22c46a96899a07b5655b7d286dee2',\n",
       " '.git/objects/2b/809fe04a7cbd3136e0135e8243206fe03730a1',\n",
       " '.git/objects/2b/8577999c10e467e680e2d4ffe459342e43e879',\n",
       " '.git/objects/2b/979be0b5f6dd1ffe5251d2e17cfb89f760a3a0',\n",
       " '.git/objects/2b/a456a4a30f29e1df273d1991ee2604cbcb79b0',\n",
       " '.git/objects/2b/b77aa931e91863b449bf5229f6952594b09be4',\n",
       " '.git/objects/2b/bc20c9224b0372a73e2e10d7f78ca25c8dc11f',\n",
       " '.git/objects/2b/bd8277bb80137aa44ae5ac4490143184131230',\n",
       " '.git/objects/2b/c1bf1f287f1793b4f2e9703e281c17c1865785',\n",
       " '.git/objects/2b/ce5d098443d4e6283aa4278446a3925ba89e66',\n",
       " '.git/objects/2b/d7d5d5af06f1b92989ab8848a9bf2898a25fb5',\n",
       " '.git/objects/2b/dcbd3f025a8e8eaf7b2c1190fb0dfcd2b759c0',\n",
       " '.git/objects/2b/dde1fffeabee9f5133f7f9427493d573617c00',\n",
       " '.git/objects/2b/e018be1ba5f3958699f9d4f395c52bcd80c394',\n",
       " '.git/objects/2b/e451c28db0a21a910058ef43e00679d0e878a1',\n",
       " '.git/objects/2b/e470bf5c75ab4e66f107cc1582b31a078cde19',\n",
       " '.git/objects/2c/0dc56c63f3cae9a13520ebb1ed4adb9c5cc286',\n",
       " '.git/objects/2c/1e2b17495fc07e6930ab38247e3b40c9d1fd6f',\n",
       " '.git/objects/2c/321ebd06b123ce1f5e1e9efe907c5b19b5fd2b',\n",
       " '.git/objects/2c/376cec7025933b206ba1d28233bd6957f75d0b',\n",
       " '.git/objects/2c/3dc12aece7f01d3aaf043c3f06746d272d61ff',\n",
       " '.git/objects/2c/47130fd154786459017dd497de3cfed8e58acb',\n",
       " '.git/objects/2c/4f8f8c652b3bcd154a75a6f384669b6488d27f',\n",
       " '.git/objects/2c/68180a09fdd161c7a3b8c3ac2bab52614e7de4',\n",
       " '.git/objects/2c/6834597127dee841027361b5a913bc0bdc768d',\n",
       " '.git/objects/2c/6c747b13896a894e9b623388b5f14c237c9345',\n",
       " '.git/objects/2c/738eccb145557bc4ab0ada8b33ccedc4309a4d',\n",
       " '.git/objects/2c/acdc6622d6e4b010ebb17a6c55c6970452cbb3',\n",
       " '.git/objects/2c/ae9808efae04b772eafe42ba5b3a64069c4252',\n",
       " '.git/objects/2c/b0ff6e751065c38a2152cfd6533301f4439e02',\n",
       " '.git/objects/2c/c6dd664a1596b0de01bf83a11992c6da7d73e9',\n",
       " '.git/objects/2c/ea948f57e441136fc142a4cd8b9ccc9e915ae8',\n",
       " '.git/objects/2c/ea9f42310187b4f157c48dfb769b92b47f1720',\n",
       " '.git/objects/2c/ff5f00096b9c7d1ec905d26142fcac3e8e8b1d',\n",
       " '.git/objects/2d/0302f091d45d2d0b4d767f8ca5375ae174c9f9',\n",
       " '.git/objects/2d/0c827c7d30b37fc165be9c994f427e2d47270e',\n",
       " '.git/objects/2d/3d050d91b94efa387298a28d9e0e68a31257ea',\n",
       " '.git/objects/2d/3e1ebec785398c98dc2f80f55b5da13e7eb3fe',\n",
       " '.git/objects/2d/3e909aab6b1721a7855cac6616180f6d487589',\n",
       " '.git/objects/2d/593daf455a19e4abf91e7a40b6a397746e6e8a',\n",
       " '.git/objects/2d/5a5055e1ce49f8931f6435f7adde92a1bf0151',\n",
       " '.git/objects/2d/61119b643ec407d1ad98853e63c57958434014',\n",
       " '.git/objects/2d/690245a5a0f9952ac4253949e552caee9f5d5a',\n",
       " '.git/objects/2d/6ca662e23a12370191337bd3faa91d6f9b34a0',\n",
       " '.git/objects/2d/873325b235e82884a4e272d6d1ebdcab964580',\n",
       " '.git/objects/2d/9692578bf56d63ff8b590e1b7a8b05e41cb0f3',\n",
       " '.git/objects/2d/9fc19a9e77e5a1a76a0e09e80dd2df2b416e7c',\n",
       " '.git/objects/2d/a0054f3eb0496dab85c99c4274fa708c3f2ba1',\n",
       " '.git/objects/2d/a1d75d9a8ca60dc1e98c361eeb5fa569dd2ed3',\n",
       " '.git/objects/2d/aa0fec4acb3f447e3f2751a1c6dca39665f751',\n",
       " '.git/objects/2d/b56e0f0c2c6dd598a2abc421e9ae93af3a8b19',\n",
       " '.git/objects/2d/b66b0c7762e30851d067557ee1e74be61e3ae4',\n",
       " '.git/objects/2d/ca96040120b20c821492ea262f3b1b4dee74d1',\n",
       " '.git/objects/2d/e854966caa2faf25e59216dad28bce573cdb6c',\n",
       " '.git/objects/2d/f635153544e137d08d96bc85b84cb4030ef745',\n",
       " '.git/objects/2e/03453f5bb0a1c1058f4751ce6711433d21ef31',\n",
       " '.git/objects/2e/178f9f0fdf79dd2ca6ee5a8faa70ccb5a75cdb',\n",
       " '.git/objects/2e/1fa34bdbda82fcb8235ac81208cc2faa26a950',\n",
       " '.git/objects/2e/303301a86177b2b8e3f9477e972f696230158e',\n",
       " '.git/objects/2e/408e087d24c67fdd488213e9c9bd8b4d82c239',\n",
       " '.git/objects/2e/49b43e7fc5be266f8c0a753412bd67fdabd193',\n",
       " '.git/objects/2e/54345d272ecc693186f29641861c69da934d45',\n",
       " '.git/objects/2e/5d2dfd54fd16ed478c0de068a2bf659dcd1699',\n",
       " '.git/objects/2e/6a1f5ed48416ebf112e3261a2fde8da684212a',\n",
       " '.git/objects/2e/6b14e418d872a44a3bf0b4074c48b6c45b9804',\n",
       " '.git/objects/2e/6b5da2d09445757f149b3132f421a9a808c2e1',\n",
       " '.git/objects/2e/754122eefb6bd8172e8196cd75b54d77ce4ef9',\n",
       " '.git/objects/2e/7773ae1387c48f63f559267522a19f09e492e8',\n",
       " '.git/objects/2e/9c5d82cbf8290d7104c93652243a6112bfd787',\n",
       " '.git/objects/2e/9f53768baf477be0a379621c6b7f993b571917',\n",
       " '.git/objects/2e/a934157846a749ac0dbad09a4f6ec1d056bff4',\n",
       " '.git/objects/2e/b0ee3edbdb4a28372f33838cd0989512c5513c',\n",
       " '.git/objects/2e/b357776535ed335eda94814c39bd9434051668',\n",
       " '.git/objects/2e/b6fee8c68dbb0338b5d98e893401842bd45d84',\n",
       " '.git/objects/2e/b78a43063a7429f4eb7df0f77f82dc40c982a3',\n",
       " '.git/objects/2e/bbf8947ad4bfba20c482a9660523fbfbc4b128',\n",
       " '.git/objects/2e/cb430bb32b400dd93dcdc4a5e2245b09d000d2',\n",
       " '.git/objects/2e/d7b9123435659b6b3f83bf49d484add65cedcf',\n",
       " '.git/objects/2e/d917230c74661f7170a846e2f24be700692454',\n",
       " '.git/objects/2e/e764dfba120b0c8943e3d8e9979a6bc2eacb8a',\n",
       " '.git/objects/2e/eec4d8082d2bdfc9764a1167264493b4dd8330',\n",
       " '.git/objects/2e/f7e0c7e74a5ea33ec545610533ea314b84b2c3',\n",
       " '.git/objects/2e/f9537a0a77d2e7b2b5827d93108bfab546806b',\n",
       " '.git/objects/2f/0759d8db00fa1029488287776119d146c403be',\n",
       " '.git/objects/2f/087d5869e2c558ac4e9f2b6d0c7c7f627c875f',\n",
       " '.git/objects/2f/0b501920536e36f7f6320a5b943d8582e2f5da',\n",
       " '.git/objects/2f/2abcdc74101b98977fb0aaaeb4c7e411b56874',\n",
       " '.git/objects/2f/3d80556d52a14ccc3186a6a66fbc187cb5a741',\n",
       " ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = '/Users/benkaufman/repositories/dxspaces-soa/'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordlists.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Random Text With Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'the'),\n",
       " ('the', 'beginning'),\n",
       " ('beginning', 'God'),\n",
       " ('God', 'created'),\n",
       " ('created', 'the'),\n",
       " ('the', 'heaven'),\n",
       " ('heaven', 'and'),\n",
       " ('and', 'the'),\n",
       " ('the', 'earth'),\n",
       " ('earth', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven', 'and', 'the', 'earth', '.']\n",
    "list(nltk.bigrams(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writes 15 words by picking a word. Finding the most common word to follow it, prints it and repeats\n",
    "def generate_model(cfdist, word, num=15):\n",
    "    for i in range(num):\n",
    "        print(word, end=' ')\n",
    "        word = cfdist[word].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "bigrams = nltk.bigrams(text)\n",
    "cfd = nltk.ConditionalFreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created he said , and the land of the land of the land of the "
     ]
    }
   ],
   "source": [
    "generate_model(cfd, 'created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordlist Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of words from text that do not show up in the english dictionary\n",
    "def unusual_words(text):\n",
    "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    unusual = text_vocab - english_vocab\n",
    "    return sorted(unusual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# high-frequency words with little lexical content\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Returns the percentage of text that are not stopwords\n",
    "def content_fraction(text):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    content = [w for w in text if w.lower() not in stopwords]\n",
    "    return len(content) / len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7364374824583169"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_fraction(nltk.corpus.reuters.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wordlist has first names categorized by gender\n",
    "names = nltk.corpus.names\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAENCAYAAAAc1VI3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4/F4PIHwHYbH4/F4AuE7DI/H4/EE4j8w6F8RdK5eTgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1093390f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Shows frequency distribution of female and male last letters of first names\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (fileid, name[-1])\n",
    "    for fileid in names.fileids()\n",
    "    for name in names.words(fileid))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pronouncing Dictionary (CMU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('anomalous', ['AH0', 'N', 'AA1', 'M', 'AH0', 'L', 'AH0', 'S'])\n",
      "('anomaly', ['AH0', 'N', 'AA1', 'M', 'AH0', 'L', 'IY0'])\n",
      "('anomie', ['AE1', 'N', 'AH0', 'M', 'IY0'])\n",
      "('anona', ['AA0', 'N', 'OW1', 'N', 'AH0'])\n",
      "('anonymity', ['AE2', 'N', 'AH0', 'N', 'IH1', 'M', 'IH0', 'T', 'IY0'])\n",
      "('anonymous', ['AH0', 'N', 'AA1', 'N', 'AH0', 'M', 'AH0', 'S'])\n",
      "('anonymously', ['AH0', 'N', 'AA1', 'N', 'AH0', 'M', 'AH0', 'S', 'L', 'IY0'])\n",
      "('anora', ['AA0', 'N', 'AO1', 'R', 'AH0'])\n"
     ]
    }
   ],
   "source": [
    "entries = nltk.corpus.cmudict.entries()\n",
    "for entry in entries[4371:4379]:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the level of stress put on saying all parts of a word\n",
    "def stress(pron):\n",
    "    return [char for phone in pron for char in phone if char.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aerogenosa',\n",
       " 'aleatory',\n",
       " 'alienated',\n",
       " 'alienated',\n",
       " 'alienating',\n",
       " 'alphabetizes',\n",
       " 'alphabetizing',\n",
       " 'ambulatory',\n",
       " 'angioplasty',\n",
       " 'arianism',\n",
       " 'brutalization',\n",
       " 'cannibalism',\n",
       " 'cannibalizing',\n",
       " 'capitalism',\n",
       " \"capitalism's\",\n",
       " 'capitalizes',\n",
       " 'capitalizing',\n",
       " 'categorizes',\n",
       " 'categorizing',\n",
       " 'characterizes',\n",
       " 'characterizing',\n",
       " 'choreographing',\n",
       " 'circularizing',\n",
       " 'circulatory',\n",
       " 'corporatism',\n",
       " 'counterattacking',\n",
       " 'counterattacking',\n",
       " 'counterproductive',\n",
       " 'counterproductive',\n",
       " 'counterproposal',\n",
       " 'counterproposal',\n",
       " 'counterproposals',\n",
       " 'counterproposals',\n",
       " 'criminalizing',\n",
       " 'disciplinary',\n",
       " 'factionalism',\n",
       " 'favoritism',\n",
       " 'federalism',\n",
       " 'federalizing',\n",
       " 'generalizing',\n",
       " 'gradualism',\n",
       " 'halogenated',\n",
       " 'heterodoxy',\n",
       " 'hooliganism',\n",
       " 'hyakutake',\n",
       " 'hyakutake',\n",
       " 'hydrogenated',\n",
       " 'hydrogenating',\n",
       " 'immunomedic',\n",
       " 'immunomedics',\n",
       " 'industrivaerden',\n",
       " 'interminably',\n",
       " 'liberalism',\n",
       " \"liberalism's\",\n",
       " 'liberalizes',\n",
       " 'liberalizing',\n",
       " 'marginalizes',\n",
       " 'marginalizing',\n",
       " 'militarism',\n",
       " 'minimalism',\n",
       " 'monetarism',\n",
       " 'mutualism',\n",
       " 'nationalism',\n",
       " 'nationalizes',\n",
       " 'nationalizing',\n",
       " 'naturalism',\n",
       " 'naturalizes',\n",
       " 'naturalizing',\n",
       " 'negativism',\n",
       " 'orientated',\n",
       " 'oscillatory',\n",
       " 'overambitious',\n",
       " 'overconsumption',\n",
       " 'overdependence',\n",
       " 'overdependent',\n",
       " 'overreacted',\n",
       " 'overreacting',\n",
       " 'overreaction',\n",
       " 'overreliance',\n",
       " 'oxygenated',\n",
       " 'parkinsonism',\n",
       " 'pastoralism',\n",
       " 'patriotism',\n",
       " 'personalizes',\n",
       " 'personalizing',\n",
       " 'policyholder',\n",
       " \"policyholder's\",\n",
       " 'policyholders',\n",
       " \"policyholders'\",\n",
       " 'policymaker',\n",
       " 'policymakers',\n",
       " 'policymaking',\n",
       " 'polymerizes',\n",
       " 'popularizer',\n",
       " 'popularizing',\n",
       " 'positivism',\n",
       " 'proselytizing',\n",
       " 'protestantism',\n",
       " 'puritanism',\n",
       " 'puritanisms',\n",
       " 'radicalism',\n",
       " 'rationalizing',\n",
       " 'reasonableness',\n",
       " 'regulatory',\n",
       " 'relativism',\n",
       " 'respiratory',\n",
       " 'secularism',\n",
       " 'separatism',\n",
       " 'stereotyping',\n",
       " 'stereotyping',\n",
       " 'superconducting',\n",
       " 'superconductor',\n",
       " 'superconductors',\n",
       " 'telecommuter',\n",
       " \"telecommuter's\",\n",
       " 'telecommuters',\n",
       " 'trivializes',\n",
       " 'trivializing',\n",
       " 'underachiever',\n",
       " 'underachievers',\n",
       " 'underperformer',\n",
       " 'underperforming',\n",
       " 'videoconference',\n",
       " 'videotaping']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w, pron in entries if stress(pron) == ['1', '0', '0', '2', '0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['F', 'AY1', 'ER0'], ['F', 'AY1', 'R']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prondict = nltk.corpus.cmudict.dict()\n",
    "prondict['fire']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'you (singular), thou',\n",
       " 'he',\n",
       " 'we',\n",
       " 'you (plural)',\n",
       " 'they',\n",
       " 'this',\n",
       " 'that',\n",
       " 'here',\n",
       " 'there',\n",
       " 'who',\n",
       " 'what',\n",
       " 'where',\n",
       " 'when',\n",
       " 'how',\n",
       " 'not',\n",
       " 'all',\n",
       " 'many',\n",
       " 'some',\n",
       " 'few',\n",
       " 'other',\n",
       " 'one',\n",
       " 'two',\n",
       " 'three',\n",
       " 'four',\n",
       " 'five',\n",
       " 'big',\n",
       " 'long',\n",
       " 'wide',\n",
       " 'thick',\n",
       " 'heavy',\n",
       " 'small',\n",
       " 'short',\n",
       " 'narrow',\n",
       " 'thin',\n",
       " 'woman',\n",
       " 'man (adult male)',\n",
       " 'man (human being)',\n",
       " 'child',\n",
       " 'wife',\n",
       " 'husband',\n",
       " 'mother',\n",
       " 'father',\n",
       " 'animal',\n",
       " 'fish',\n",
       " 'bird',\n",
       " 'dog',\n",
       " 'louse',\n",
       " 'snake',\n",
       " 'worm',\n",
       " 'tree',\n",
       " 'forest',\n",
       " 'stick',\n",
       " 'fruit',\n",
       " 'seed',\n",
       " 'leaf',\n",
       " 'root',\n",
       " 'bark (from tree)',\n",
       " 'flower',\n",
       " 'grass',\n",
       " 'rope',\n",
       " 'skin',\n",
       " 'meat',\n",
       " 'blood',\n",
       " 'bone',\n",
       " 'fat (noun)',\n",
       " 'egg',\n",
       " 'horn',\n",
       " 'tail',\n",
       " 'feather',\n",
       " 'hair',\n",
       " 'head',\n",
       " 'ear',\n",
       " 'eye',\n",
       " 'nose',\n",
       " 'mouth',\n",
       " 'tooth',\n",
       " 'tongue',\n",
       " 'fingernail',\n",
       " 'foot',\n",
       " 'leg',\n",
       " 'knee',\n",
       " 'hand',\n",
       " 'wing',\n",
       " 'belly',\n",
       " 'guts',\n",
       " 'neck',\n",
       " 'back',\n",
       " 'breast',\n",
       " 'heart',\n",
       " 'liver',\n",
       " 'drink',\n",
       " 'eat',\n",
       " 'bite',\n",
       " 'suck',\n",
       " 'spit',\n",
       " 'vomit',\n",
       " 'blow',\n",
       " 'breathe',\n",
       " 'laugh',\n",
       " 'see',\n",
       " 'hear',\n",
       " 'know (a fact)',\n",
       " 'think',\n",
       " 'smell',\n",
       " 'fear',\n",
       " 'sleep',\n",
       " 'live',\n",
       " 'die',\n",
       " 'kill',\n",
       " 'fight',\n",
       " 'hunt',\n",
       " 'hit',\n",
       " 'cut',\n",
       " 'split',\n",
       " 'stab',\n",
       " 'scratch',\n",
       " 'dig',\n",
       " 'swim',\n",
       " 'fly (verb)',\n",
       " 'walk',\n",
       " 'come',\n",
       " 'lie',\n",
       " 'sit',\n",
       " 'stand',\n",
       " 'turn',\n",
       " 'fall',\n",
       " 'give',\n",
       " 'hold',\n",
       " 'squeeze',\n",
       " 'rub',\n",
       " 'wash',\n",
       " 'wipe',\n",
       " 'pull',\n",
       " 'push',\n",
       " 'throw',\n",
       " 'tie',\n",
       " 'sew',\n",
       " 'count',\n",
       " 'say',\n",
       " 'sing',\n",
       " 'play',\n",
       " 'float',\n",
       " 'flow',\n",
       " 'freeze',\n",
       " 'swell',\n",
       " 'sun',\n",
       " 'moon',\n",
       " 'star',\n",
       " 'water',\n",
       " 'rain',\n",
       " 'river',\n",
       " 'lake',\n",
       " 'sea',\n",
       " 'salt',\n",
       " 'stone',\n",
       " 'sand',\n",
       " 'dust',\n",
       " 'earth',\n",
       " 'cloud',\n",
       " 'fog',\n",
       " 'sky',\n",
       " 'wind',\n",
       " 'snow',\n",
       " 'ice',\n",
       " 'smoke',\n",
       " 'fire',\n",
       " 'ashes',\n",
       " 'burn',\n",
       " 'road',\n",
       " 'mountain',\n",
       " 'red',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'white',\n",
       " 'black',\n",
       " 'night',\n",
       " 'day',\n",
       " 'year',\n",
       " 'warm',\n",
       " 'cold',\n",
       " 'full',\n",
       " 'new',\n",
       " 'old',\n",
       " 'good',\n",
       " 'bad',\n",
       " 'rotten',\n",
       " 'dirty',\n",
       " 'straight',\n",
       " 'round',\n",
       " 'sharp',\n",
       " 'dull',\n",
       " 'smooth',\n",
       " 'wet',\n",
       " 'dry',\n",
       " 'correct',\n",
       " 'near',\n",
       " 'far',\n",
       " 'right',\n",
       " 'left',\n",
       " 'at',\n",
       " 'in',\n",
       " 'with',\n",
       " 'and',\n",
       " 'if',\n",
       " 'because',\n",
       " 'name']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import swadesh\n",
    "swadesh.words('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'je'),\n",
       " ('you (singular), thou', 'tu, vous'),\n",
       " ('he', 'il'),\n",
       " ('we', 'nous'),\n",
       " ('you (plural)', 'vous'),\n",
       " ('they', 'ils, elles'),\n",
       " ('this', 'ceci'),\n",
       " ('that', 'cela'),\n",
       " ('here', 'ici'),\n",
       " ('there', 'l'),\n",
       " ('who', 'qui'),\n",
       " ('what', 'quoi'),\n",
       " ('where', 'o'),\n",
       " ('when', 'quand'),\n",
       " ('how', 'comment'),\n",
       " ('not', 'ne...pas'),\n",
       " ('all', 'tout'),\n",
       " ('many', 'plusieurs'),\n",
       " ('some', 'quelques'),\n",
       " ('few', 'peu'),\n",
       " ('other', 'autre'),\n",
       " ('one', 'un'),\n",
       " ('two', 'deux'),\n",
       " ('three', 'trois'),\n",
       " ('four', 'quatre'),\n",
       " ('five', 'cinq'),\n",
       " ('big', 'grand'),\n",
       " ('long', 'long'),\n",
       " ('wide', 'large'),\n",
       " ('thick', 'pais'),\n",
       " ('heavy', 'lourd'),\n",
       " ('small', 'petit'),\n",
       " ('short', 'court'),\n",
       " ('narrow', 'troit'),\n",
       " ('thin', 'mince'),\n",
       " ('woman', 'femme'),\n",
       " ('man (adult male)', 'homme'),\n",
       " ('man (human being)', 'homme'),\n",
       " ('child', 'enfant'),\n",
       " ('wife', 'femme, pouse'),\n",
       " ('husband', 'mari, poux'),\n",
       " ('mother', 'mre'),\n",
       " ('father', 'pre'),\n",
       " ('animal', 'animal'),\n",
       " ('fish', 'poisson'),\n",
       " ('bird', 'oiseau'),\n",
       " ('dog', 'chien'),\n",
       " ('louse', 'pou'),\n",
       " ('snake', 'serpent'),\n",
       " ('worm', 'ver'),\n",
       " ('tree', 'arbre'),\n",
       " ('forest', 'fort'),\n",
       " ('stick', 'bton'),\n",
       " ('fruit', 'fruit'),\n",
       " ('seed', 'graine'),\n",
       " ('leaf', 'feuille'),\n",
       " ('root', 'racine'),\n",
       " ('bark (from tree)', 'corce'),\n",
       " ('flower', 'fleur'),\n",
       " ('grass', 'herbe'),\n",
       " ('rope', 'corde'),\n",
       " ('skin', 'peau'),\n",
       " ('meat', 'viande'),\n",
       " ('blood', 'sang'),\n",
       " ('bone', 'os'),\n",
       " ('fat (noun)', 'graisse'),\n",
       " ('egg', 'uf'),\n",
       " ('horn', 'corne'),\n",
       " ('tail', 'queue'),\n",
       " ('feather', 'plume'),\n",
       " ('hair', 'cheveu'),\n",
       " ('head', 'tte'),\n",
       " ('ear', 'oreille'),\n",
       " ('eye', 'il'),\n",
       " ('nose', 'nez'),\n",
       " ('mouth', 'bouche'),\n",
       " ('tooth', 'dent'),\n",
       " ('tongue', 'langue'),\n",
       " ('fingernail', 'ongle'),\n",
       " ('foot', 'pied'),\n",
       " ('leg', 'jambe'),\n",
       " ('knee', 'genou'),\n",
       " ('hand', 'main'),\n",
       " ('wing', 'aile'),\n",
       " ('belly', 'ventre'),\n",
       " ('guts', 'entrailles'),\n",
       " ('neck', 'cou'),\n",
       " ('back', 'dos'),\n",
       " ('breast', 'sein, poitrine'),\n",
       " ('heart', 'cur'),\n",
       " ('liver', 'foie'),\n",
       " ('drink', 'boire'),\n",
       " ('eat', 'manger'),\n",
       " ('bite', 'mordre'),\n",
       " ('suck', 'sucer'),\n",
       " ('spit', 'cracher'),\n",
       " ('vomit', 'vomir'),\n",
       " ('blow', 'souffler'),\n",
       " ('breathe', 'respirer'),\n",
       " ('laugh', 'rire'),\n",
       " ('see', 'voir'),\n",
       " ('hear', 'entendre'),\n",
       " ('know (a fact)', 'savoir'),\n",
       " ('think', 'penser'),\n",
       " ('smell', 'sentir'),\n",
       " ('fear', 'craindre, avoir peur'),\n",
       " ('sleep', 'dormir'),\n",
       " ('live', 'vivre'),\n",
       " ('die', 'mourir'),\n",
       " ('kill', 'tuer'),\n",
       " ('fight', 'se battre'),\n",
       " ('hunt', 'chasser'),\n",
       " ('hit', 'frapper'),\n",
       " ('cut', 'couper'),\n",
       " ('split', 'fendre'),\n",
       " ('stab', 'poignarder'),\n",
       " ('scratch', 'gratter'),\n",
       " ('dig', 'creuser'),\n",
       " ('swim', 'nager'),\n",
       " ('fly (verb)', 'voler'),\n",
       " ('walk', 'marcher'),\n",
       " ('come', 'venir'),\n",
       " ('lie', \"s'tendre\"),\n",
       " ('sit', \"s'asseoir\"),\n",
       " ('stand', 'se lever'),\n",
       " ('turn', 'tourner'),\n",
       " ('fall', 'tomber'),\n",
       " ('give', 'donner'),\n",
       " ('hold', 'tenir'),\n",
       " ('squeeze', 'serrer'),\n",
       " ('rub', 'frotter'),\n",
       " ('wash', 'laver'),\n",
       " ('wipe', 'essuyer'),\n",
       " ('pull', 'tirer'),\n",
       " ('push', 'pousser'),\n",
       " ('throw', 'jeter'),\n",
       " ('tie', 'lier'),\n",
       " ('sew', 'coudre'),\n",
       " ('count', 'compter'),\n",
       " ('say', 'dire'),\n",
       " ('sing', 'chanter'),\n",
       " ('play', 'jouer'),\n",
       " ('float', 'flotter'),\n",
       " ('flow', 'couler'),\n",
       " ('freeze', 'geler'),\n",
       " ('swell', 'gonfler'),\n",
       " ('sun', 'soleil'),\n",
       " ('moon', 'lune'),\n",
       " ('star', 'toile'),\n",
       " ('water', 'eau'),\n",
       " ('rain', 'pluie'),\n",
       " ('river', 'rivire'),\n",
       " ('lake', 'lac'),\n",
       " ('sea', 'mer'),\n",
       " ('salt', 'sel'),\n",
       " ('stone', 'pierre'),\n",
       " ('sand', 'sable'),\n",
       " ('dust', 'poussire'),\n",
       " ('earth', 'terre'),\n",
       " ('cloud', 'nuage'),\n",
       " ('fog', 'brouillard'),\n",
       " ('sky', 'ciel'),\n",
       " ('wind', 'vent'),\n",
       " ('snow', 'neige'),\n",
       " ('ice', 'glace'),\n",
       " ('smoke', 'fume'),\n",
       " ('fire', 'feu'),\n",
       " ('ashes', 'cendres'),\n",
       " ('burn', 'brler'),\n",
       " ('road', 'route'),\n",
       " ('mountain', 'montagne'),\n",
       " ('red', 'rouge'),\n",
       " ('green', 'vert'),\n",
       " ('yellow', 'jaune'),\n",
       " ('white', 'blanc'),\n",
       " ('black', 'noir'),\n",
       " ('night', 'nuit'),\n",
       " ('day', 'jour'),\n",
       " ('year', 'an, anne'),\n",
       " ('warm', 'chaud'),\n",
       " ('cold', 'froid'),\n",
       " ('full', 'plein'),\n",
       " ('new', 'nouveau'),\n",
       " ('old', 'vieux'),\n",
       " ('good', 'bon'),\n",
       " ('bad', 'mauvais'),\n",
       " ('rotten', 'pourri'),\n",
       " ('dirty', 'sale'),\n",
       " ('straight', 'droit'),\n",
       " ('round', 'rond'),\n",
       " ('sharp', 'tranchant, pointu, aigu'),\n",
       " ('dull', 'mouss'),\n",
       " ('smooth', 'lisse'),\n",
       " ('wet', 'mouill'),\n",
       " ('dry', 'sec'),\n",
       " ('correct', 'juste, correct'),\n",
       " ('near', 'proche'),\n",
       " ('far', 'loin'),\n",
       " ('right', ' droite'),\n",
       " ('left', ' gauche'),\n",
       " ('at', ''),\n",
       " ('in', 'dans'),\n",
       " ('with', 'avec'),\n",
       " ('and', 'et'),\n",
       " ('if', 'si'),\n",
       " ('because', 'parce que'),\n",
       " ('name', 'nom')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en2fr = swadesh.entries(['en', 'fr'])\n",
    "en2fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translate = dict(en2fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chien'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate['dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('car.n.01')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('motorcar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car', 'auto', 'automobile', 'machine', 'motorcar']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a motor vehicle with four wheels; usually propelled by an internal combustion engine'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he needs a car to get to work']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('car.n.01.car'),\n",
       " Lemma('car.n.01.auto'),\n",
       " Lemma('car.n.01.automobile'),\n",
       " Lemma('car.n.01.machine'),\n",
       " Lemma('car.n.01.motorcar')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('car.n.01').lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'automobile'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.lemma('car.n.01.automobile').name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'auto', 'automobile', 'machine', 'motorcar']\n",
      "['car', 'railcar', 'railway_car', 'railroad_car']\n",
      "['car', 'gondola']\n",
      "['car', 'elevator_car']\n",
      "['cable_car', 'car']\n"
     ]
    }
   ],
   "source": [
    "# Get all synonyms of all usages of the word car\n",
    "for synset in wn.synsets('car'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordnet Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('ambulance.n.01')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motorcar = wn.synset('car.n.01')\n",
    "# Hyponyms are more specific versions of words\n",
    "types_of_motorcar = motorcar.hyponyms()\n",
    "types_of_motorcar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('motor_vehicle.n.01')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hypernyms are more abstract versions of words\n",
    "motorcar.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of paths to complete abstraction a word can take\n",
    "paths = motorcar.hypernym_paths()\n",
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'physical_entity.n.01',\n",
       " 'object.n.01',\n",
       " 'whole.n.02',\n",
       " 'artifact.n.01',\n",
       " 'instrumentality.n.03',\n",
       " 'container.n.01',\n",
       " 'wheeled_vehicle.n.01',\n",
       " 'self-propelled_vehicle.n.01',\n",
       " 'motor_vehicle.n.01',\n",
       " 'car.n.01']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One whole abstraction path for a word\n",
    "[synset.name() for synset in paths[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity.n.01',\n",
       " 'physical_entity.n.01',\n",
       " 'object.n.01',\n",
       " 'whole.n.02',\n",
       " 'artifact.n.01',\n",
       " 'instrumentality.n.03',\n",
       " 'conveyance.n.03',\n",
       " 'vehicle.n.01',\n",
       " 'wheeled_vehicle.n.01',\n",
       " 'self-propelled_vehicle.n.01',\n",
       " 'motor_vehicle.n.01',\n",
       " 'car.n.01']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[synset.name() for synset in paths[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The root hypernym of a word\n",
    "motorcar.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('burl.n.02'),\n",
       " Synset('crown.n.07'),\n",
       " Synset('limb.n.02'),\n",
       " Synset('stump.n.01'),\n",
       " Synset('trunk.n.01')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part_meronyms are things that make up the object that the word conveys\n",
    "wn.synset('tree.n.01').part_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heartwood.n.01'), Synset('sapwood.n.01')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtance the object is made of\n",
    "wn.synset('tree.n.01').substance_meronyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('forest.n.01')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the physical group the object can belong to\n",
    "wn.synset('tree.n.01').member_holonyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('step.v.01')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an action which \"entails\" doing something as well\n",
    "wn.synset('walk.v.01').entailments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('demand.n.02.demand')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opposites of words\n",
    "wn.lemma('supply.n.02.supply').antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "right_whale = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('whale.n.02')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_whale.lowest_common_hypernyms(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('vertebrate.n.01')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_whale.lowest_common_hypernyms(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min_depth is sort of a measure of how specific a word is\n",
    "wn.synset('baleen_whale.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('entity.n.01').min_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07692307692307693"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# paths that are closer to one are more similar, -1 if no path similarities found\n",
    "right_whale.path_similarity(tortoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "right_whale.path_similarity(orca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
